<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA QUALITY CHALLENGES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Abdallah, Mohammad</field>
  <field name="keywords">Big Data;Quality Measurement;Quality Model;Quality Assurance</field>
  <field name="abstract">Big Data, is a growing technique these days. There are many uses of Big Data; Artificial Intelligence, Health Care, Business, and many more. For that reason, it becomes necessary to deal with this massive volume of data with caution and care in a term to make sure that the data used and produced is in high quality. Therefore, the Big Data quality is must, and its rules have to be satisfied. In this paper, the main Big Data Quality Factors, which need to be measured, is presented in the perspective of the data itself, the data management, data processing, and data users. This research highlights the quality factors that may be used later to create different Big Data quality models.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICBDCI.2019.8686099</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA QUALITY FRAMEWORK: PRE-PROCESSING DATA IN WEATHER MONITORING APPLICATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Juneja, Ashish and Das, Nripendra Narayan</field>
  <field name="keywords">Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big Data;Big Data Quality;Data Quality;preprocessing;pre-processing</field>
  <field name="abstract">Big Data has become an imminent part of all industries and business sectors today. All organizations in any sector like energy, banking, retail, hardware, networking, etc all generate huge quantum of heterogenous data which if mined, processed and analyzed accurately can reveal immensely useful patterns for business heads to apply to generate and grow their businesses. Big Data helps in acquiring, processing and analyzing large amounts of heterogeneous data to derive valuable results. Quality of information is affected by size, speed and format in which data is generated. Hence, Quality of Big Data is of great relevance and importance. We propose addressing various aspects of the raw data to improve its quality in the pre-processing stage, as the raw data may not usable as-is. We are exploring process like Cleansing to fix as much data as feasible, Noise filters to remove bad data, as well sub-processes for Integration and Filtering along with Data Transformation/Normalization. We evaluate and profile the Big Data during acquisition stage, which is adapted to expectations to avoid cost overheads later while also improving and leading to accurate data analysis. Hence, it is imperative to improve Data quality even it is absorbed and utilized in an industry's Big Data system. In this paper, we propose a Pre-Processing Framework to address quality of data in a weather monitoring and forecasting application that also takes into account global warming parameters and raises alerts/notifications to warn users and scientists in advance.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/COMITCon.2019.8862267</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">QUALIBD: A TOOL FOR MODELLING QUALITY REQUIREMENTS FOR BIG DATA APPLICATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Arruda, Darlan and Madhavji, Nazim H.</field>
  <field name="keywords">Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements Modelling Tool</field>
  <field name="abstract">The development of Big Data applications is not well-explored, to our knowledge. Embracing Big Data in system building, questions arise as to how to elicit, specify, analyse, model, and document Big Data quality requirements. In our ongoing research, we explore a requirements modelling language for Big Data software applications. In this paper, we introduce QualiBD, a modelling tool that implements the proposed goal-oriented requirements language that facilitates the modelling of Big Data quality requirements.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9006294</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PROVENANCE–AWARE WORKFLOW FOR DATA QUALITY MANAGEMENT AND IMPROVEMENT FOR LARGE CONTINUOUS SCIENTIFIC DATA STREAMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa, Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield, Harold A. and Singh, Alka</field>
  <field name="keywords">Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific data workflows;data quality;provenance;atmospheric science</field>
  <field name="abstract">Data quality assessment, management and improvement is an integral part of any big data intensive scientific research to ensure accurate, reliable, and reproducible scientific discoveries. The task of maintaining the quality of data, however, is non-trivial and poses a challenge for a program like the Department of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds of instruments across the world, and distributes thousands of streaming data products that are continuously produced in near-real-time for an archive 1.7 Petabyte in size and growing. In this paper, we present a computational data processing workflow to address the data quality issues via an easy and intuitive web-based portal that allows reporting of any quality issues for any site, facility or instruments at a granularity down to individual variables in the data files. This portal allows instrument specialists and scientists to provide corrective actions in the form of symbolic equations. A parallel processing framework applies the data improvement to a large volume of data in an efficient, parallel environment, while optimizing data transfer and file I/O operations; corrected files are then systematically versioned and archived. A provenance tracking module tracks and records any change made to the data during its entire life cycle which are communicated transparently to the scientific users. Developed in Python using open source technologies, this software architecture enables fast and efficient management and improvement of data in an operational data center environment.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9006358</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DQA: SCALABLE, AUTOMATED AND INTERACTIVE DATA QUALITY ADVISOR</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford, Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh and Kalagnanam, Jayant R.</field>
  <field name="keywords">Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data quality;machine learning;data cleaning;scalability;automation;data science</field>
  <field name="abstract">Fueled with growth in the fields of Internet of Things (IoT) and Big Data, data has become one of the most valuable assets in today's world. While we are leveraging this data for analyzing complex systems using machine learning and deep learning, a considerable amount of time and effort is spent on addressing data quality issues. If undetected, data quality issues can cause large deviations in the analysis, misleading data scientists. To ease the effort of identifying and addressing data quality challenges, we introduce DQA, a scalable, automated and interactive data quality advisor. In this paper, we describe the DQA framework, provide detailed description of its components and the benefits of integrating it in a data science process. We propose a programmatic approach for implementing the data quality framework which automatically generates dynamic executable graphs for performing data validations fine-tuned for a given dataset. We discuss the use of DQA to build a library of validation checks common to many applications. We provide insight into how DQA addresses many persistence and usability issues which currently make data cleaning a laborious task for data scientists. Finally, we provide a case study of how DQA is implemented in a realworld system and describe the benefits realized.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9006187</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA QUALITY MANAGEMENT FOR BIG DATA APPLICATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Khaleel, Majida Yaseen and Hamad, Murtadha M.</field>
  <field name="keywords">Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big Data;data quality;unstructured Data Distributed data file system;and statistical model.</field>
  <field name="abstract">Currently, as a result of the continuous increase of data, one of the key issues is the development of systems and applications to deal with storage, management and processing of big numbers of data. These data are found in unstructured ways. Data management with traditional approaches is inappropriate because of the large and complex data sizes. Hadoop is a suitable solution for the continuous increase in data sizes. The important characteristics of the Hadoop are distributed processing, high storage space, and easy administration. Hadoop is better known for distributed file systems. In this paper, we have proposed techniques and algorithms that deal with big data including data collecting, data preprocessing, algorithms for data cleaning, A Technique for Converting Unstructured Data to Structured Data using metadata, distributed data file system (fragmentation algorithm) and Quality assurance algorithms by using the model is the statistical model to evaluate the highest educational institutions. We concluded that Metadata accelerates query response required and facilitates query execution, metadata will be content for reports, fields and descriptions. Total time access for three complex queries in distributed processing it is 00: 03: 00 per second while in nondistributed processing it is at 00: 15: 77 per second, average is approximately five minutes per second. Quality assurance note values (T-test) is 0.239 and values (T-dis) is 1.96, as a result of dealing with scientific sets and humanities sets. In the comparison law, it can be deduced that if the t-test is smaller than the t-dis; so there is no difference between the mean of the scientific and humanities samples, the values of C.V for both scientific is (8.585) and humanities sets is (7.427), using the law of homogeneity know whether any sets are more homogeneous whenever the value of a small C.V was more homogeneous however the humanity set is more homogeneity.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/DeSE.2019.00072</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS A MULTI-AGENTS MODEL FOR ERRORS DETECTION AND CORRECTION IN BIG DATA FLOWS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif, Abdelaziz</field>
  <field name="keywords">Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction of data errors</field>
  <field name="abstract">The quality of data for decision-making will always be a major factor for companies that want to remain competitors. In addition, the era of Big Data has brought new challenges for the processing, management, storage of data and in particular the challenge represented by the veracity of these data which is one of the 5Vs that characterizes Big Data. This characteristic that defines the quality or reliability of the data and its sources must be verified in the future systems of each company. In this paper, we present an approach that helps to improve the quality of Big Data by the distributed execution of algorithms for detecting and correcting data errors. The idea is to have a multi-agents model for errors detection and correction in big data flow. This model linked to a repository specific to each company. This repository contains the most frequent errors, metadata, error types, error detection algorithms and error correction algorithms. Each agent of this model represents an algorithm and will be deployed in multiple instances when needed. The use of these agents will go through two steps. In the first step, the detection agents and error correction agents manage each flow entering the system in real time. In the second step, all the processed data flows in first step will be a dataset to which the error detection and correction agents are applied in batch in order to process other types of errors. Among architectures who allow this processing type, we have chosen Lambda architecture.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICDS47004.2019.8942297</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA QUALITY ASSURANCE THROUGH DATA TRACEABILITY: A CASE STUDY OF THE NATIONAL STANDARD REFERENCE DATA PROGRAM OF KOREA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lee, Doyoung</field>
  <field name="keywords">Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical measurement;Big data;data quality;data traceability;metrology;standard reference data;uncertainty</field>
  <field name="abstract">In the era of big data, the scientific and social demand for quality data is aggressive and urgent. This paper sheds light on the expanded role of metrology of verifying validated procedures of data production and developing adequate uncertainty evaluation methods to ensure the trustworthiness of data and information. In this regard, I explore the mechanism of the national standard reference data (SRD) program of Korea, which connects various scientific and social sectors to metrology by applying useful metrological concepts and methods to produce reliable data and convert such data into national standards. In particular, the changing interpretation of metrological key concepts, such as “measurement,” “traceability,” and “uncertainty,” will be explored and reconsidered from the perspective of data quality assurance. As a result, I suggest the concept of “data traceability” with “the matrix of data quality evaluation” according to the elements of a data production system and related evaluation criteria. To conclude, I suggest social and policy implications for the new role of metrology and standards for producing and disseminating reliable knowledge sources from big data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2904286</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AN INTERACTIVE DATA QUALITY TEST APPROACH FOR CONSTRAINT DISCOVERY AND FAULT DETECTION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael G</field>
  <field name="keywords">Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive learning;Unsupervised learning</field>
  <field name="abstract">Data quality tests validate heterogeneous data to detect violations of syntactic and semantic constraints. The specification of these constraints can be incomplete because domain experts typically specify them in an ad hoc manner. Existing automated test approaches can generate false alarms and do not explain the constraint violations while reporting faulty data records. In previous work, we proposed ADQuaTe, which is an automated data quality test approach that uses an unsupervised deep learning techni que (1) to discover constraints from big datasets that may have been missed by experts, and (2) to label as suspicious those records that violate the constraints. These records are grouped and explanations for constraint violations are presented to domain experts who determine whether or not the groups are actually faulty. This paper presents ADQuaTe2, which extends ADQuaTe to use an interactive learning technique that incorporates expert feedback to retrain the learning model and improve the accuracy of constraint discovery and fault detection. We evaluate the effectiveness of the approach on real-world datasets from a health data warehouse and a plant diagnosis database. We also use datasets with known faults from the UCI repository to evaluate the improvement in the accuracy of the approach after incorporating ground truth knowledge.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9006446</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A BIG DATA PLATFORM FOR SURFACE ENHANCED RAMAN SPECTROSCOPY DATA WITH AN APPLICATION ON IMAGE-BASED SENSOR QUALITY CONTROL</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs, Anita</field>
  <field name="keywords">Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big data platform, Surface Enhanced Raman Spectroscopy, sensor quality control</field>
  <field name="abstract">Surface-enhanced Raman spectroscopy (SERS) significantly enhances the Raman scattering by molecules, enabling detection and identification of small quantities of relevant bio-/chemical markers in a wide range of applications. In this paper, we present a big data platform with both a local client and cloud server built for acquiring, processing, visualizing and storing SERS sensor data. The local client controls the hardware (i.e., spectrometer and stage) to collect SERS spectra from HP designed sensors, and offers the options to analyze, visualize and save the spectra with meta-data records, including relevant experimental conditions. The cloud server contains remote databases and web interface for centralized data management to users from different locations. Here we describe how this platform was built and demonstrate its use for automated sensor quality control based on sensor images. Sensor quality control is a common practice, employed in sensor production to select high performing sensors. Image-based approach is a natural way to perform sensor quality control without destructing the sensors. Automating this process using the proposed platform can also reduce the time spent and achieve consistent result by avoiding human visual inspection.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/MIPR.2019.00093</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A MISSING POWER DATA FILLING METHOD BASED ON IMPROVED RANDOM FOREST ALGORITHM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and Zhu, Liang</field>
  <field name="keywords">Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data mining;Big data cleaning;missing data filling;data preprocessing;random forest;data quality</field>
  <field name="abstract">Missing data filling is a key step in power big data preprocessing, which helps to improve the quality and the utilization of electric power data. Due to the limitations of the traditional methods of filling missing data, an improved random forest filling algorithm is proposed. As a result of the horizontal and vertical directions of the electric power data are based on the characteristics of time series. Therefore, the method of improved random forest filling missing data combines the methods of linear interpolation, matrix combination and matrix transposition to solve the problem of filling large amount of electric power missing data. The filling results show that the improved random forest filling algorithm is applicable to filling electric power data in various missing forms. What's more, the accuracy of the filling results is high and the stability of the model is strong, which is beneficial in improving the quality of electric power data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.23919/CJEE.2019.000025</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ADDRESSING DATA QUALITY PROBLEMS WITH METAMORPHIC DATA RELATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Auer, Florian and Felderer, Michael</field>
  <field name="keywords">Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic testing, data quality, big data, quality assessment, metamorphic data relations</field>
  <field name="abstract">In the era of big data, cloud computing and the Internet of Things, the quality of data has tremendous impact on our everyday life. Moreover, the increasing velocity, volume and variety of data requires new approaches for quality assessment. In this paper, a new approach for quality assessment is presented that applies metamorphic testing to data quality. The exemplary application of the approach on a big data application shows promising results for the suitability of the approach.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/MET.2019.00019</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AN AUTOMATED BIG DATA ACCURACY ASSESSMENT TOOL</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin</field>
  <field name="keywords">Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest neighbors;logistic regression;decision trees</field>
  <field name="abstract">Data analysis is the most important aspect of any business as it is critical to decision-making. Data quality assessment is a necessary function to be performed before data analysis, as the quality of data has high impact on the outcome of the analysis. Data quality is a multi-dimensional factor that affects the analysis in numerous ways. Among all the dimensions, accuracy is the most important and hardest dimension to assess. With the advent of big data, this problem becomes more complicated. There are only few studies that focus on data accuracy with minimal domain expert dependency. In this paper, we propose an extensive data accuracy assessment tool that uses machine learning to determine the accuracy of data. In addition, our model also addresses the intrinsic and contextual categories of data accuracy. Our model was developed on Apache Spark which serves as the big data environment for handling large datasets.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICBDA.2019.8713218</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">MULTI-DIMENSIONAL INDEX CONSTRUCTION OF ELECTRIC POWER MULTI-SOURCE MEASUREMENT DATA CONSIDERING SPATIO-TEMPORAL CORRELATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Pan, Lingling and Liu, Jun and Li, Feng</field>
  <field name="keywords">Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional index</field>
  <field name="abstract">The operation of complex AC/DC power grid changes rapidly and dynamically, which objectively puts forward higher requirements for on-line analysis, and it is urgent to improve the basic data quality of power grid. Because of low quality and poor synchronization of the basic data of power grid, it is impossible to accurately map the actual operation of the power grid. At the same time, the cross-system data matching degree is low and the data correlation is poor, so it can not support the multi-scale data analysis for all kinds of applications. In this paper, the associated method of multi-source heterogeneous data in the power grid is studied. Combined with big data's access characteristics, big data storage, big data retrieval and artificial intelligence technology, the high-speed data storage and index architecture of power big data are constructed, and a multi-dimensional index reflecting the associated relationship of operating data is established from the dimensions of time, space, application, device and so on. It is easier to analyze multi-source data, to improve the basic data quality of power grid, which provides effective support for accurate data analysis and evaluation of power grid.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/IAEAC47372.2019.8997699</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA DRIVEN SMART AGRICULTURE: PATHWAY FOR SUSTAINABLE DEVELOPMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada, Shaheen and Li, Dan and Zhang, Jie</field>
  <field name="keywords">Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big data;smart agriculture;data driven;precision agriculture;smart farming</field>
  <field name="abstract">Increasing agricultural production is top most solution in the face of rapid population growth through digitalization of agriculture by using most developed technology like big data. There is a long debate on the application of big data in agriculture. This study is an attempt to explore the suitability of the big data technologies for increasing production and improving quality in agriculture. The study uses an extensive review of current research works and studies in agriculture for exploring the best and compatible practices which can help farmers at field level for increasing production and improving quality. This study reveals a number of available big data technologies and practices in agriculture for solving the current problems and challenges at field level. A conceptual model is developed for proper implementation of available big data technologies at farmer's field level. The study highlights data generation procedure, availability of technology, availability of hardware, software, data collection techniques, method of analysis and suitability of application of big data technologies for smart agriculture. The article explores that there are still some challenges exists in this field as a new domain in agriculture like privacy of data, data quality, availability, initial investment, infrastructure and related expertise. The study suggests that government initiatives, public-private partnership, openness of data, financial investment and regional basis research work are necessary for implementing the big data technologies in agriculture at large scale.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICAIBD.2019.8836982</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA TECHNOLOGY: CHALLENGES, PROSPECTS, AND REALITIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.</field>
  <field name="keywords">Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big data;business analytics;Hadoop;people;talent gap;implementation</field>
  <field name="abstract">We attempt to demonstrate the value of big data to enterprises by interweaving the perceptions, challenges, and opportunities of big data for businesses. While enterprises are aware of the value of big data to their businesses, there are challenges of exploiting big data in terms of data quality and usage. Executives might lack knowledge on how applications are related to one another in the big data ecosystem and the business benefits to reap. We summarize the results of a research study that explores emerging business perceptions of big data. We examine the current practice in 20 large enterprises, each having an annual revenue of more than USD 0.5 billion and present our findings related to executive perceptions. We provide insights on how firms can develop their big data expertise along various dimensions and identify critical ideas to be further investigated to better understand the issues that practitioners and researchers might be equally grappling with.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/EMR.2019.2900208</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A POLICY-BASED APPROACH FOR MEASURING DATA QUALITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O’Gorman, Tristan</field>
  <field name="keywords">Data integrity;Standards;Measurement;Asset management;Data models;Complexity theory;Data Quality;Asset Management Systems;Policy based Data Management</field>
  <field name="abstract">With the growing importance of data in all aspects of the functioning of an enterprise, having good quality of data is crucial in support of business processes. However, there do not exist good metrics to measure the quality of data that is available within an enterprise. While there are several data quality standards, their complexity and their required customization makes them difficult to use in real-world industrial scenarios. In this paper, we discuss the challenges encountered in measuring data quality within asset management systems. We propose a policy-based approach for measuring data quality, and show how such an approach can be customized and interpreted easily by practitioners in the field.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9006422</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AN INTEGRATED BIG AND FAST DATA ANALYTICS PLATFORM FOR SMART URBAN TRANSPORTATION MANAGEMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre, Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and Kozievitch, Nádia P. and Fonseca, Keiko Verônica Ono and Antunes, Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira, Wagner and Aloisio, Giovanni</field>
  <field name="keywords">Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed environment;public transport management;smart city</field>
  <field name="abstract">Smart urban transportation management can be considered as a multifaceted big data challenge. It strongly relies on the information collected into multiple, widespread, and heterogeneous data sources as well as on the ability to extract actionable insights from them. Besides data, full stack (from platform to services and applications) Information and Communications Technology (ICT) solutions need to be specifically adopted to address smart cities challenges. Smart urban transportation management is one of the key use cases addressed in the context of the EUBra-BIGSEA (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric Applications) project. This paper specifically focuses on the City Administration Dashboard, a public transport analytics application that has been developed on top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges. The solution proposed in this paper joins together a scalable big and fast data analytics platform, a flexible and dynamic cloud infrastructure, data quality and entity matching algorithms as well as security and privacy techniques. By exploiting an interoperable programming framework based on Python Application Programming Interface (API), it allows an easy, rapid and transparent development of smart cities applications.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2936941</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RELAY PROTECTION DATA INTEGRITY CHECK METHOD BASED ON BIG DATA ASSOCIATION ALGORITHM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang, Lie and Zhang, Hanfang</field>
  <field name="keywords">Apriori algorithm;relay protection;defect data;integrity check</field>
  <field name="abstract">Relay protection big data creates good conditions for the improvement of professional applications, and data integrity is an important aspect that reflects data quality. The association of relay protection big data is intense. This paper applies Apriori algorithm to mine data relevance and generate association rules. Based on this, the integrity of relay protection data is checked, and the incomplete data is predicted. Taking the relay protection defect data as an example, the paper explores the correlation among 251 items of the six dimensions of protection relay defect data such as type of protection, the severity of the defect, whether the protection is out of operation, the defect location, the cause of the defect, and the equipment manufacturer, completing the processing of incomplete data with great application results.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/EEI48997.2019.00115</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PRACTICES AND EXPERIENCES IN HIGH VOLUMES OF SATELLITE DATA MANAGEMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Han, Weiguo and Jochum, Matthew</field>
  <field name="keywords">Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository</field>
  <field name="abstract">High volumes of satellite data management within an organization is still challenging and daunting in the era of big data. The increasing information technology costs and limited budgets, growing satellite data needs, data availability across multiple teams and projects, strategic goals of organization, and expected project outcomes require better satellite data management mechanism and system to facilitate research and development activities. An organization level centralized satellite data repository is a practical solution to satisfy these requirements. This paper describes the best practices and experiences from building such a central satellite data repository within our organization, including data management strategy and policy, scalable and extensible system infrastructure, comprehensive data management system, and technical support and user assistance. These practices can be borrowed and applied in other organizations with similar requirements.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/IGARSS.2019.8900190</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INFORMATION-CENTRIC VIRTUALIZATION FOR SOFTWARE-DEFINED STATISTICAL QOS PROVISIONING OVER 5G MULTIMEDIA BIG DATA WIRELESS NETWORKS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhang, Xi and Zhu, Qixuan</field>
  <field name="keywords">Big Data;Quality of service;Wireless networks;5G mobile communication;Resource management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal transmit power;statistical delay-bounded QoS;effective capacity;relay selection</field>
  <field name="abstract">The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/JSAC.2019.2927088</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ONE-PASS INCONSISTENCY DETECTION ALGORITHMS FOR BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong</field>
  <field name="keywords">Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency detection;big data;one-pass algorithm;data quality;denial constraint</field>
  <field name="abstract">Data in the real world is often dirty. Inconsistency is an important kind of dirty data; before repairing inconsistency, we need to detect them first. The time complexities of the current inconsistency detection algorithms are super-linear to the size of data and not suitable for the big data. For the inconsistency detection of big data, we develop an algorithm that detects inconsistency within the one-pass scan of the data according to both the functional dependency (FD) and the conditional functional dependency (CFD) in our previous work. In this paper, we propose inconsistency detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are more expressive than FDs and CFDs. Developing the algorithm to detect the violation of DCs increases the applicability of our inconsistency detection algorithms. We compare the performance of our algorithm with the performance of implementing SQL queries in MySQL and BigQuery. The experimental results indicate the high efficiency of our algorithms.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2898707</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INTELLIGENT DATA ENGINEERING FOR MIGRATION TO NOSQL BASED SECURE ENVIRONMENTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed</field>
  <field name="keywords">Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational databases;NoSQL;big data;data cleansing</field>
  <field name="abstract">In an era of super computing, data is increasing exponentially requiring more proficiency from the available technologies of data storage, data processing, and analysis. Such continuous massive growth of structured and unstructured data is referred to as a “Big data”. The processing and storage of big data through a conventional technique is not possible. Due to improved proficiency of Big Data solution in handling data, such as NoSQL caused the developers in the previous decade to start preferring big data databases, such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology that is designed to provide scalability to support voluminous data, leading to the rise of NoSQL as the most viable database solution. These modern databases aim to overcome the limitations of relational databases such as unlimited scalability, high performance, data modeling, data distribution, and continuous availability. These days, the larger enterprises need to shift NoSQL databases due to their more flexible models. It is a great challenge for business organizations and enterprises to transform their existing databases to NoSQL databases considering heterogeneity and complexity in relational data. In addition, with the emergence of big data, data cleansing has become a great challenge. In this paper, we proposed an approach that has two modules: data transformation and data cleansing module. The first phase is the transformation of a relational database to Oracle NoSQL database through model transformation. The second phase provides data cleansing ability to improve data quality and prepare it for big data analytics. The experiments show the proposed approach successfully transforms the relational database to a big data database and improve data quality.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2916912</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SCALABLE SEMISUPERVISED GMM FOR BIG DATA QUALITY PREDICTION IN MULTIMODE PROCESSES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yao, Le and Ge, Zhiqiang</field>
  <field name="keywords">Data models;Big Data;Predictive models;Inference algorithms;Prediction algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic variational inference (SVI)</field>
  <field name="abstract">In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/TIE.2018.2856200</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">FUZZY COMPREHENSIVE EVALUATION METHOD FOR ON-LINE MONITORING DATA QUALITY OF SUBSTATION EQUIPMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dehui, Fu and Feng, Wang and Shuai, Yuan and Guangzhen, Wang and Mingxin, Shao</field>
  <field name="keywords">big data;data quality;subordination;fuzzy comprehensive evaluation;On-line monitoring</field>
  <field name="abstract">This paper analyses the existing problems in on-line monitoring and data quality evaluation of substation equipment, and proposes a multi-dimensional fuzzy comprehensive evaluation method for on-line monitoring data quality of substation equipment. The evaluation index set of online monitoring data quality of substation equipment with 5 dimensions and 11 secondary indexes is established. The weight is determined by combining subjective and objective methods. The fuzzy transformation is completed based on membership function and a multi-dimensional fuzzy comprehensive evaluation model is established. Finally, the evaluation grade of online monitoring data of substation equipment is obtained. Finally, compared with other methods, the validity and accuracy of this method are verified.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICISCE48695.2019.00154</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RISK ASSESSMENT MODEL AND EXPERIMENTAL ANALYSIS OF ELECTRIC POWER PRODUCTION BASED ON BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong</field>
  <field name="keywords">risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index</field>
  <field name="abstract">This paper studies the characteristics of big data of power, and aims at the data quality problems faced by power system. It puts forward an assessment method of power system data quality. Based on the characteristics of large power data, a series of indicators influencing the data are analyzed and hierarchically divided to determine the measurement standard of power production data during the process of risk management, namely, the risk index system. Then, the risk assessment model of power data is established by referring to the assessment model in other fields or the rules of deduction and induction in data mining. It can be used to evaluate the quality of power system data, and find a framework and solution suitable for large data quality assessment. Finally, the model is implemented on Hadoop platform, which proves that it takes into account the completeness of the index system, the objectivity of the assessment method and the rapidity of the calculation method.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICSGEA.2019.00028</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA PREPARATION FOR DATA MINING IN CHEMICAL PLANTS USING BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer</field>
  <field name="keywords">Data quality;Soft sensors;Big data</field>
  <field name="abstract">Data preparation for data mining in industrial applications is a key success factor which requires considerable repeated efforts. Although the required activities need to be repeated in very similar fashion across many projects, details of their implementation differ and require both application understanding and experience. As a result, data preparation is done by data mining experts with a strong domain background and a good understanding of the characteristics of the data to be analyzed. Experts with these profiles usually have an engineering background and no strong expertise in distributed programming or big data technology. Unfortunately, the amount of data can be so large that distributed algorithms are required to allow for inspection of results and iteration of preparation steps. This contribution introduces an interactive data preparation workflow for signal data from chemical plants enabling domain experts without background in distributed computing and extensive programming experience to leverage the power of big data technologies.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/INDIN41052.2019.8972078</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">QUICKADAPT: SCALABLE ADAPTATION FOR BIG DATA CYBER SECURITY ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ullah, Faheem and Ali Babar, M.</field>
  <field name="keywords">Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer architecture;big data, cyber security, adaptation, accuracy</field>
  <field name="abstract">Big Data Cyber Security Analytics (BDCA) leverages big data technologies for collecting, storing, and analyzing a large volume of security events data to detect cyber-attacks. Accuracy and response time, being the most important quality concerns for BDCA, are impacted by changes in security events data. Whilst it is promising to adapt a BDCA system's architecture to the changes in security events data for optimizing accuracy and response time, it is important to consider large search space of architectural configurations. Searching a large space of configurations for potential adaptation incurs an overwhelming adaptation time, which may cancel the benefits of adaptation. We present an adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system. QuickAdapt uses descriptive statistics (e.g., mean and variance) of security events data and fuzzy rules to (re) compose a system with a set of components to ensure optimal accuracy and response time. We have evaluated QuickAdapt for a distributed BDCA system using four datasets. Our evaluation shows that on average QuickAdapt reduces adaptation time by 105× with a competitive adaptation accuracy of 70% as compared to an existing solution.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICECCS.2019.00016</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE ANALYSIS AND DESIGN OF SHIP MONITORING SYSTEM BASED ON HYBRID REPLICATION TECHNOLOGY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jiang, Ying and Zhang, Na and Fang, Ying</field>
  <field name="keywords">Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine vehicles;Distributed database;advanced replication;materialized view;data conflict</field>
  <field name="abstract">As the core of informatization, data has a huge significance to the development of information-based enterprises. Data replication technology is an important approach to solve the problem of enterprise data sharing based on distributed database system. It plays a crucial role in promoting business integration of enterprises and institutions, improving data quality, enhancing data sharing and improving the application level of back-end big data analysis [1]. It is necessary to do research for making a good data management of the distributed database application system, synchronizing the data to the data, preventing data conflicting and being able to synchronize or asynchronous replication. Combining with the database design model of the ship monitoring and control system, this paper mainly described how to complete the construction of distributed database system using Oracle, based on advanced replication technology named as the combination of multi-agent replication and materialized views hybrid replication technology.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICITBS.2019.00118</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ASSESSING CONTEXT-AWARE DATA CONSISTENCY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.</field>
  <field name="keywords">Feature extraction;Data analysis;Data integrity;Data models;Machine learning algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache hadoop;apache spark</field>
  <field name="abstract">Data analysis is a demanding task that involves extracting deep insights hidden in data. Many businesses enforce data analysis irrespective of the domain, as it is crucial in minimizing developmental risks. Raw data cannot be used to perform any analysis as poor-quality data leads to erroneous decision-making. This makes data quality assessment a necessary function before data analysis. Data quality is a multi-dimensional factor that affects the analysis in multiple ways. Among all the dimensions, consistency is one of the most critical dimensions to assess. Context of data plays an important role in consistency assessment, as the records are inherently related within a dataset. Existing studies are computationally expensive and do not consider the context of data. In this paper, we propose a comprehensive context-aware data consistency assessment tool that uses machine learning to evaluate the consistency of data. Our model was developed on Apache Hadoop and Apache Spark to support big data, as well as to boost some computationally intensive algorithms.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/AICCSA47632.2019.9035250</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">QUALITY DRIVEN JUDICIAL DATA GOVERNANCE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia</field>
  <field name="keywords">Data integrity;Big Data;Decision making;Organizations;Standards organizations;data quality;judicial data governance;quality measurement</field>
  <field name="abstract">With the development of Smart Court 3.0, the amount of judicial data that can be stored and processed by the computer is increasing rapidly. People gradually realize that judicial data contains tremendous social and business value. However, we need stronger ability to handle with and apply massive, multi-source and heterogeneous judicial data. A complete data governance system should be built in order to make full use of the value of data assets. In such a data governance system, data quality control is one of the key steps of data governance, and also the bottleneck of data service development, because data quality determines the upper limit of data application. This paper proposes a judicial data quality measurement framework by analyzing some judicial business data, followed by a data governance method driven by it.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/QRS-C.2019.00026</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA CLEANING OPTIMIZATION FOR GRAIN BIG DATA PROCESSING USING TASK MERGING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ju, Xingang and Lian, Feiyu and Zhang, Yuan</field>
  <field name="keywords">grain big data;data cleaning;task merging;hadoop;mapReduce</field>
  <field name="abstract">Data quality has exerted important influence over the application of grain big data, so data cleaning is a necessary and important work. In MapReduce frame, we can use parallel technique to execute data cleaning in high scalability mode, but due to the lack of effective design there are amounts of computing redundancy in the process of data cleaning, which results in lower performance. In this research, we found some tasks often are carried out multiple times on same input files, or require same operation results in the process of data cleaning. For this problem, we proposed a new optimization technique that is based on task merge. By merging simple or redundancy computations on same input files, the number of the loop computation in MapReduce can be reduced greatly. The experiment shows, by this means, the overall system runtime is significantly reduced, which proves that the process of data cleaning is optimized. In this paper, we optimized several modules of data cleaning such as entity identification, inconsistent data restoration, and missing value filling. Experimental results show that the proposed method in this paper can increase efficiency for grain big data cleaning.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICISCE48695.2019.00053</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IMPLEMENTATION OF INDUSTRIAL CYBER PHYSICAL SYSTEM: CHALLENGES AND SOLUTIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yu, Wenjin and Dillon, Tharam and Mostafa, Fahed and Rahayu, Wenny and Liu, Yuehua</field>
  <field name="keywords">Big Data;Manufacturing;Industries;Sensors;Data integrity;Real-time systems;Cloud computing;Cyber-Physical System;Internet of Things;Industry 4.0;cloud computing;big data ecosystem;data quality</field>
  <field name="abstract">The Industry Internet of Things (IIoT) and the Industry Cyber-Physical System (ICPS) for real industry are becoming vitally necessary in the smart manufacturing environment. Very large number of intelligent sensors are being available generating an exploding amount of data. Several issues come with the big data in real industry, including the a grand-scale connected network construction with the data security and access protocol issues, data quality with considerable noise when gathered from industrial factories, efficient data storage, smart interconnection with cloud services, and real-time analytics requirements. This paper proposes an integrated CPS based architecture for smart manufacturing and provides the deployment details, addressing all the potential problems in an appropriate way. It has been successfully implemented in a real industry environment, and won the Best Industry Application of IoT at the BigInsights Data & AI Innovation Awards.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICPHYS.2019.8780271</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA DRIVEN COMPUTING OFFLOADING SCHEME WITH DRIVERLESS VEHICLES ASSISTANCE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Chen, Chengling and Su, Zhou and Li, Weiwei and Wang, Yuntao</field>
  <field name="keywords">Task analysis;Computational modeling;Automobiles;Big Data;Quality of service;Privacy;Bandwidth;Vehicular ad-hoc networks, reverse auction, computing offloading, driverless vehicles</field>
  <field name="abstract">In the era of big data, edge computing is emerged as a promising paradigm to alleviate the pressure on the backbone network and facilitate vehicular services on the road. As edge nodes deployed for vehicular applications, roadside units (RSUs) need to undertake a large number of local computing tasks. However, due to the uncertainty of the vehicular network topology, static RSU deployments are subject to short-term overload and cannot handle various delay-sensitive computing tasks concurrently. To address the problem, we propose a big data driven computing offloading scheme to dispatch idle driverless vehicles to enhance the capacities of RSUs dynamically. First, we present a trust assessment model to evaluate the credibility of driverless vehicles. Then, a multi-attribute reverse auction is applied to maximize the utilities of RSUs and driverless vehicles. In addition, a secure forwarding method is developed to protect the privacy of computing tasks.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00084</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CONSTRUCTION OF ELEVATOR INSPECTION QUALITY EVALUATION SYSTEM BASED ON BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhang, Xupeng and Liang, Du</field>
  <field name="keywords">Elevators;Inspection;Big Data;Safety;Market research;Testing;Monitoring;Elevator;inspection;big data;quality evaluation</field>
  <field name="abstract">Elevator inspection information has typical big data characteristics. This paper points out that the elevator inspection data introduces the method of elevator inspection big data analysis. Taking elevator inspection as an example, it lists several kinds of big data analysis methods for inspection data, including the risk points describing the basic information of the elevator, the scanning inspection process and the inspection quality. Based on frequency analysis of active factors, outlier test, quality assessment, correlation analysis. Using big data technology, it can make statistical analysis on the data obtained by elevator inspection, make the inspection situation more intuitive, help the management organization to understand the overall elevator quality and elevator inspection, and build an elevator inspection quality evaluation system to make the work more transparent and management more precise. Find more accurate questions, deeper supervision, and more scientific government decisions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICEIEC.2019.8784484</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">APPROXIMATE QUALITY ASSESSMENT WITH SAMPLING APPROACHES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liu, Hong and Sang, Zhenhua and Karali, Sameer</field>
  <field name="keywords">Data integrity;Quality assessment;Writing;Gaussian distribution;Big Data;Sampling methods;Time-frequency analysis;Data Quality, Quality Assessment, Sampling</field>
  <field name="abstract">Data is useful to the extent that it can be quickly analyzed to reveal valuable information. With high-quality data, we can increase revenue, reduce cost, and reduce risk. On the other hand, the consequences of poor-quality data can be severe. It has been estimated that poor quality customer data costs U.S. businesses $611 billion annually in postage, printing, and staff overhead. These issues make data quality assessment a necessary and critical step in any data-related systems. Big data brings new challenges to data quality assessment due to the scale of data, streaming data, and different forms of data. Therefore, we proposed a sampling-based approximate quality assessment model on large data. Sampling large datasets can make all quality assessment processes cheaper and more feasible because of data reduction. The protocol of this work: First, the sample size is determined for estimating a large dataset. Next, sampling techniques are applied to collect samples. Then, these samples are used to estimate the quality of the large dataset. The objective of quality assessment in this work is to evaluate the completeness, accuracy, and timeliness of data and to return fast and approximate scores. Using different sample sizes and different sampling methods, we obtained 72 sets of data and compared them. These results show that the proposed approach is efficient and provides some insight into the quality assessment with samples.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/CSCI49370.2019.00244</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">NORMALIZATION OF DUPLICATE RECORDS FROM MULTIPLE SOURCES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dong, Yongquan and Dragut, Eduard C. and Meng, Weiyi</field>
  <field name="keywords">Data integration;Standards;Task analysis;Databases;Google;Data mining;Terminology;Record normalization;data quality;data fusion;web data integration;deep web</field>
  <field name="abstract">Data consolidation is a challenging issue in data integration. The usefulness of data increases when it is linked and fused with other data from numerous (Web) sources. The promise of Big Data hinges upon addressing several big data integration challenges, such as record linkage at scale, real-time data fusion, and integrating Deep Web. Although much work has been conducted on these problems, there is limited work on creating a uniform, standard record from a group of records corresponding to the same real-world entity. We refer to this task as record normalization. Such a record representation, coined normalized record, is important for both front-end and back-end applications. In this paper, we formalize the record normalization problem, present in-depth analysis of normalization granularity levels (e.g., record, field, and value-component) and of normalization forms (e.g., typical versus complete). We propose a comprehensive framework for computing the normalized record. The proposed framework includes a suit of record normalization methods, from naive ones, which use only the information gathered from records themselves, to complex strategies, which globally mine a group of duplicate records before selecting a value for an attribute of a normalized record. We conducted extensive empirical studies with all the proposed methods. We indicate the weaknesses and strengths of each of them and recommend the ones to be used in practice.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/TKDE.2018.2844176</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">UNDERSTANDING SPATIO-TEMPORAL URBAN PROCESSES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Rocha, Lais M. A. and Bessa, Aline and Chirigati, Fernando and OFriel, Eugene and Moro, Mirella M. and Freire, Juliana</field>
  <field name="keywords">Urban areas;Correlation;Spatial resolution;Data analysis;Mathematical model;Public transportation;Standards;data quality;data profiling;urban data</field>
  <field name="abstract">Increasingly, decisions are based on insights and conclusions derived from the results of data analysis. Thus, determining the validity of these results is of paramount importance. In this paper, we take a step towards helping users identify potential issues in spatio-temporal data and thus gain trust in the results they derived from these data. We focus on processes that are captured by relationships among datasets that serve as the data exhaust for different components of urban environments. In this scenario, debugging data involves two important challenges: the inherent complexity of spatio-temporal data, and the number of possible relationships. We propose a framework for profiling spatio-temporal relationships that automatically identifies data slices that present a significant deviation from what is expected, and thus, helps focus a user's attention on slices of the data that may have quality issues and/or that may affect the conclusions derived from the analysis' results. We describe the profiling methodology and how it derives relationships, identifies candidate deviations, assesses their statistical significance, and measures their magnitude. We also present a series of cases studies using real datasets from New York City which demonstrate the usefulness of spatio-temporal profiling to build trust on data analysis' results.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9006289</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RFID DATA WAREHOUSING AND OLAP WITH HIVE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yoo, Yeisol and Yoo, Jin Soung</field>
  <field name="keywords">RFID data warehousing;OLAP;Hive;Cloud computing</field>
  <field name="abstract">Radio Frequency Identification (RFID) technology is used in many applications for monitoring object movement. The use of RFID in supply chain management systems enables to track the movement of products from suppliers to warehouses, store backrooms, and eventually points of sale. The vast amount of data resulting from the proliferation of RFID readers and tags poses challenges for data management and analytics. RFID data warehousing can enhance data quality and consistency, and give great potential benefits for Online Analytical Processing (OLAP) applications. Traditional data warehouses are built primarily on relational database management systems. However, the size of RFID data being collected and analyzed in the industry for business intelligence is growing rapidly, making traditional warehousing solutions prohibitively expensive. Hive is an open-source data warehousing solution built on top of Hadoop which is a popular Big Data computing framework. This paper presents alternative RFID data warehouse designs which can handle a large amount of RFID data and support a variety of OLAP queries. The proposed approaches are implemented on Hive and evaluated for query performance in cloud computing environment.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/IUCC/DSCI/SmartCNS.2019.00105</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A COLLABORATIVE MULTI-MODALITY SELECTION METHOD BASED ON DATA UTILITY ASSESSMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xiao, Yunlong and Gu, Yang and Wang, Jiwei and Wu, Tong</field>
  <field name="keywords">Data integrity;Machine learning;Data models;Training;Task analysis;Mathematical model;Gesture recognition;data selection;multimodal;data utility;data quality assessment</field>
  <field name="abstract">Multimodal fusion is more and more widely used in the field of machine learning, but it faces a prominent problem in practical application: data utility is not stable. The data of different modalities may be missing and noisy randomly, which will interfere the machine learning model of multi-modal fusion. Most of the existing multi-modal fusion methods neglect data utility problems or only adopt simple data denoising methods to improve data utility. To solve the problem of unstable data utility, we propose a data selection method based on the evaluation of data utility. By training a special machine learning model, the optimal modal combination is predicted according to the quality evaluation of multi-modal data samples to accomplish the dynamic selection of data modalities. The experimental results show that the proposed method can effectively improve the accuracy of multi-modal recognition under low data utility.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00120</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">OPEN DATA MARKET ARCHITECTURE AND FUNCTIONAL COMPONENTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Demchenko, Yuri and Cushing, Reggie and Los, Wouter and Grosso, Paola and de Laat, Cees and Gommans, Leon</field>
  <field name="keywords">Economics;Cloud computing;Data models;Contracts;Big Data;Computer architecture;Open Data Market;Data Marketplace;Trusted Data Market;Industrial Data Space;Data Economics;STREAM Data Properties</field>
  <field name="abstract">This paper discusses the principles of organisation and infrastructure components of Open Data Markets (ODM) that would facilitate secure and trusted data exchange between data market participants, and other cooperating organisations. The paper provides a definition of the data properties as economic goods and identifies the generic characteristics of ODM as a Service. This is followed by a detailed description of the generic data market infrastructure that can be provisioned on demand for a group of cooperating parties. The proposed data market infrastructure and its operation are employing blockchain technologies for securing data provenance and providing a basis for data monetisation. Suggestions for trust management and data quality assurance are discussed.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/HPCS48598.2019.9188195</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A FRAMEWORK FOR IDENTIFYING AND PRIORITIZING DATA ANALYTICS OPPORTUNITIES IN ADDITIVE MANUFACTURING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Park, Hyunseop and Ko, Hyunwoong and Lee, Yung-Tsun T. and Cho, Hyunbo and Witherell, Paul</field>
  <field name="keywords">Decision making;Solid modeling;Mechanical variables measurement;Electric variables measurement;Shape measurement;Data analysis;Analytical models;Data analytics;opportunity identification and prioritization;architecture;additive manufacturing</field>
  <field name="abstract">Many industries, including manufacturing, are adopting data analytics (DA) in making decisions to improve quality, cost, and on-time delivery. In recent years, more research and development efforts have applied DA to additive manufacturing (AM) decision-making problems such as part design and process planning. Though there are many AM decision-making problems, not all benefit greatly from DA. This may be due to insufficient AM data, unreliable data quality, or the fact that DA is not cost effective when it is applied to some AM problems. This paper proposes a framework to investigate DA opportunities in a manufacturing operation, specifically AM. The proposed framework identifies and prioritizes AM potential opportunities where DA can make impact. The proposed framework is presented in a five-tier architecture, including value, decision-making, data analytics, data, and data source tiers. A case study is developed to illustrate how the proposed framework identifies DA opportunities in AM.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9006489</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">APPLICATION OF DATA MINING TECHNOLOGY IN THE RECALL OF DEFECTIVE AUTOMOBILE PRODUCTS IN CHINA ——A TYPICAL CASE OF THE CONSTRUCTION OF DIGITAL CHINA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Li, Song and Ning, Sun and Yezhou, Yao and Jingjing, Tian and Wenxue, Zhang and Liang, Chi</field>
  <field name="keywords">Automobiles;Safety;Big Data;Government;Personnel;Data mining;automobile recall;data mining;information cluster</field>
  <field name="abstract">According to multisource quality safety data of defective automobile products, key quality safety factors of defective automobile products are extracted, a defect information indicator system for automobile products is systematically constructed and a correlated graph is established between quality safety factors. Based on the optimization and correlation of the quality safety factor indicator system, Big Data technology is used to design a data structure for multisource quality safety information cluster, develop a data platform for the defect information analysis of automobile products and achieve information clustering and correlation analysis based on multisource quality safety data, providing technical support for the recall management of defective automobile products.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/IICSPI48186.2019.9095877</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RESEARCH ON WEB SERVICE SELECTION BASED ON PARALLEL SKYLINE ALGORITHM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xinmei, Liang and Luqin</field>
  <field name="keywords">Sparks;Web services;Clustering algorithms;Big Data;Quality of service;Computer science;Parallel processing;Skyline;big data;Spark;Hadoop;parallelization</field>
  <field name="abstract">With the continuous development of the Internet, there are many web services with the same functional attributes but different functional attributes. It is urgent to find a web service that can satisfy itself quickly and efficiently from the massive web service data. This paper improves the traditional Skyline algorithm, divides the web service data set into regions, greatly reduces the data points without dominance, and saves memory usage. The improved Skyline algorithm can significantly improve the speed of Web service selection. However, the improved Skyline algorithm will still have insufficient computing resources when processing massive Web service data, resulting in a significant decrease in computing speed and even computer jam. In view of the above situation, this paper will parallelize the improved Skyline algorithm and parallelize the improved Skyline algorithm through the Spark platform. Experiments show that the parallelized Skyline algorithm can better handle massive Web service data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICEIEC.2019.8784671</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">FOUNDATIONS OF DATA QUALITY ASSURANCE FOR IOT-BASED SMART APPLICATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Togneri, Rodrigo and Camponogara, Glauber and Soininen, Juha-Pekka and Kamienski, Carlos</field>
  <field name="keywords">Irrigation;Quality assurance;Data integrity;Signal processing;Feature extraction;Robustness;Object recognition;Data quality;internet of things;smart applications;precision irrigation</field>
  <field name="abstract">Most current scientific and industrial efforts in IoT are geared towards building integrated platforms to finally realize its potential in commercial scale applications. The IoT and Big Data contemporary context brings a number of challenges, such as providing quality assurance (defined by availability and veracity) for sensor data. Traditional signal processing approaches are no longer sufficient, requiring combined approaches in both architectural and analytical layers. This paper proposes a discussion on the adequate foundations of a new general approach aimed at increasing robustness and antifragility of IoT-based smart applications. In addition, it shows results of preliminary experiments with real data in the context of precision irrigation using multivariate methods to identify relevant situations, such as sensor failures and the mismatch of contextual sensor information due to different spatial granularities capture. Our results provide initial indications of the adequacy of the proposed framework.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/LATINCOM48065.2019.8937930</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TRUTHFUL MOBILE CROWDSENSING FOR STRATEGIC USERS WITH PRIVATE DATA QUALITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Gong, Xiaowen and Shroff, Ness B.</field>
  <field name="keywords">Task analysis;Transmitters;Signal to noise ratio;Data integrity;Sensors;IEEE transactions;Big Data;Crowdsensing;truthful incentive mechanism;data quality</field>
  <field name="abstract">Mobile crowdsensing has found a variety of applications (e.g., spectrum sensing, environmental monitoring) by leveraging the “wisdom” of a potentially large crowd of mobile users. An important metric of a crowdsensing task is data accuracy, which relies on the data quality of the participating users' data (e.g., users' received SNRs for measuring a transmitter's transmit signal strength). However, the quality of a user can be its private information (which, e.g., may depend on the user's location) that it can manipulate to its own advantage, which can mislead the crowdsensing requester about the knowledge of the data's accuracy. This issue is exacerbated by the fact that the user can also manipulate its effort made in the crowdsensing task, which is a hidden action that could result in the requester having incorrect knowledge of the data's accuracy. In this paper, we devise truthful crowdsensing mechanisms for Quality and Effort Elicitation (QEE), which incentivize strategic users to truthfully reveal their private quality and truthfully make efforts as desired by the requester. The QEE mechanisms achieve the truthful design by overcoming the intricate dependency of a user's data on its private quality and hidden effort. Under the QEE mechanisms, we show that the crowdsensing requester's optimal (RO) effort assignment assigns effort only to the best user that has the smallest “virtual valuation”, which depends on the user's quality and the quality's distribution. We also show that, as the number of users increases, the performance gap between the RO effort assignment and the socially optimal effort assignment decreases, and converges to 0 asymptotically. We further discuss some extensions of the QEE mechanisms. Simulation results demonstrate the truthfulness of the QEE mechanisms and the system efficiency of the RO effort assignment.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/TNET.2019.2934026</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RESEARCH AND ANALYSIS VALIDATION OF DATA FUSION TECHNOLOGY BASED ON EDGE COMPUTING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Gao, Jian and Zhen, Yan and Bai, Huifeng and Huo, Chao and Wang, Dongshan and Zhang, Ganghong</field>
  <field name="keywords">Data integration;Power grids;Kalman filters;Monitoring;Big Data;Data models;Distributed databases;smart grid;multi-source data;edge computing;filtering algorithm</field>
  <field name="abstract">Based on the smart grid as the research background, this paper responded to the massive multi-source data processing requirements of the smart grid, and combined with distributed computing to provide the edge of the solution, aiming at the existing data of electric power equipment state monitoring data in noise and redundant data problems. A distributed kalman filter algorithm based on edge of computing was put forward. In this algorithm, event decision strategy was added to the data processing and transmission process of edge computing terminal to control the communication times between nodes and terminals in an event-driven way. Meanwhile, redundant data and data interfered by noise were reduced through the processing of the algorithm, so as to ensure the data quality and improve the fusion efficiency. Finally, the effectiveness of the method was verified by the analysis of compression efficiency and data fusion time.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICCC47050.2019.9064032</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WIND POWER GENERATION FORECASTING AND DATA QUALITY IMPROVEMENT BASED ON BIG DATA WITH MULTIPLE TEMPORAL-SPATUAL SCALE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Qiao, Lin and Chen, Shuo and Bo, Jue and Liu, Sai and Ma, Guiwei and Wang, Haixin and Yang, Junyou</field>
  <field name="keywords">Wind speed;Support vector machines;Biological neural networks;Indexes;Wind turbines;Mathematical model</field>
  <field name="abstract">Wind energy is one of the renewable energy sources with a large number of installations in the world. The accuracy of power generation prediction using wind speed data severely challenges the regulation and safe operation of power system. Since there are many time points in the dispatching strategy of power system, which is related to the area condition. It is of great significance for power grid dispatching to be able to timely and accurately predict the generation capacity of wind turbines in a certain period. Due to the randomness and intermittency of wind speed, the accuracy of data quality will be influenced greatly. In this paper, a neural network algorithm based on combination of back propagation (BP) and Newton interpolation mathematical function method is proposed to effectively process wind speed data, so as to predict power generation. BP neural network is a kind of multi-layer feedforward neural network including a hidden layer, which can solve the learning problem of hidden layer connection weight in a multi-layer network. From the perspective of space scale, this paper studies different wind speed data at different heights in the same area. Research results show: compared with the traditional support vector machine method, the accuracy with the proposed method is improved by 3.1%.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICEI.2019.00104</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">QUALITY MANAGEMENT OF WORKERS IN AN IN-HOUSE CROWDSOURCING-BASED FRAMEWORK FOR DEDUPLICATION OF ORGANIZATIONS’ DATABASES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Saberi, Morteza and Hussain, Omar Khadeer and Chang, Elizabeth</field>
  <field name="keywords">Crowdsourcing;Databases;Task analysis;Object recognition;Monitoring;Error analysis;Big Data;Quality management;quality control;data quality;duplicate detection;in-house crowdsourcing</field>
  <field name="abstract">While organizations in the current era of big data are generating massive volumes of data, they also need to ensure that its quality is maintained for it to be useful in decision-making purposes. The problem of dirty data plagues every organization. One aspect of dirty data is the presence of duplicate data records that negatively impact the organization's operations in many ways. Many existing approaches attempt to address this problem by using traditional data cleansing methods. In this paper, we address this problem by using an in-house crowdsourcing-based framework, namely, DedupCrowd. One of the main obstacles of crowdsourcing-based approaches is to monitor the performance of the crowd, by which the integrity of the whole process is maintained. In this paper, a statistical quality control-based technique is proposed to regulate the performance of the crowd. We apply our proposed framework in the context of a contact center, where the Customer Service Representatives are used as the crowd to assist in the process of deduplicating detection. By using comprehensive working examples, we show how the different modules of the DedupCrowd work not only to monitor the performance of the crowd but also to assist in duplicate detection.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2924979</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">REGULARIZED OPERATING ENVELOPE WITH INTERPRETABILITY AND IMPLEMENTABILITY CONSTRAINTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wang, Qiyao and Wang, Haiyan and Gupta, Chetan and Serita, Susumu</field>
  <field name="keywords">Genetic algorithms;Machine learning;Oils;Optimization;Search problems;Big Data;Quality assessment;Operating envelope;Genetic algorithm;Penalty approach;Generalization</field>
  <field name="abstract">Operating envelope is an important concept in industrial operations. Accurate identification for operating envelope can be extremely beneficial to stakeholders as it provides a set of operational parameters that optimizes some key performance indicators (KPI) such as product quality, operational safety, equipment efficiency, environmental impact, etc. Given the importance, data-driven approaches for computing the operating envelope are gaining popularity. These approaches typically use classifiers such as support vector machines, to set the operating envelope by learning the boundary in the operational parameter spaces between the manually assigned `large KPI' and `small KPI' groups. One challenge to these approaches is that the assignment to these groups is often ad-hoc and hence arbitrary. However, a bigger challenge with these approaches is that they don't take into account two key features that are needed to operationalize operating envelopes: (i) interpretability of the envelope by the operator and (ii) implementability of the envelope from a practical standpoint. In this work, we propose a new definition for operating envelope which directly targets the expected magnitude of KPI (i.e., no need to arbitrarily bin the data instances into groups) and accounts for the interpretability and the implementability. We then propose a regularized `GA +penalty' algorithm that outputs an envelope where the user can tradeoff between bias and variance. The validity of our proposed algorithm is demonstrated by two sets of simulation studies and an application to a real-world challenge in the mining processes of a flotation plant.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/BigData47090.2019.9005484</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CRACKSENSE: A CROWDSOURCING BASED URBAN ROAD CRACK DETECTION SYSTEM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wang, Liang and Yang, Congying and Yu, Zhiwen and Liu, Yimeng and Wang, Zhu and Guo, Bin</field>
  <field name="keywords">Roads;Crowdsourcing;Sensors;Estimation;Data models;Data integrity;Three-dimensional displays;Mobile crowdsourcing;road crack detection;image processing;sensors</field>
  <field name="abstract">As a common road surface distress, cracks pose a serious threat to road infrastructure and traffic safety in cities today. Consequently, road crack detection is considered as an essential step for effective road maintenance and road structure sustainability. However, due to the high cost incurred by dedicated devices and professional operators, it is impossible for existing systems to achieve universal spatiotemporal coverage across citywide road networks. To fill this gap, in this paper, we present the CrackSense, a mobile crowdsourcing based system to detect urban road crack and estimate its damage degree. Specifically, for the heterogeneous crack data, we put forward a crowdsourcing data quality evaluation and selection mechanism. And then, by utilizing the multi-source sensing data aggregation, we propose tow algorithms, namely RCTR and RCDE, to recognize road crack types, i.e., horizontal crack, vertical crack, and net crack, and estimate the crack damage degree, respectively. We implement the system and develop a smartphone APP for mobile users. By conducting intensive experiments and field study, the results demonstrate the accuracy and effectiveness of our proposed approaches.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00188</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">LEARNING THE EVOLUTION REGULARITIES FOR BIGSERVICE-ORIENTED ONLINE RELIABILITY PREDICTION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wang, Hongbing and Wang, Lei and Yu, Qi and Zheng, Zibin</field>
  <field name="keywords">Reliability;Time series analysis;Web services;Computer network reliability;Meteorology;Big data;Quality of service;Temporal evolution regularities;online reliability prediction;big service;convolutional neural networks</field>
  <field name="abstract">Service computing is an emerging technology in System of Systems Engineering (SoS Engineering or SoSE), which regards a System as a Service, and aims at constructing a robust and value-added complex system by outsourcing external component systems through service composition. The burgeoning Big Service computing just covers the significant challenges in constructing and maintaining a stable service-oriented SoS. A service-oriented SoS runs under a volatile and uncertain environment. As a step toward big service, service fault tolerance (FT) can guarantee the run-time quality of a service-oriented SoS. To successfully deploy FT in an SoS, online reliability time series prediction, which aims at predicting the reliability in near future for a service-oriented SoS arises as a grand challenge in SoS research. In particular, we need to tackle a number of big data related issues given the large and fast increasing size of the historical data that will be used for prediction purpose. The decision-making of prediction solution space be more complex. To provide highly accurate prediction results, we tackle the prediction challenges by identifying the evolution regularities of component systems' running states via different machine learning models. We present in this paper the motifs-based Dynamic Bayesian Networks (or m_DBNs) to perform one-step-ahead online reliability time series prediction. We also propose a multi-steps trajectory DBNs (or multi_DBNs) to further improve the accuracy of future reliability prediction. Finally, a Convolutional Neural Networks (CNN)-based prediction approach is developed to deal with the big data challenges. Extensive experiments conducted on real-world Web services demonstrate that our models outperform other well-known approaches consistently.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/TSC.2016.2633264</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INFOXICATION IN THE GENOMIC DATA ERA AND IMPLICATIONS IN THE DEVELOPMENT OF INFORMATION SYSTEMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Palacio, Ana León and López, Óscar Pastor</field>
  <field name="keywords">Bioinformatics;Genomics;Diseases;DNA;Task analysis;Databases;Infoxication;Genomics;Information Systems;SILE method</field>
  <field name="abstract">We live in an age where data acquisition is no longer a problem and the real challenge is how to determine which information is the right one to take important and sometimes difficult decisions. Infoxication (also known as Infobesity or Information Overload) is a term used to describe the difficulty of adapting to new situations and effectively making decisions when there is too much information to manage. With the advent of the Big Data, infoxication is affecting critical domains such as Health Sciences, where tough decisions for patient's health is being taken every day based on heterogeneous, unconnected and sometimes conflicting information. In order to understand the magnitude of the challenge, based on the information publicly available about the genetic causes of the disease and using data quality assessment techniques, we performed an exhaustive analysis of the DNA variations that have been associated to the risk of suffering migraine headache. The same analysis has been repeated 8 months after, and the results have allowed us to exemplify i) how fragile is the information in this domain, ii) the difficulty of finding repositories of contrasted and reliable data, and iii) the need to have information systems that, far from integrating and storing huge volumes of data, are able to support the decision-making process by providing mechanisms agile and flexible enough to be able to adapt to the changing user needs.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/RCIS.2019.8877003</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">REAL-TIME VESSEL TRAJECTORY DATA-BASED COLLISON RISK ASSESSMENT IN CROWDED INLAND WATERWAYS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Feng, Zikun and Yang, Haojie and Li, Xinyi and Li, Yan and Liu, Zhao and Liu, Ryan Wen</field>
  <field name="keywords">Marine vehicles;Artificial intelligence;Interpolation;Trajectory;Navigation;Rivers;Accidents;Ship domain;trajectory data;ship collision risk;automatic identification system;Monte Carlo method</field>
  <field name="abstract">With the rapid development of maritime industries, the vessel traffic density has been gradually increased leading to increasing the potential risk of ship collision accidents in crowded inland waterways. It will bring negative effects on human life safety and enterprise economy. Therefore, it is of vital significance to study the risk of ship collision in practical applications. This paper proposes to quantitatively estimate the ship collision risk based on ship domain modeling and real-time vessel trajectory data. In particular, the trajectory data quality is improved using the cubic spline interpolation method. We assume that the ship collision risk is highly related to the cross areas of ship domains between different ships, which are computed via the Monte Carlo probabilistic algorithm. For the sake of better understanding, the kernel density estimation method is adopted to visually generate the ship collision risk in maps. Experimental results have illustrated the effectiveness of the proposed method in crowded inland waterways.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICBDA.2019.8712843</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ACTIVE DISTRIBUTION NETWORK STATE ESTIMATION ALGORITHM BASED ON DECISION TREE OF SELF-IDENTIFICATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ding, Jian and Ma, Chunlei and Fu, Bin and Liu, Bing</field>
  <field name="keywords">DATA CHECK;DATA QUALITY IDENTIFICATION;DECISION TREE;STATE ESTIMATION;BIG DATA</field>
  <field name="abstract">Under the background of active distribution network, this paper proposes a state estimation algorithm of managing analysis data to solve the problem of big data, data missing and complex analysis. This paper proposes an active distribution network state estimation algorithm based on decision tree self-identification. Setting appropriate quality weight of big data based on the check rules different from traditional single-phase currents. Data in the input state estimation model is better compatible by estimating the pre-processed data, classifying and correcting the data including voltage and current. Moreover, on the premise of lacking of distributed energy measurement devices, this paper establishes a state estimation model for distributed power, which is used to correct the default data of distributed energy and improve the quality of input data in wind power and photovoltaic. The method can be verified in the actual example. Compared with the traditional state estimation, the active distribution network state estimation algorithm based on decision tree self-identification has better estimation effect and faster iteration speed. Therefore, the proposed algorithm can be effectively applied to the current state estimation of large-scale distributed energy access.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1049/cp.2019.0490</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SPATIO-TEMPORAL VESSEL TRAJECTORY SMOOTHING USING EMPIRICAL MODE DECOMPOSITION AND WAVELET TRANSFORM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Li, Xinyi and Feng, Zikun and Li, Yan and Liu, Zhao and Liu, Ryan Wen</field>
  <field name="keywords">Trajectory;Wavelet transforms;Noise reduction;Artificial intelligence;Marine vehicles;Navigation;Empirical mode decomposition;wavelet transform;data denoising;automatic identification system;trajectory data</field>
  <field name="abstract">The Automatic Identification System (AIS) has attracted increasing attention in recent years for its superior properties in ocean engineering and maritime management. The spatio-temporal vessel trajectory data is highly related to the received AIS data. However, the AIS raw data often suffer from undesirable noise during signal acquisition and analog-to-digital conversion. To improve AIS-based vessel trajectory data quality, we propose to develop a vessel trajectory smoothing method by combining empirical mode decomposition (EMD) with wavelet transform. In particular, EMD is introduced to decompose the original vessel trajectory data into several sub-trajectories. The EMD decomposition is able to assist in enhancing the robustness of trajectory smoothing. Wavelet transform is directly adopted to smooth the decomposed sub-trajectories. The final high-quality trajectory is obtained by combining the smoothed sub-trajectories in this work. The proposed method has the capacity of removing the unwanted noise while preserving the important trajectory features. Numerous experiments have illustrated the superior smoothing performance of the proposed combined method.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICBDA.2019.8713242</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SAFEGUARDING DATA INTEGRITY BY CLUSTER-BASED DATA VALIDATION NETWORK</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wallis, Kevin and Schillinger, Fabian and Reich, Christoph and Schindelhauer, Christian</field>
  <field name="keywords">Logic gates;Data integrity;Task analysis;Maintenance engineering;Blockchain;Industrial Internet of Things;Internet of Things;Data Validation;Cluster-Based Data Validation;Big Data</field>
  <field name="abstract">Ensuring data quality is central to the digital transformation in industry. Business processes such as predictive maintenance or condition monitoring can be implemented or improved based on the available data. In order to guarantee high data quality, a single data validation system are usually used to validate the production data for further use. However, using a single system allows an attacker only to perform one successful attack to corrupt the whole system. We present a new approach in which a data validation system using multiple different validators minimizes the probability of success for the attacker. The validators are arranged in clusters based on their properties. For a validation process, a challenge is given that specifies which validators should perform the current validation. Validation results from other validators are dropped. This ensures that even for more than half of the validators being corrupted anomalies can be detected during the validation process.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/WorldS4.2019.8904039</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SELECTING SENSING LOCATION LEVERAGING SPATIAL AND CROSS-DOMAIN CORRELATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Chang, Huijuan and Yu, Zhiyong and Yu, Zhiwen and Guo, Bin</field>
  <field name="keywords">Sensors;Correlation;Air quality;Data models;Estimation;Data integrity;Task analysis;Active learning;location selection;kriging interpolation;regression tree</field>
  <field name="abstract">In environmental monitoring applications, selecting appropriate locations to sense is important relating to data quality and Sensing cost. This paper addresses the challenge by collecting data from a subset of locations, then leveraging the spatial and cross-domain correlations to deduce data of other locations, thus can obtain acceptable data quality with lower sensing cost. Referring to active learning, the proposed framework is constructed by two types modules (i.e., estimators and selectors) and a cyclic process of estimating and selecting. Estimators based on kriging interpolation and regression tree are implemented, and their corresponding selectors are designed. We evaluate the effectiveness of the framework by taking air quality sensing as an example. Results show that to reach data quality of about 25% MAPE, the framework only needs 15% locations, while random selector needs 25% locations.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00149</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A BLOCKCHAIN AND AUTOML APPROACH FOR OPEN AND AUTOMATED CUSTOMER SERVICE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Li, Zhi and Guo, Hanyang and Wang, Wai Ming and Guan, Yijiang and Barenji, Ali Vatankhah and Huang, George Q. and McFall, Kevin S. and Chen, Xin</field>
  <field name="keywords">Customer services;Blockchain;Companies;Machine learning;Personnel;Data models;Smart contracts;Automated customer service;automated machine learning (AutoML);blockchain;open customer service</field>
  <field name="abstract">Customer service is transforming from traditional manual service toward automated service, which utilizes different computational informatics to achieve a higher efficient and quality services. Automated customer service requires big data and expertise in data analysis as prerequisites. However, many companies, especially small and medium enterprises, do not have sufficient data and experience due to their limited scale and resources. They need to rely on third parties, and this reliance results in the lack of development of core customer service competency. In order to overcome these challenges, an open and automated customer service platform based on Internet of things (IoT), blockchain, and automated machine learning (AutoML) is proposed. The data are gathered with the use of IoT devices during the customer service. An open but secured environment to achieve data trading is ensured by using blockchain. AutoML is adopted to automate the data analysis processes for reducing the reliance of costly experts. The proposed platform is analyzed through use case evaluation. A prototype system has also been developed and evaluated. The simulation results show that our platform is scalable and efficient.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/TII.2019.2900987</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SURVEY OF TASK ALLOCATION: CONTRASTIVE PERSPECTIVES FROM WIRELESS SENSOR NETWORKS AND MOBILE CROWDSENSING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Guo, Wenzhong and Zhu, Weiping and Yu, Zhiyong and Wang, Jiangtao and Guo, Bin</field>
  <field name="keywords">Sensors;Task analysis;Wireless sensor networks;Resource management;Data integrity;Wireless communication;Mobile handsets;Mobile crowdsensing (MCS);task allocation;wireless sensor networks (WSNs)</field>
  <field name="abstract">Wireless sensor networks (WSNs) and mobile crowdsensing (MCS) are two important paradigms in urban dynamic sensing. In both sensing paradigms, task allocation is a significant problem, which may affect the completion quality of sensing tasks. In this paper, we give a survey of task allocation in WSNs and MCS from the contrastive perspectives in terms of data quality and sensing cost, which help to better understand related objectives and strategies. We first analyze the different characteristics of two sensing paradigms, which may lead to difference in task allocation issues or strategies. Then, we present some common issues in task allocation with objectives in data quality and sensing cost. Furthermore, we provide reviews of unique task allocation issues in MCS according to its new characteristics. Finally, we identify some potential opportunities for the future research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2896226</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ACHIEVING ACCURATE UBIQUITOUS SLEEP SENSING WITH CONSUMER WEARABLE ACTIVITY WRISTBANDS USING MULTI-CLASS IMBALANCED CLASSIFICATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liang, Zilu and Chapa Martell, Mario Alberto</field>
  <field name="keywords">Sleep;Training;Machine learning algorithms;Classification algorithms;Standards;Vegetation;Heart rate;wearable;data quality;sleep;machine learning;Fitbit</field>
  <field name="abstract">Consumer activity wristbands such as Fitbit provide an affordable method for ubiquitous sleep sensing in daily settings. These devices are also increasingly used in scientific studies as measurement tools of sleep outcomes. Nevertheless, the accuracy of Fitbit has raised wide concern. In this paper, we explore the feasibility of applying machine learning to improve the quality of Fitbit sleep data. The problem of interest was formulated into a multiclass imbalanced classification problem. We examined the performance of different combinations of seven machine learning algorithms and three resampling techniques. The preliminary results showed that the accuracy in detecting wakefulness and light sleep was improved by up to 43% and 44% respectively compared to the proprietary algorithm of Fitbit. Our future work will focus on improving the overall accuracy of the classification models in detecting all sleep stages.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00143</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PROBABILISTIC LINGUISTIC VIKOR METHOD BASED ON TODIM FOR RELIABLE PARTICIPANT SELECTION PROBLEM IN MOBILE CROWDSENSING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Huang, Chao and Lin, Mingwei and Chen, Riqing</field>
  <field name="keywords">Linguistics;Sensors;Crowdsensing;Task analysis;Decision making;Probabilistic logic;Cloud computing;Mobile crowdsensing, Probabilistic linguistic term set, Participants ranking, TODIM, VIKOR</field>
  <field name="abstract">In the mobile crowdsensing systems, the participants of great variety and diversity voluntarily submit their sensing data. Evaluating the participants and ranking them is a critical problem that should be solved to ensure the data quality. In this paper, we introduce the concept of probabilistic linguistic term sets (PLTSs) to model the group preference information during the process of ranking candidate participants and then propose novel VIKOR methods based on TODIM for solving the process of ranking reliable participants and selecting the best one in the mobile crowdsensing system. This proposed methods combine the advantages from the VIKOR method and TODIM method. To show the implementation process of evaluating participants and selecting the best one under the PLTS information context, a practical case is given to verify the feasibility of the proposed methods. Compared with the existing decision making methods, the proposed methods show their effectiveness.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00108</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A STUDY ON QUALITY PREDICTION FOR SMART MANUFACTURING BASED ON THE OPTIMIZED BP-ADABOOST MODEL</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Caihong, Zhou and Zengyuan, Wu and Chang, Liu</field>
  <field name="keywords">smart manufacturing;big data;quality prediction;BP neural network;AdaBoost algorithm;BP-AdsysBoost model</field>
  <field name="abstract">To accurately predict the product quality in smart manufacturing, this paper designs the BP-AdsysBoost model on the basis of BP neural network and AdaBoost algorithm. The BP-AdsysBoost model considers both the data characteristics and the technology advantages, which pays more attentions to the unqualified products wrongly predicted. To further examine the model, the 110560 data of smart manufacturing from German BOSCH company is used for this research. The proposed BP-AdsysBoost model is compared with the BP neural network and the unmodified BP-AdaBoost model according to prediction performance. The results show that the BP-AdsysBoost model has significant advantages in prediction accuracy and FDR, which proves its satisfied prediction ability for product quality in smart manufacturing.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/SMILE45626.2019.8965303</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DIFFERENTIAL PRIVACY-BASED INDOOR LOCALIZATION PRIVACY PROTECTION IN EDGE COMPUTING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhang, Xuejun and Chen, Qian and Peng, Xiaohui and Jiang, Xinlong</field>
  <field name="keywords">Privacy;Training;Fingerprint recognition;Edge computing;Cloud computing;Indoor localization, Differential privacy, Privacy preserving, Edge computing.</field>
  <field name="abstract">With the popularity of smart devices and the widespread use of the Wi-Fi-based indoor localization, edge computing is becoming the mainstream paradigm of processing massive sensing data to acquire indoor localization service. However, these data which were conveyed to train the localization model unintentionally contain some sensitive information of users/devices, and were released without any protection may cause serious privacy leakage. To solve this issue, we propose a lightweight differential privacy-preserving mechanism for the edge computing environment. We extend ε-differential privacy theory to a mature machine learning localization technology to achieve privacy protection while training the localization model. Experimental results on multiple real-world datasets show that, compared with the original localization technology without privacy-preserving, our proposed scheme can achieve high accuracy of indoor localization while providing differential privacy guarantee. Through regulating the value of ε, the data quality loss of our method can be controlled up to 8.9% and the time consumption can be almost negligible. Therefore, our scheme can be efficiently applied in the edge networks and provides some guidance on indoor localization privacy protection in the edge computing.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00125</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS DEEP LEARNING-BASED DETECTION SCHEME WITH RAW ECG SIGNAL FOR WEARABLE TELEHEALTH SYSTEMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhao, Peng and Quan, Dekui and Yu, Wei and Yang, Xinyu and Fu, Xinwen</field>
  <field name="keywords">Electrocardiography;Feature extraction;Biomedical monitoring;Sensors;Cloud computing;Computational modeling;Data analysis</field>
  <field name="abstract">The electrocardiogram (ECG) signal, as one of the most important vital signs, can provide indications of many heart-related diseases. Nonetheless, in the case of telehealth context, the automated analysis and accurate detection of ECG signals remain unsolved issues, because the poor data quality collected by the wearable devices and unprofessional users further increases the complexity of hand-crafted feature extraction, ultimately affecting the efficiency of feature extraction and the detection accuracy. To address this issue and improve the detection accuracy, in this paper we present a novel detection scheme with the raw ECG signal in wearable telehealth system. Our system benefits from the concept of big data, sensing and pervasive computing and the emerging deep learning technology. In particular, a Deep Heartbeat Classification (DHC) scheme is proposed to analyze the ECG signal for arrhythmia detection. Distinct from existing solutions, the detection model in DHC can be trained directly on the raw ECG signal without hand-crafted feature extraction. A cloud-based prototypical system is also designed and implemented with the functions of data acquisition, wireless transmission, back-end data management, and ECG detection. The experimental results demonstrate that our prototypical system is feasible and effective in real-world practice, and extensive experimentation based on the MIT-BIH database demonstrates that the proposed DHC scheme outperforms baseline schemes.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICCCN.2019.8847069</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EXPLOITING USER TAGGING FOR WEB SERVICE CO-CLUSTERING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liang, Tingting and Chen, Yishan and Gao, Wei and Chen, Ming and Zheng, Meilian and Wu, Jian</field>
  <field name="keywords">Web services;Tagging;Search engines;Clustering algorithms;Task analysis;Feature extraction;Web service;WSDL documents clustering;co-clustering;tag recommendation</field>
  <field name="abstract">We propose a novel Web services clustering framework by considering the word distribution of WSDL documents and tags. Typically, tags are annotated to Web services by users for organization. In this paper, four strategies are proposed to integrate the tagging data and WSDL documents in the process of service clustering. Tagging data is inherently uncontrolled, ambiguous, and overly personalized. Two tag recommendation approaches are proposed to improve the tagging data quality and service clustering performance. Comprehensive experiments demonstrate the effectiveness of the proposed framework using a real-world dataset.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2950355</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARD EMPIRICALLY INVESTIGATING NON-FUNCTIONAL REQUIREMENTS OF IOS DEVELOPERS ON STACK OVERFLOW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ahmad, Arshad and Feng, Chong and Li, Kan and Asim, Syed Mohammad and Sun, Tingting</field>
  <field name="keywords">Software;Market research;Data mining;Application programming interfaces;Mobile communication;Mobile applications;Technological innovation;Non-functional requirements (NFRs);quality requirements;iOS;Latent Dirichlet allocation (LDA);Stack Overflow</field>
  <field name="abstract">Context: Mobile application developers are getting more concerned due to the importance of quality requirements or non-functional requirements (NFR) in software quality. Developers around the globe are actively asking a question(s) and sharing solutions to the problems related to software development on Stack Overflow (SO). The knowledge shared by developers on SO contains useful information related to software development such as feature requests (functional/non-functional), code snippets, reporting bugs or sentiments. Extracting the NFRs shared by iOS developers on programming Q&A website SO has become a challenge and a less researched area. Objective: To identify and understand the real problems, needs, trends, and the critical NFRs or quality requirements discussed on Stack Overflow related to iOS mobile application development. Method: We extracted and used only the iOS posts data of SO. We applied the well-known statistical topical model Latent Dirichlet Allocation (LDA) to identify the main topics in iOS posts on SO. Then, we labeled the extracted topics with quality requirements or NFRs by using the wordlists to assess the trend, evolution, hot and unresolved NFRs in all iOS discussions. Results: Our findings revealed that the highly frequent topics the iOS developers discussed are related to usability, reliability, and functionality followed by efficiency. Interestingly, the most problematic areas unresolved are also usability, reliability, and functionality though followed by portability. Besides, the evolution trend of each of the six different quality requirements or NFRs over time is depicted through comprehensive visualization. Conclusion: Our first empirical investigation on approximately 1.5 million iOS posts and comments of SO gives insight on comprehending the NFRs in iOS application development through the lens of real-world practitioners.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ACCESS.2019.2914429</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE EXPONENTIATED PHASE MEASUREMENT, AND OBJECTIVE-FUNCTION HYBRIDIZATION FOR ADJOINT WAVEFORM TOMOGRAPHY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yuan, Yanhua O and Bozdağ, Ebru and Ciardelli, Caio and Gao, Fuchun and Simons, F J</field>
  <field name="keywords">Inverse theory;Time-series analysis;Seismic tomography</field>
  <field name="abstract">Seismic tomography has arrived at the threshold of the era of big data. However, how to extract information optimally from every available time-series remains a challenge; one that is directly related to the objective function chosen as a distance metric between observed and synthetic data. Time-domain cross-correlation and frequency-dependent multitaper traveltime measurements are generally tied to window selection algorithms in order to balance the amplitude differences between seismic phases. Even then, such measurements naturally favour the dominant signals within the chosen windows. Hence, it is difficult to select all usable portions of seismograms with any sort of optimality. As a consequence, information ends up being lost, in particular from scattered waves. In contrast, measurements based on instantaneous phase allow extracting information uniformly over the seismic records without requiring their segmentation. And yet, measuring instantaneous phase, like any other phase measurement, is impeded by phase wrapping. In this paper, we address this limitation by using a complex-valued phase representation that we call ‘exponentiated phase’. We demonstrate that the exponentiated phase is a good substitute for instantaneous-phase measurements. To assimilate as much information as possible from every seismogram while tackling the non-linearity of inversion problems, we discuss a flexible hybrid approach to combine various objective functions in adjoint seismic tomography. We focus on those based on the exponentiated phase, to take into account relatively small-magnitude scattered waves; on multitaper measurements of selected surface waves; and on cross-correlation measurements on specific windows to select distinct body-wave arrivals. Guided by synthetic experiments, we discuss how exponentiated-phase, multitaper and cross-correlation measurements, and their hybridization, affect tomographic results. Despite their use of multiple measurements, the computational cost to evaluate gradient kernels for the objective functions is scarcely affected, allowing for issues with data quality and measurement challenges to be simultaneously addressed efficiently.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1093/gji/ggaa063</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ONTOLOGY: FOOTSTONE FOR STRONG ARTIFICIAL INTELLIGENCE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu</field>
  <field name="keywords">ontology, artificial intelligence, biomedicine, big data</field>
  <field name="abstract">Abstract
In the past ten years, the application of artificial intelligence (AI) in biomedicine has increased rapidly, which roots in the rapid growth of biomedicine data, the improvement of computing performance, and the development of deep learning methods. At present, there are great difficulties in front of AI for solving complex and comprehensive medical problems. Ontology can play an important role in how to make machines have stronger intelligence and has wider applications in the medical field. By using ontologies, (meta) data can be standardized so that data quality is improved and more data analysis methods can be introduced, data integration can be supported by the semantics relationships which are specified in ontologies, and effective logic expression in nature language can be better understood by machine. This can be a pathway to stronger AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and Terminology was held in Beijing in autumn 2019, with the theme “Making Machine Understand Data”. The success of this conference further improves the development of ontology in the field of biomedical information in China, and will promote the integration of Chinese ontology research and application with the international standards and the findability, accessibility, interoperability, and reusability(FAIR) Data Principle.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.24920/003701</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">POTENTIAL PROBLEM DATA TAGGING: AUGMENTING INFORMATION SYSTEMS WITH THE CAPABILITY TO DEAL WITH INACCURACIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Philip Woodall and Vaggelis Giannikas and Wenrong Lu and Duncan McFarlane</field>
  <field name="keywords">Data quality, Information quality, Accuracy, Metadata, Data analytics, Data tags</field>
  <field name="abstract">Data quality tags are a means of informing decision makers about the quality of the data they use from information systems. Unfortunately, data quality tags have not been successfully adopted despite their potential to assist decision makers. One reason for the non-adoption is that maintaining the tags is expensive and time-consuming: having a tag that represents accuracy, for example, would be massively time-consuming to measure because it requires some physical observation of reality to check the true value. We argue that a useful surrogate tag for accuracy can be created—without having to physically measure it—by counting the number of times the data has been exposed to an event that could cause it to become inaccurate. Experimental results show that the tags can help to avoid problems caused by inaccuracies, and also to help find the inaccuracies themselves.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.dss.2019.04.007</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ENERGY PERFORMANCE CERTIFICATES — NEW OPPORTUNITIES FOR DATA-ENABLED URBAN ENERGY POLICY INSTRUMENTS?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Oleksii Pasichnyi and Jörgen Wallin and Fabian Levihn and Hossein Shahrokni and Olga Kordas</field>
  <field name="keywords">Energy performance certificate (EPC), Building energy efficiency, Data applications, Data quality, Sweden</field>
  <field name="abstract">Energy performance certificates (EPC) were introduced in European Union to support reaching energy efficiency targets by informing actors in the building sector about energy efficiency in buildings. While EPC have become a core source of information about building energy, the domains of its applications have not been studied systematically. This partly explains the limitation of conventional EPC data quality studies that fail to expose the essential problems and secure effective use of the data. This study reviews existing applications of EPC data and proposes a new method for assessing the quality of EPCs using data analytics. Thirteen application domains were identified from systematic mapping of 79 papers, revealing increases in the number and complexity of studies and advances in applied data analysis techniques. The proposed data quality assurance method based on six validation levels was tested using four samples of EPC dataset for the case of Sweden. The analysis showed that EPC data can be improved through adding or revising the EPC features and assuring interoperability of EPC datasets. In conclusion, EPC data have wider applications than initially intended by the EPC policy instrument, placing stronger requirements on the quality and content of the data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.enpol.2018.11.051</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG AND OPEN LINKED DATA ANALYTICS ECOSYSTEM: THEORETICAL BACKGROUND AND ESSENTIAL ELEMENTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Martin Lnenicka and Jitka Komarkova</field>
  <field name="keywords">Big and open linked data, Ecosystem approach, Dimensions, Data analytics lifecycle, Stakeholders, Conceptual framework</field>
  <field name="abstract">Big and open linked data are often mentioned together because storing, processing, and publishing large amounts of these data play an increasingly important role in today's society. However, although this topic is described from the political, economic, and social points of view, a technical dimension, which is represented by big data analytics, is insufficient. The aim of this review article was to provide a theoretical background of big and open linked data analytics ecosystem and its essential elements. First, the key terms were introduced including related dimensions. Then, the key lifecycle phases were defined and involved stakeholders were identified. Finally, a conceptual framework was proposed. In contrast to previous research, the new ecosystem is formed by interactions of stakeholders in the following dimensions and their sub-dimensions: transparency, engagement, legal, technical, social, and economic. These relationships are characterized by the most important requisites and public policy choices affecting the data analytics ecosystem together with the key phases and activities of the data analytics lifecycle. The findings should contribute to relevant initiatives, strategies, and policies and their effective implementation.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.giq.2018.11.004</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 8 - BLOCKCHAIN IN HEALTHCARE: CHALLENGES AND SOLUTIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Md. Mehedi Hassan Onik and Satyabrata Aich and Jinhong Yang and Chul-Soo Kim and Hee-Cheol Kim</field>
  <field name="keywords">Blockchain, Big data, Healthcare, Data privacy, EHR security</field>
  <field name="abstract">The main challenge in distributing electronic health records (EHRs) for patient-centered research, market analysis, medicine investigation, healthcare data mining etc., is data privacy. Handling the large-scale data and preserving the privacy of patients has been a challenge to researchers for a long period of time. On the contrary, blockchain technology has alleviated some of the problems by providing a protected and distributed platform. Sadly, existing electronic health record (EHR) management systems suffer from data manipulation, delayed communication, and trustless cooperation in data collection, storage, and distribution. This chapter discusses the current issues of healthcare data privacy and existing and upcoming regulations on this sector. This chapter also includes an overview of the architecture, existing issues, and future scope of blockchain technology for successfully handling privacy and management of current and future medical records. This chapter also presents few blockchain solutions that advocate the future research scopes in healthcare, big data, and blockchain.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-818146-1.00008-8</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">M-LEAN: AN END-TO-END DEVELOPMENT FRAMEWORK FOR PREDICTIVE MODELS IN B2B SCENARIOS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad Marston</field>
  <field name="keywords">Big data, Machine learning, Business-to-business, User trust, Case study</field>
  <field name="abstract">Context
The need for business intelligence has led to advances in machine learning in the business domain, especially with the rise of big data analytics. However, the resulting predictive systems often fail to maintain a satisfactory level of performance in production. Besides, for predictive systems used in business-to-business scenarios, user trust is subject to the model performance. Therefore, the processes of creating, evaluating, and deploying machine learning systems in the business domain need innovative solutions to solve the critical challenges of assuring the quality of the resulting systems.
Objective
Applying machine learning in business-to-business situations imposes specific requirements. This paper aims at providing an integrated solution to businesses to help them transform their data into actions.
Method
The paper presents MLean, an end-to-end framework, that aims at guiding businesses in designing, developing, evaluating, and deploying business-to-business predictive systems. The framework employs the Lean Startup methodology and aims at maximizing the business value while eliminating wasteful development practices.
Results
To evaluate the proposed framework, with the help of our industrial partner, we applied the framework to a case study to build a predictive product. The case study resulted in a predictive system to predict the risks of software license cancellations. The system was iteratively developed and evaluated while adopting the management and end-user perspectives.
Conclusion
It is concluded that, in industry, it is important to be aware of the businesses requirements before considering the application of machine learning. The framework accommodates business perspective from the beginning to produce a holistic product. From the results of the case study, we think that this framework can help businesses define the right opportunities for applying machine learning, developing solutions, evaluating the effectiveness of these solutions, and maintaining their performance in production.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.infsof.2019.05.009</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">STATUS EPILEPTICUS PREVENTION, AMBULATORY MONITORING, EARLY SEIZURE DETECTION AND PREDICTION IN AT-RISK PATIENTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper</field>
  <field name="keywords">Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure detection sensors, Automated seizure detection</field>
  <field name="abstract">Purpose
Status epilepticus is an often apparently randomly occurring, life-threatening medical emergency which affects the quality of life in patients with epilepsy and their families. The purpose of this review is to summarize information on ambulatory seizure detection, seizure prediction, and status epilepticus prevention.
Method
Narrative review.
Results
Seizure detection devices are currently under investigation with regards to utility and feasibility in the detection of isolated seizures, mainly in adult patients with generalized tonic-clonic seizures, in long-term epilepsy monitoring units, and occasionally in the outpatient setting. Detection modalities include accelerometry, electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors, surface electromyography, video detection systems, gyroscope, peripheral temperature, photoplethysmography, and respiratory sensors, among others. Initial detection results are promising, and improve even further, when several modalities are combined. Some portable devices have already been U.S. FDA approved to detect specific seizures. Improved seizure prediction may be attainable in the future given that epileptic seizure occurrence follows complex patient-specific non-random patterns. The combination of multimodal monitoring devices, big data sets, and machine learning may enhance patient-specific detection and predictive algorithms. The integration of these technological advances and novel approaches into closed-loop warning and treatment systems in the ambulatory setting may help detect seizures sooner, and tentatively prevent status epilepticus in the future.
Conclusions
Ambulatory monitoring systems are being developed to improve seizure detection and the quality of life in patients with epilepsy and their families.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.seizure.2018.09.013</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BUSINESS ANALYTICS ADOPTION PROCESS: AN INNOVATION DIFFUSION PERSPECTIVE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dalwoo Nam and Junyeong Lee and Heeseok Lee</field>
  <field name="keywords">Business analytics, Innovation diffusion, Adoption process, Data infrastructure, Data quality management, Analytics centralization</field>
  <field name="abstract">Although business analytics (BA) have been increasingly adopted into businesses, there is limited empirical research examining the drivers of each stage of BA adoption in organizations. Drawing upon technological-organizational-environmental framework and innovation diffusion process, we developed an integrative model to examine BA adoption processes and tested with 170 Korean firms. The analysis shows data-related technological characteristics derive all stages of BA adoption: initiation, adoption and assimilation. While organizational characteristics are associated with adoption and assimilation stage, only competition intensity in environmental characteristics is associated with initiation stage. Our findings help practitioners and researchers to understand what factors can enable companies to adopt BA in each stage.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijinfomgt.2019.07.017</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 11 - MACHINE LEARNING-BASED ARTIFICIAL NOSE ON A LOW-COST IOT-HARDWARE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Matthias Dziubany and Marcel Garling and Anke Schmeink and Guido Burger and Guido Dartmann and Stefan Naumann and Klaus-Uwe Gollmer</field>
  <field name="keywords">Artificial Nose, PCA, SVM, Feature selection, low cost</field>
  <field name="abstract">In order to make Internet of things applications easily available and cost-effective, we aim at using low-cost hardware for typical measurement tasks, and in return putting more effort into the signal processing and data analysis. By the example of beverage recognition with a low-cost temperature-modulated gas sensor, we demonstrate the benefits of processing techniques in big data such as feature selection and dimensionality reduction. Specifically, we determine a subset of temperatures that yields good support vector machine classification results and thereby shortens the measurement process.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-816637-6.00011-7</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 4-2 - NOVEL BIOINFORMATICS METHODS FOR TOXICOEPIGENETICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Raymond G. Cavalcante and Tingting Qin and Maureen A. Sartor</field>
  <field name="keywords">Epigenomic analysis, Toxicoepigenomics, Bisulfite sequencing, Chromatin accessibility, ChIP-seq, Integrative analysis, Chromosomal interactions</field>
  <field name="abstract">The use of high-throughput, genome-wide assays in toxicoepigenetics is rapidly developing and expanding. With recent advances in experimental technologies, a great amount of multiomics epigenomic data has been generated requiring the development of correspondingly advanced bioinformatics approaches to analyze and interpret such big data. This chapter discusses analysis methods for current epigenomic assays available for use in toxicoepigenetic and novel bioinformatics approaches to interpret, visualize, and integrate a variety of epigenomic data and data resources. The epigenomic features covered include DNA methylation, DNA hydroxymethylation, histone modification, chromatin accessibility, and chromatin interaction. For each type of assay used to interrogate those features, bioinformatics tools for data quality control, epigenetic mark detection, comparative analysis, data visualization, functional analysis, and integrative analysis are suggested. Looking forward, it is anticipated that researchers in toxicoepigenomics will adopt newer techniques such as single-cell assays and the bioinformatics methods will continue to evolve.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-812433-8.00012-5</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IBRIDIA: A HYBRID SOLUTION FOR PROCESSING BIG LOGISTICS DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Saïd Hacid and Mohamed Dbouk</field>
  <field name="keywords">Realtime processing, Clustering, Big data, Internet of Things, Logistics, Hierarchical clustering algorithm</field>
  <field name="abstract">Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. Logistics services providers use sensor technologies such as GPS or telemetry to track and manage their shipment processes. Additionally, they use external data that contain critical information about events such as traffic, accidents, and natural disasters. Correlating data from different sensors and social media and performing analysis in real-time provide opportunities to predict events and prevent unexpected delivery delay at run-time. However, collecting and processing data from heterogeneous sources foster problems due to the variety and velocity of data. In addition, processing data in real-time is heavily challenging that it cannot be dealt with using conventional logistics information systems. In this paper, we present a hybrid framework for processing massive volume of data in batch style and real-time. Our framework is built upon Johnson’s hierarchical clustering (HCL) algorithm which produces a dendrogram that represents different clusters of data objects.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.future.2019.02.044</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">FRAMEWORK FOR THE USAGE OF DATA FROM REAL-TIME INDOOR LOCALIZATION SYSTEMS TO DERIVE INPUTS FOR MANUFACTURING SIMULATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Carina Mieth and Anne Meyer and Michael Henke</field>
  <field name="keywords">real-time indoor localization system, input data management, cyber-physical system, manufacturing simulation, digital twin</field>
  <field name="abstract">Discrete event simulation is becoming increasingly important in the planning and operation of complex manufacturing systems. A major problem with today’s approach to manufacturing simulation studies is the collection and processing of data from heterogeneous sources, because the data is often of poor quality and does not contain all the necessary information for a simulation. This work introduces a framework that uses a real-time indoor localization systems (RTILS) as a central main data harmonizer, that is designed to feed production data into a manufacturing simulation from a single source of truth. It is shown, based on different data quality dimensions, how this contributes to a better overall data quality in manufacturing simulation. Furthermore, a detailed overview on which simulation inputs can be derived from the RTILS data is given.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procir.2019.03.216</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">NETWORK-WIDE IDENTIFICATION OF TURN-LEVEL INTERSECTION CONGESTION USING ONLY LOW-FREQUENCY PROBE VEHICLE DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen</field>
  <field name="keywords">Big data, Floating car data, Urban road network, Traffic congestion, Road intersection</field>
  <field name="abstract">Locating the bottlenecks in cities where traffic congestion usually occurs is essential prior to solving congestion problems. Therefore, this paper proposes a low-frequency probe vehicle data (PVD)-based method to identify turn-level intersection traffic congestion in an urban road network. This method initially divides an urban area into meter-scale square cells and maps PVD into those cells and then identifies the cells that correspond to road intersections by taking advantage of the fixed-location stop-and-go characteristics of traffic passing through intersections. With those rasterized road intersections, the proposed method recognizes probe vehicles’ turning directions and provides preliminary analysis of traffic conditions at all turning directions. The proposed method is map-independent (i.e., no digital map is needed) and computationally efficient and is able to rapidly screen most of the intersections for turn-level congestion in a road network. Thereby, this method is expected to greatly decrease traffic engineers’ workloads by providing information regarding where and when to investigate and solve traffic congestion problems.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.trc.2019.10.001</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE DATA DRIVEN TRANSPORT RESEARCH TRAIN IS LEAVING THE STATION. CONSULTANTS ALL ABOARD?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Hanne Seter and Petter Arnesen and Odd André Hjelkrem</field>
  <field name="keywords">nan</field>
  <field name="abstract">This study sets out to assess whether there is a knowledge gap between the research frontier and the consultation business in how transport data are collected, managed and analysed. The consulting business plays an important role in applying data and methods as they typically carry out public tasks in various parts of the transport system, which are becoming more and more specialised. At the same time, big data has emerged with the promise to provide new, more and better information to help understand society and execute policies more efficiently – what we refer to as the data driven transition. We conduct a literature review to identify the state of the art within international research and compare this with results from interviews and with a survey sent to representatives from the Norwegian consultation business. We find that there is a considerable gap between international researchers and the consulting business within the entire process of collection, management and analysis of traffic data, and that this gap is increasing with the emergence of the data driven transition. Finally, we argue that the results are applicable to other countries as well. Action should be taken to keep the consultants up to speed, which will require efforts from several actors, including governmental agencies, the education institutions, the consulting business and researchers.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.tranpol.2019.05.016</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WHAT THE HACK? A GROWTH HACKING TAXONOMY AND PRACTICAL APPLICATIONS FOR FIRMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">René Bohnsack and Meike Malena Liesner</field>
  <field name="keywords">Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data</field>
  <field name="abstract">As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.bushor.2019.09.001</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA GOVERNANCE: A CONCEPTUAL FRAMEWORK, STRUCTURED REVIEW, AND RESEARCH AGENDA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Rene Abraham and Johannes Schneider and Jan {vom Brocke}</field>
  <field name="keywords">Data governance, Information governance, Conceptual framework, Literature review, Research agenda</field>
  <field name="abstract">Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijinfomgt.2019.07.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">OIL AND GAS 4.0 ERA: A SYSTEMATIC REVIEW AND OUTLOOK</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang</field>
  <field name="keywords">Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization</field>
  <field name="abstract">Recently, with the development of “Industry 4.0”, “Oil and Gas 4.0” has also been put on the agenda in the past two years. Some companies and experts believe that “Oil and Gas 4.0” can completely change the status quo of the oil and gas industry, which can bring huge benefits because it accelerates the digitization and intelligentization of the oil and gas industry. However, the “Oil and Gas 4.0” is still in its infancy. Therefore, this paper systematically introduces the concept and core technologies of “Oil and Gas 4.0”, such as big data and the industrial Internet of Things (IIoT). Moreover, this paper analyzes typical application scenarios of the oil and gas industry chain (upstream, midstream and downstream) through examples, such as intelligent oilfield, intelligent pipeline, and intelligent refinery. It is concluded that the essence of “Oil and Gas 4.0” is a data-driven intelligence system based on the highly digitization. To the best of our knowledge, this is the first academic peer-reviewed paper on the “Oil and Gas 4.0” era, aiming to let more oil and gas industry personnel understand its benefits and application scenarios, so as to better apply it to practical engineering in the future. In the discussion section, this paper also analyzes the opportunities and difficulties that may be brought about by the “Oil and Gas 4.0” era. Finally, relevant policy recommendations are proposed.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.compind.2019.06.007</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ARTIFICIAL INTELLIGENCE IN CARDIOVASCULAR IMAGING: JACC STATE-OF-THE-ART REVIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick</field>
  <field name="keywords">artificial intelligence, cardiovascular imaging, deep learning, machine learning</field>
  <field name="abstract">Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with “big data” from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jacc.2018.12.054</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SURVEY OF DATA FUSION IN SMART CITY APPLICATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan</field>
  <field name="keywords">Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification</field>
  <field name="abstract">The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.inffus.2019.05.004</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">30 YEARS OF INTELLIGENCE MODELS IN MANAGEMENT AND BUSINESS: A BIBLIOMETRIC REVIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">J.R. López-Robles and J.R. Otegi-Olaso and I. {Porto Gómez} and M.J. Cobo</field>
  <field name="keywords">Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis</field>
  <field name="abstract">The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization’s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijinfomgt.2019.01.013</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">NEAR-GROUND EFFECT OF HEIGHT ON POLLEN EXPOSURE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jesús Rojo and Jose Oteros and Rosa Pérez-Badia and Patricia Cervigón and Zuzana Ferencova and A. Monserrat Gutiérrez-Bustillo and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto Albertini and David {Rodríguez-De la Cruz} and Estefanía Sánchez-Reyes and José Sánchez-Sánchez and Anna-Mari Pessi and Jukka Reiniharju and Annika Saarto and M. Carmen Calderón and César Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia Fernández-González and José García and M. Mar Trigo and Dorota Myszkowska and Santiago Fernández-Rodríguez and Rafael Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann and Elena Severova and Elsa Caeiro and Helena Ribeiro and Donát Magyar and László Makra and Orsolya Udvardy and Purificación Alcázar and Carmen Galán and Katarzyna Borycka and Idalia Kasprzyk and Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk and Carsten A. Skjøth and Predrag Radišić and Branko Šikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters</field>
  <field name="keywords">Height, Pollen, Aerobiology, Monitoring network, Big data</field>
  <field name="abstract">The effect of height on pollen concentration is not well documented and little is known about the near-ground vertical profile of airborne pollen. This is important as most measuring stations are on roofs, but patient exposure is at ground level. Our study used a big data approach to estimate the near-ground vertical profile of pollen concentrations based on a global study of paired stations located at different heights. We analyzed paired sampling stations located at different heights between 1.5 and 50 m above ground level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from 25 different areas, mainly in Europe, but also covering North America and Australia, resulting in about 2,000,000 daily pollen concentrations analyzed. The daily ratio of the amounts of pollen from different heights per location was used, and the values of the lower station were divided by the higher station. The lower station of paired traps recorded more pollen than the higher trap. However, while the effect of height on pollen concentration was clear, it was also limited (average ratio 1.3, range 0.7–2.2). The standard deviation of the pollen ratio was highly variable when the lower station was located close to the ground level (below 10 m AGL). We show that pollen concentrations measured at >10 m are representative for background near-ground levels.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.envres.2019.04.027</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INTERNET OF THINGS TO NETWORK SMART DEVICES FOR ECOSYSTEM MONITORING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu and Yuke Zhou</field>
  <field name="keywords">Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless sensor network, Smart device</field>
  <field name="abstract">Smart, real-time, low-cost, and distributed ecosystem monitoring is essential for understanding and managing rapidly changing ecosystems. However, new techniques in the big data era have rarely been introduced into operational ecosystem monitoring, particularly for fragile ecosystems in remote areas. We introduce the Internet of Things (IoT) techniques to establish a prototype ecosystem monitoring system by developing innovative smart devices and using IoT technologies for ecosystem monitoring in isolated environments. The developed smart devices include four categories: large-scale and nonintrusive instruments to measure evapotranspiration and soil moisture, in situ observing systems for CO2 and δ13C associated with soil respiration, portable and distributed devices for monitoring vegetation variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial vertebrate monitoring. These new devices outperform conventional devices and are connected to each other via wireless communication networks. The breakthroughs in the ecosystem monitoring IoT include new data loggers and long-distance wireless sensor network technology that supports the rapid transmission of data from devices to wireless networks. The applicability of this ecosystem monitoring IoT is verified in three fragile ecosystems, including a karst rocky desertification area, the National Park for Amur Tigers, and the oasis-desert ecotone in China. By integrating these devices and technologies with an ecosystem monitoring information system, a seamless data acquisition, transmission, processing, and application IoT is created. The establishment of this ecosystem monitoring IoT will serve as a new paradigm for ecosystem monitoring and therefore provide a platform for ecosystem management and decision making in the era of big data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.scib.2019.07.004</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">REFLECTIONS ON QUALITY REQUIREMENTS FOR DIGITAL TRACE DATA IN IS RESEARCH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Gregory Vial</field>
  <field name="keywords">Digital trace data, Data quality, GitHub</field>
  <field name="abstract">In recent years an increasing number of academic disciplines, including IS, have sourced digital trace data for their research. Notwithstanding the potential of such data in (re)investigations of various phenomena of interest that would otherwise be difficult or impossible to study using other sources of data, we view the quality of digital trace data as an underappreciated issue in IS research. To initiate a discussion of how to evaluate and report on the quality of digital trace data in IS research, we couch our arguments within the broader tradition of research on data quality. We explain how the uncontrolled nature of digital trace data creates unique challenges for IS researchers, who need to collect, store, retrieve, and transform those data for the purpose of numerical analysis. We then draw parallels with concepts and patterns commonly used in data analysis projects and argue that, although IS researchers probably apply such concepts and patterns, this is not reported in publications, undermining the reader's ability to assess the reliability, statistical power and replicability of the findings. Using the case of GitHub to illustrate such challenges, we develop a preliminary set of guidelines to help researchers consider and report on the quality of the digital trace data they use in their research. Our work contributes to the debate on data quality and provides relevant recommendations for scholars and IS journals at a time when a growing number of publications are relying on digital trace data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.dss.2019.113133</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ROBUST ANALYSIS AND OPTIMIZATION OF A NOVEL EFFICIENT QUALITY ASSURANCE MODEL IN DATA WAREHOUSING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">P. Amuthabala and R. Santhosh</field>
  <field name="keywords">Data warehouse, Distributed, Data complexity, Data quality, Quality assurance, Optimization, Machine learning</field>
  <field name="abstract">The significance of distributed data warehouses is to initiate the proliferation of various analytical applications. However, with the increase of ubiquitous devices, it is likely that massive volumes of data will be generated, which poses further problems based on the degradation of data quality. The practical reasons for the degradation of data quality in distributed warehouses are identified as heterogeneous data, uncertain inferior data which further affect predictions. The proposed system presents an integrated optimization model to address all the quality degradation problems and to provide a better computational model which effectively incorporates a higher degree of quality assurance. An analytical methodology is adopted in order to develop the proposed quality assurance model for distributed data warehouses.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.compeleceng.2019.02.003</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">COGNITIVE SECURITY: A COMPREHENSIVE STUDY OF COGNITIVE SCIENCE IN CYBERSECURITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Roberto O Andrade and Sang Guun Yoo</field>
  <field name="keywords">Cognitive security, Cognitive science, Situation awareness, Cyber operations</field>
  <field name="abstract">Nowadays, IoT, cloud computing, mobile and social networks are generating a transformation in social processes. Nevertheless, this technological change rise to new threats and security attacks that produce new and complex cybersecurity scenarios with large volumes of data and different attack vectors that can exceeded the cognitive skills of security analysts. In this context, cognitive sciences can enhance the cognitive processes, which can help to security analysts to establish actions in less time and more efficiently within cybersecurity operations. This works presents a cognitive security model that integrates technological solutions such as Big Data, Machine Learning, and Support Decision Systems with the cognitive processes of security analysts used to generate knowledge, understanding and execution of security response actions. The model considers alternatives to establish the automation process in the execution of cognitive tasks defined in the cyber operations processes and includes the analyst as the central axis in the processes of validation and decision making through the use of MAPE-K, OODA and Human in the Loop.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jisa.2019.06.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA-DRIVEN MODELING AND LEARNING IN SCIENCE AND ENGINEERING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Francisco J. Montáns and Francisco Chinesta and Rafael Gómez-Bombarelli and J. Nathan Kutz</field>
  <field name="keywords">Data-driven science, Data-driven modeling, Artificial intelligence, Machine learning, Data-science, Big data</field>
  <field name="abstract">In the past, data in which science and engineering is based, was scarce and frequently obtained by experiments proposed to verify a given hypothesis. Each experiment was able to yield only very limited data. Today, data is abundant and abundantly collected in each single experiment at a very small cost. Data-driven modeling and scientific discovery is a change of paradigm on how many problems, both in science and engineering, are addressed. Some scientific fields have been using artificial intelligence for some time due to the inherent difficulty in obtaining laws and equations to describe some phenomena. However, today data-driven approaches are also flooding fields like mechanics and materials science, where the traditional approach seemed to be highly satisfactory. In this paper we review the application of data-driven modeling and model learning procedures to different fields in science and engineering.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.crme.2019.11.009</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INNOVATIONS IN THE USE OF DATA FACILITATING INSURANCE AS A RESILIENCE MECHANISM FOR COASTAL FLOOD RISK</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Alexander G. Rumson and Stephen H. Hallett</field>
  <field name="keywords">Risk analytics, Adaptation, Remote sensing, Big Data</field>
  <field name="abstract">Insurance plays a crucial role in human efforts to adapt to environmental hazards. Effective insurance can serve as both a measure to distribute, and a method to communicate risk. In order for insurance to fulfil these roles successfully, policy pricing and cover choices must be risk-based and founded on accurate information. This is reliant on a robust evidence base forming the foundation of policy choices. This paper focuses on the evidence available to insurers and emergent innovation in the use of data. The main risk considered is coastal flooding, for which the insurance sector offers an option for potential adaptation, capable of increasing resilience. However, inadequate supply and analysis of data have been highlighted as factors preventing insurance from fulfilling this role. Research was undertaken to evaluate how data are currently, and could potentially, be used within risk evaluations for the insurance industry. This comprised of 50 interviews with those working and associated with the London insurance market. The research reveals new opportunities, which could facilitate improvements in risk-reflective pricing of policies. These relate to a new generation of data collection techniques and analytics, such as those associated with satellite-derived data, IoT (Internet of Things) sensors, cloud computing, and Big Data solutions. Such technologies present opportunities to reduce moral hazard through basing predictions and pricing of risk on large empirical datasets. The value of insurers' claims data is also revealed, and is shown to have the potential to refine, calibrate, and validate models and methods. The adoption of such data-driven techniques could enable insurers to re-evaluate risk ratings, and in some instances, extend coverage to locations and developments, previously rated as too high a risk to insure. Conversely, other areas may be revealed more vulnerable, which could generate negative impacts for residents in these regions, such as increased premiums. However, the enhanced risk awareness generated, by new technology, data and data analytics, could positively alter future planning, development and investment decisions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.scitotenv.2019.01.114</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SYSTEMIC CYBERCRIME STAKEHOLDERS ARCHITECTURAL MODEL</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Manmeet Mahinderjit Singh and Anizah Abu Bakar</field>
  <field name="keywords">Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory, Systemic</field>
  <field name="abstract">The increased of cybercrime incidents taking place in the world is at its perilous magnitude causing losses in term of money and trust. Even though there are various cybersecurity solutions in place; the threat of cybercrime is still a hard problem. Exploration of cybercrime challenges, especially the preventions and detections of the cybercrime should be investigated by composing all the stakeholders and players of a cybercrime issue. In this paper; an exploration of several cybercrime stakeholders is done. It is argued that cybercrime is a systemic threat and cannot be tackled with cybersecurity and legal systems. The architectural model proposed is significant and should become one of the considered milestones in designing security control in tackling cybercrime globally.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.11.227</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS A DATA GOVERNANCE FRAMEWORK FOR THIRD GENERATION PLATFORMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Juan Yebenes and Marta Zorrilla</field>
  <field name="keywords">Data governance, Industry 4.0, data bus architecture, cloud computing, IoT, big data</field>
  <field name="abstract">The fourth industrial revolution considers data as a business asset and therefore this is placed as a central element of the software architecture (data as a service) that will support the horizontal and vertical digitalization of industrial processes. The large volume of data that the environment generates, its heterogeneity and complexity, as well as its reuse for later processes (e.g. analytics, IA) requires the adoption of policies, directives and standards for its right governance. Furthermore, the issues related to the use of resources in the cloud computing must be taken into account with the aim of meeting the requirements of performance and security of the different processes. This article, in the absence of frameworks adapted to this new architecture, proposes an initial schema for developing an effective data governance programme for third generation platforms, that means, a conceptual tool which guides organizations to define, design, develop and deploy services aligned with its vision and business goals in I4.0 era.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.04.082</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EXPOQUAL: EVALUATING MEASURED AND MODELED HUMAN EXPOSURE DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Judy S. LaKind and Cian O’Mahony and Thomas Armstrong and Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman</field>
  <field name="keywords">ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument, Biomonitoring, Model uncertainty</field>
  <field name="abstract">Recent rapid technological advances are producing exposure data sets for which there are no available data quality assessment tools. At the same time, regulatory agencies are moving in the direction of data quality assessment for environmental risk assessment and decision-making. A transparent and systematic approach to evaluating exposure data will aid in those efforts. Any approach to assessing data quality must consider the level of quality needed for the ultimate use of the data. While various fields have developed approaches to assess data quality, there is as yet no general, user-friendly approach to assess both measured and modeled data in the context of a fit-for-purpose risk assessment. Here we describe ExpoQual, an instrument developed for this purpose which applies recognized parameters and exposure data quality elements from existing approaches for assessing exposure data quality. Broad data streams such as quantitative measured and modeled human exposure data as well as newer and developing approaches can be evaluated. The key strength of ExpoQual is that it facilitates a structured, reproducible and transparent approach to exposure data quality evaluation and provides for an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity and to include transparency in aspects based on professional judgment. ExpoQual is freely available on-line for testing and user feedback (exposurequality.com).</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.envres.2019.01.039</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IMPROVED POPULATION MAPPING FOR CHINA USING REMOTELY SENSED AND POINTS-OF-INTEREST DATA WITHIN A RANDOM FORESTS MODEL</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and Peng Jia</field>
  <field name="keywords">Points of interest, Population, Random forests, Nighttime light, China</field>
  <field name="abstract">Remote sensing image products (e.g. brightness of nighttime lights and land cover/land use types) have been widely used to disaggregate census data to produce gridded population maps for large geographic areas. The advent of the geospatial big data revolution has created additional opportunities to map population distributions at fine resolutions with high accuracy. A considerable proportion of the geospatial data contains semantic information that indicates different categories of human activities occurring at exact geographic locations. Such information is often lacking in remote sensing data. In addition, the remarkable progress in machine learning provides toolkits for demographers to model complex nonlinear correlations between population and heterogeneous geographic covariates. In this study, a typical type of geospatial big data, points-of-interest (POIs), was combined with multi-source remote sensing data in a random forests model to disaggregate the 2010 county-level census population data to 100 × 100 m grids. Compared with the WorldPop population dataset, our population map showed higher accuracy. The root mean square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing for this method and WorldPop were 27,829 and 34,193, respectively. The large under-allocation of the population in urban areas and over-allocation in rural areas in the WorldPop dataset was greatly reduced in this new population map. Apart from revealing the effectiveness of POIs in improving population mapping, this study promises the potential of geospatial big data for mapping other socioeconomic parameters in the future.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.scitotenv.2018.12.276</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AN INTERNET-OF-THINGS ENABLED SMART MANUFACTURING TESTBED</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Devarshi Shah and Jin Wang and Q. Peter He</field>
  <field name="keywords">Internet-of-things, smart manufacturing, big data, data analytics, statistical analysis, vibration, soft sensor, process monitoring</field>
  <field name="abstract">The emergence of the industrial Internet of Things (IoT) and ever advancing computing and communication technologies have fueled a new industrial revolution which is happening worldwide to make current manufacturing systems smarter, safer, and more efficient. Although many general frameworks have been proposed for IoT enabled systems for industrial application, there is limited literature on demonstrations or testbeds of such systems. In addition, there is a lack of systematic study on the characteristics of IoT sensors and data analytics challenges associated with IoT sensor data. This study is an attempt to help fill this gap by exploring the characteristics of IoT vibration sensors and show how IoT sensors and big data analytics can be used to develop real time monitoring frameworks.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ifacol.2019.06.122</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CONGESTION BY ACCIDENT? A TWO-WAY RELATIONSHIP FOR HIGHWAYS IN ENGLAND</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ilias Pasidis</field>
  <field name="keywords">Accidents, Traffic congestion, Big data, Highways, England</field>
  <field name="abstract">This paper aims to estimate the causal effect of accidents on traffic congestion and vice versa. In order to identify both effects of this two-way relationship, I use dynamic panel data techniques and open access ‘big data’ of highway traffic and accidents in England for the period 2012–2014. The research design is based on the daily-and-hourly specific mean reversion pattern of highway traffic, which can be used to define a recurrent congestion benchmark. Using this benchmark, I am able to identify the causal effect of accidents on non-recurrent traffic congestion. A positive relationship between traffic congestion and road accidents would yield multiplicative benefits for policies that aim at reducing either of these issues. Additionally, I explore the duration of the effect of an accident on congestion, the ‘rubbernecking’ effect, as well as heterogeneous effects in the most congested highway segments. Then, I test the use of methods which employ the bulk of information in big data and other methods using a very reduced sample. In my application, both approaches produce similar results. Finally, I find a non-linear negative effect of traffic congestion on the probability of an accident.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jtrangeo.2017.10.006</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PREDICTIVE MODELING OF WILDFIRES: A NEW DATASET AND MACHINE LEARNING APPROACH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}</field>
  <field name="keywords">Big data, Remote sensing, Machine learning, Wildfire prediction, Data mining, Artificial intelligence</field>
  <field name="abstract">Wildfires, whether natural or caused by humans, are considered among the most dangerous and devastating disasters around the world. Their complexity comes from the fact that they are hard to predict, hard to extinguish and cause enormous financial losses. To address this issue, many research efforts have been conducted in order to monitor, predict and prevent wildfires using several Artificial Intelligence techniques and strategies such as Big Data, Machine Learning, and Remote Sensing. The latter offers a rich source of satellite images, from which we can retrieve a huge amount of data that can be used to monitor wildfires. The method used in this paper combines Big Data, Remote Sensing and Data Mining algorithms (Artificial Neural Network and SVM) to process data collected from satellite images over large areas and extract insights from them to predict the occurrence of wildfires and avoid such disasters. For this reason, we implemented a methodology that serves this purpose by building a dataset based on Remote Sensing data related to the state of the crops (NDVI), meteorological conditions (LST), as well as the fire indicator “Thermal Anomalies”, these data, were acquired from “MODIS” (Moderate Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires). Experiments were made using the big data platform “Databricks”. Experimental results gave high prediction accuracy (98.32%). These results were assessed using several validation strategies (e.g., classification metrics, cross-validation, and regularization) as well as a comparison with some wildfire early warning systems.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.firesaf.2019.01.006</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AN AUTOMATED WORKFLOW FOR MALDI-TOF MASS SPECTRA PATTERN IDENTIFICATION ON LARGE DATA SETS: AN APPLICATION TO DETECT ANEUPLOIDIES FROM PREGNANCY URINE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles</field>
  <field name="keywords">MALDI-ToF, Pattern recognition, Quality control, Comparative intensity data, Automated processing</field>
  <field name="abstract">Urine from first trimester pregnancies has been found to be rich in information related to aneuploidies and other clinical conditions. Mass spectral analysis derived from matrix assisted laser desorption ionization (MALDI) time of flight (ToF) data has been proven to be a cost effective method for clinical diagnostics. However, urine mass spectra are complex and require data modelling frameworks. Therefore, computational approaches that systematically analyse big data generated from MALDI-ToF mass spectra are essential. To address this issue, we developed an automated workflow that successfully processed large data sets from MALDI-ToF which is 100-fold faster than using a common software tool. Our method performs accurate data quality control decisions, and generates a comparative analysis to extract peak intensity patterns from a data set. We successfully applied our framework to the identification of peak intensity patterns for Trisomy 21 and Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in the UK and China. The results from our automated comparative analysis have shown characteristic patterns associated with aneuploidies in the first trimester pregnancy. Moreover, we have shown that the intensity patterns depended on the population origin, gestational age, and MALDI-ToF instrument.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.imu.2019.100194</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">COHORT PROFILE: BEYOND BIRTH COHORT STUDY – THE KOREAN CHILDREN'S ENVIRONMENTAL HEALTH STUDY (KO-CHENS)</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Kyoung Sook Jeong and Suejin Kim and Woo Jin Kim and Hwan-Cheol Kim and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee and Yangho Kim and Eunhee Ha</field>
  <field name="keywords">Ko-CHENS, Children, Environment, Cohort profile, Birth cohort</field>
  <field name="abstract">The Korean CHildren's ENvironmental health Study (Ko-CHENS) is a nationwide prospective birth cohort showing the correlation between the environmental exposures and the health effects to prevent the environmental diseases in children, and it provides the guidelines for the environmental hazardous factors, applying the life-course approach to the environmental-health management system. The Ko-CHENS consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort are followed up at 6 months, every year before their admission into the elementary school, and every 3 years from the first year after this admission. The children in the Cohort will be followed up through the data links (Statistics Korea, National Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens will be analyzed for 19 substances. The long-term-storage biological samples will be used for the further substance analysis. The Ko-CHENS will investigate whether the environmental variables including the perinatal outdoor and indoor factors and the greenness contribute causally to the health outcomes in the children and adolescents. In addition to the individual surveys, the assessments of the outdoor exposures and health outcomes will use the national air-quality monitoring data and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS are as follows: The Ko-CHENS data that can be linked with the nationally registered NHIS health-related database, including the medical utilization and the periodic health screening, and the birth/mortality database in the Statistics; the other is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS is a very cost-effective study that fully exploits the existing national big-data systems with the data linkage.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.envres.2018.12.009</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SYSTEMATIC APPROACH FOR DISCOVERING CAUSAL DEPENDENCIES BETWEEN OBSERVATIONS AND INCIDENTS IN THE HEALTH AND SAFETY DOMAIN</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter Hofstede}</field>
  <field name="keywords">Big data, Data mining, Process mining, Proximity of events, Causality, Health and safety, Cause of incidents</field>
  <field name="abstract">The paper at hand motivates, proposes, demonstrates, and evaluates a novel systematic approach to discovering causal dependencies between events encoded in large arrays of data, called causality mining. The approach has emerged in the discussions with our project partner, an Australian public energy company. It was successfully evaluated in a case study with the project partner to extract valuable, and otherwise unknown, information on the causal dependencies between observations reported by the company’s employees as part of the organizational health and safety management practices and incidents that had occurred at the organization’s sites. The dependencies were derived based on the notion of proximity of the observations and incidents. The setup and results of the evaluation are reported in this paper. The new approach and the delivered insights aim at improving the overall health and safety culture of the project partner practices, as they can be applied to caution and, thus, prevent future incidents.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ssci.2019.04.045</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CURIOUS FEATURE SELECTION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Michal Moran and Goren Gordon</field>
  <field name="keywords">Intrinsic motivation learning, Curiosity loop, Reinforcement learning, Big data, Data science, Feature selection</field>
  <field name="abstract">In state-of-the-art big-data applications, the process of building machine learning models can be very challenging due to continuous changes in data structures and the need for human interaction to tune the variables and models over time. Hence, expedited learning in rapidly changing environments is required. In this work, we address this challenge by implementing concepts from the field of intrinsically motivated computational learning, also known as artificial curiosity (AC). In AC, an autonomous agent acts to optimize its learning about itself and its environment by receiving internal rewards based on prediction errors. We present a novel method of intrinsically motivated learning, based on the curiosity loop, to learn the data structures in large and varied datasets. An autonomous agent learns to select a subset of relevant features in the data, i.e., feature selection, to be used later for model construction. The agent optimizes its learning about the data structure over time without requiring external supervision. We show that our method, called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy of learning models on three public datasets.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ins.2019.02.009</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">USING SOCIAL NETWORK AND SEMANTIC ANALYSIS TO ANALYZE ONLINE TRAVEL FORUMS AND FORECAST TOURISM DEMAND</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella</field>
  <field name="keywords">Tourism forecasting, Social network analysis, Semantic analysis, Online community, Text mining, Big data</field>
  <field name="abstract">Forecasting tourism demand has important implications for both policy makers and companies operating in the tourism industry. In this research, we applied methods and tools of social network and semantic analysis to study user-generated content retrieved from online communities which interacted on the TripAdvisor travel forum. We analyzed the forums of 7 major European capital cities, over a period of 10 years, collecting more than 2,660,000 posts, written by about 147,000 users. We present a new methodology of analysis of tourism-related big data and a set of variables which could be integrated into traditional forecasting models. We implemented Factor Augmented Autoregressive and Bridge models with social network and semantic variables which often led to a better forecasting performance than univariate models and models based on Google Trend data. Forum language complexity and the centralization of the communication network – i.e. the presence of eminent contributors – were the variables that contributed more to the forecasting of international airport arrivals.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.dss.2019.113075</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">KNOWLEDGE AREAS, THEMES AND FUTURE RESEARCH ON OPEN DATA: A CO-WORD ANALYSIS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-María Mora-Valentín</field>
  <field name="keywords">Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends</field>
  <field name="abstract">This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.giq.2018.10.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CRIMINOLOGY IN THE AGE OF DATA EXPLOSION: NEW DIRECTIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Turgut Ozkan</field>
  <field name="keywords">Social science, Big data, Crime, Social media, Data-driven social science</field>
  <field name="abstract">This review discusses practical benefits and limitations of novel data-driven research for social scientists in general and criminologists in particular by providing a comprehensive examination of the matter. Specifically, this study is an attempt to critically evaluate ‘big data’, data-driven perspectives, and their epistemological value for both scholars and practitioners, particularly those working on crime. It serves as guidance for those who are interested in data-driven research by pointing out new research avenues. In addition to the benefits, the drawbacks associated with data-driven approaches are also discussed. Finally, critical problems that are emerging in this era, such as privacy and ethical concerns are highlighted.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.soscij.2018.10.010</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ANALYTICS-BASED DECISION-MAKING FOR SERVICE SYSTEMS: A QUALITATIVE STUDY AND AGENDA FOR FUTURE RESEARCH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos</field>
  <field name="keywords">Big data analytics, Decision-making, Service systems</field>
  <field name="abstract">While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijinfomgt.2019.01.020</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS A KNOWLEDGE DRIVEN FRAMEWORK FOR BRIDGING THE GAP BETWEEN SOFTWARE AND DATA ENGINEERING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan</field>
  <field name="keywords">Ontologies, Data engineering, Software engineering, Alignment, Integration</field>
  <field name="abstract">In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jss.2018.12.017</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DYNAMIC COOPERATION STRATEGIES OF THE CLOSED-LOOP SUPPLY CHAIN INVOLVING THE INTERNET SERVICE PLATFORM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zehua Xiang and Minli Xu</field>
  <field name="keywords">Big data marketing, Differential game, Closed-loop supply chain, Internet service platform</field>
  <field name="abstract">In the age of “Internet+”, many Internet service platforms (ISPs) in China have been widely introduced to the closed-loop supply chain (CLSC). To further study the role of the Internet service platform, this paper considers a CLSC composed of a manufacturer, a retailer and an Internet service platform who invests in research and development (R&D), advertising and Big Data marketing, and develops the goodwill dynamic model based on the differential game theory. The construction of a goodwill dynamic model has two purposes, namely, to increase sales and the return rate. The optimal decisions for 3 players under two different cooperative scenarios are obtained, namely, the retailer payment scenario (scenario D) and the manufacturer cost-sharing scenario (scenario S). The supply chain members gain more profit or achieve a higher level of goodwill for products under certain conditions, i.e., a high residual value from remanufacturing, a high sharing rate of residual value from the retailer's recycled products, and a low recycling cost. Interestingly, the wholesale price increases with the residual value of recycled products when goodwill effectiveness is low, while the price declines when goodwill effectiveness is high. After comparing two cooperative scenarios, the result shows that an Internet service platform will invest more in Big Data marketing under the manufacturer cost-sharing scenario, and cooperation between the manufacturer and the Internet service platform can help improve the goodwill of enterprises or products. Moreover, the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases through the coordination of a cost-sharing rate, and the effectiveness of Big Data marketing exerts a positive effect on goodwill and the development of the industry. In addition, the retailer has “free rider” tendencies in the manufacturer cost-sharing scenario. The results encourage more enterprises to enhance the value of goodwill through cooperation with Internet service platforms because Internet service platforms conveniently utilize Big Data marketing to increase the sales of products and the collecting rate of used products, which in turn helps environmental sustainability.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jclepro.2019.01.310</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EMERGING ROLE OF EHEALTH IN THE IDENTIFICATION OF VERY EARLY INFLAMMATORY RHEUMATIC DISEASES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Suchitra Kataria and Vinod Ravindran</field>
  <field name="keywords">Artificial intelligence, Big data, Machine learning, Data analytics, Wearable devices, Robotics, Digital health</field>
  <field name="abstract">Digital health or eHealth technologies, notably pervasive computing, robotics, big-data, wearable devices, machine learning, and artificial intelligence (AI), have opened unprecedented opportunities as to how the diseases are diagnosed and managed with active patient engagement. Patient-related data have provided insights (real world data) into understanding the disease processes. Advanced analytics have refined these insights further to draw dynamic algorithms aiding clinicians in making more accurate diagnosis with the help of machine learning. AI is another tool, which, although is still in the evolution stage, has the potential to help identify early signs even before the clinical features are apparent. The evolving digital developments pose challenges on allowing access to health-related data for further research but, at the same time, protecting each patient's privacy. This review focuses on the recent technological advances and their applications and highlights the immense potential to enable early diagnosis of rheumatological diseases.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.berh.2019.101429</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE ROLE OF INTERNET-RELATED TECHNOLOGIES IN SHAPING THE WORK OF ACCOUNTANTS: NEW DIRECTIONS FOR ACCOUNTING RESEARCH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jodie Moll and Ogan Yigitbasioglu</field>
  <field name="keywords">Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence</field>
  <field name="abstract">This paper reviews the accounting literature that focuses on four Internet-related technologies that have the potential to dramatically change and disrupt the work of accountants and accounting researchers in the near future. These include cloud, big data, blockchain, and artificial intelligence (AI). For instance, access to distributed ledgers (blockchain) and big data supported by cloud-based analytics tools and AI will automate decision making to a large extent. These technologies may significantly improve financial visibility and allow more timely intervention due to the perpetual nature of accounting. However, given the number of tasks technology has relieved of accountants, these technologies may also lead to concerns about the profession's legitimacy. The findings suggest that scholars have not given sufficient attention to these technologies and how these technologies affect the everyday work of accountants. Research is urgently needed to understand the new kinds of accounting required to manage firms in the changing digital economy and to determine the new skills and competencies accountants may need to master to remain relevant and add value. The paper outlines a set of questions to guide future research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.bar.2019.04.002</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">FRAMEWORK FOR DEFINING INFORMATION QUALITY BASED ON DATA ATTRIBUTES WITHIN THE DIGITAL SHADOW USING LDA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Michael Riesener and Christian Dölle and Günther Schuh and Christian Tönnes</field>
  <field name="keywords">Digital Shadow, Information quality, Data quality, Latent dirichlet allocation (LDA)</field>
  <field name="abstract">The amount of data, which is created in companies is increasing due to modern communication technologies and decreasing costs for storing data. This leads to an advancement of methods for data analyses as well as to an increasing awareness of benefits resulting from data-based knowledge. In the context of product service systems and product development, there are two major concepts for providing product information. The digital twin collects every information possible, while the digital shadow provides a sufficient and content-related picture of the product. Since these concepts merge data from different sources, comprehension about information quality and its relation to the data quality becomes immanently important. This paper introduces a framework to determine information quality with respect to data-related and system-related attributes. An extensive literature review with focus on “information quality” and “data quality” identifies the important approaches for describing information and data quality. A latent dirichlet allocation (LDA) algorithm is applied on 371 definitions and identify 12 data-related and system-related attributes for information quality. Those attributes are assigned to six dimensions for information quality. So the proposed framework depicts the relationships between data attributes and the influence on information quality.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procir.2019.03.131</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA-DRIVEN PERFORMANCE ANALYSES OF WASTEWATER TREATMENT PLANTS: A REVIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath</field>
  <field name="keywords">Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring</field>
  <field name="abstract">Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.watres.2019.03.030</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PERSPECTIVES ON NUMERICAL DATA QUALITY IN IS RESEARCH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">James R. Marsden and David E. Pingry and Jason B. Thatcher</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.dss.2019.113172</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA QUALITY AND BLOCKCHAIN TECHNOLOGY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami</field>
  <field name="keywords">Machine learning, Artificial intelligence, Blockchain technology</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.accpm.2018.12.015</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CONSTRUCTING LARGE SCALE COHORT FOR CLINICAL STUDY ON HEART FAILURE WITH ELECTRONIC HEALTH RECORD IN REGIONAL HEALTHCARE PLATFORM: CHALLENGES AND STRATEGIES IN DATA REUSE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Daowen Liu and Liqi Lei and Tong Ruan and Ping He</field>
  <field name="keywords">electronic health records, clinical terminology knowledge graph, clinical special disease case repository, evaluation of data quality, large scale cohort study</field>
  <field name="abstract">Regional healthcare platforms collect clinical data from hospitals in specific areas for the purpose of healthcare management. It is a common requirement to reuse the data for clinical research. However, we have to face challenges like the inconsistence of terminology in electronic health records (EHR) and the complexities in data quality and data formats in regional healthcare platform. In this paper, we propose methodology and process on constructing large scale cohorts which forms the basis of causality and comparative effectiveness relationship in epidemiology. We firstly constructed a Chinese terminology knowledge graph to deal with the diversity of vocabularies on regional platform. Secondly, we built special disease case repositories (i.e., heart failure repository) that utilize the graph to search the related patients and to normalize the data. Based on the requirements of the clinical research which aimed to explore the effectiveness of taking statin on 180-days readmission in patients with heart failure, we built a large-scale retrospective cohort with 29647 cases of heart failure patients from the heart failure repository. After the propensity score matching, the study group (n=6346) and the control group (n=6346) with parallel clinical characteristics were acquired. Logistic regression analysis showed that taking statins had a negative correlation with 180-days readmission in heart failure patients. This paper presents the workflow and application example of big data mining based on regional EHR data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.24920/003579</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WHAT CAN MILLIONS OF LABORATORY TEST RESULTS TELL US ABOUT THE TEMPORAL ASPECT OF DATA QUALITY? STUDY OF DATA SPANNING 17 YEARS IN A CLINICAL DATA WAREHOUSE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne Landau-Loriot and Benoit Vedie and Jean-Louis Paul and Laëtitia Mauge and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and Bastien Rance</field>
  <field name="keywords">Quality control, Computational biology/methods*, Information storage and retrieval, Humans, Clinical laboratory information systems</field>
  <field name="abstract">Objective
To identify common temporal evolution profiles in biological data and propose a semi-automated method to these patterns in a clinical data warehouse (CDW).
Materials and Methods
We leveraged the CDW of the European Hospital Georges Pompidou and tracked the evolution of 192 biological parameters over a period of 17 years (for 445,000 + patients, and 131 million laboratory test results).
Results
We identified three common profiles of evolution: discretization, breakpoints, and trends. We developed computational and statistical methods to identify these profiles in the CDW. Overall, of the 192 observed biological parameters (87,814,136 values), 135 presented at least one evolution. We identified breakpoints in 30 distinct parameters, discretizations in 32, and trends in 79.
Discussion and conclusion
our method allowed the identification of several temporal events in the data. Considering the distribution over time of these events, we identified probable causes for the observed profiles: instruments or software upgrades and changes in computation formulas. We evaluated the potential impact for data reuse. Finally, we formulated recommendations to enable safe use and sharing of biological data collection to limit the impact of data evolution in retrospective and federated studies (e.g. the annotation of laboratory parameters presenting breakpoints or trends).</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cmpb.2018.12.030</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TECHNOLOGY IN THE 21ST CENTURY: NEW CHALLENGES AND OPPORTUNITIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang</field>
  <field name="keywords">Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making</field>
  <field name="abstract">Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.techfore.2018.06.009</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">QDAS: QUALITY DRIVEN DATA SUMMARISATION FOR EFFECTIVE STORAGE MANAGEMENT IN INTERNET OF THINGS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen and Flora D. Salim</field>
  <field name="keywords">Quality of data, Storage management, Internet of Things (IoT), Cloud computing, Quality of service, Data summarisation</field>
  <field name="abstract">The proliferation of Internet of Things (IoT) has led to the emergence of enabling many interesting applications within the realm of several domains including smart cities. However, the accumulation of data from smart IoT devices poses significant challenges for data storage while there are needs to deliver relevant and high quality services to consumers. In this paper, we propose QDaS, a novel domain agnostic framework as a solution for effective data storage and management of IoT applications. The framework incorporates a novel data summarisation mechanism that uses an innovative data quality estimation technique. This proposed data quality estimation technique computes the quality of data (based on their utility) without requiring any feedback from users of this IoT data or domain awareness of the data. We evaluate the effectiveness of the proposed QDaS framework using real world datasets.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jpdc.2018.03.013</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TECHNOLOGY LIFE CYCLE AND DATA QUALITY: ACTION AND TRIANGULATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Daniel E. O'Leary</field>
  <field name="keywords">Technology life cycle, Data quality, Non-stationary data, Hype cycle, Data biases, Data phase triangulation</field>
  <field name="abstract">Where a technology is its life cycle can make a difference in the data generated by or about adopting, using or implementing that technology. As a result, it is arguable that adoption, usage or implementation data generated early in a technology's life cycle is directly comparable to data generated later in the life cycle. In particular, comparisons of early and late data can result in a number of disparate results and limit replicability, because of biases in the data and non-stationary data. This paper suggests that it can be important for information systems researchers to disclose an estimate of the location in the technology's life cycle, as part of their analysis. The data life cycle discussion is then extended to the notion of “data phase triangulation,” which is contrasted with “methodology triangulation” and “data (collection) triangulation.” In addition, we discuss the importance of being able to use the findings from life cycle-based research to “push” a technology from one phase to another phase.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.dss.2019.113139</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">GUEST EDITORIAL: SPECIAL ISSUE IN BIOMEDICAL DATA QUALITY ASSESSMENT METHODS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Carlos Sáez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits and Juan M Garcia-Gomez</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cmpb.2019.06.013</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 7 - SMART CITY IS A SAFE CITY: INFORMATION AND COMMUNICATION TECHNOLOGY–ENHANCED URBAN SPACE MONITORING AND SURVEILLANCE SYSTEMS: THE PROMISE AND LIMITATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu</field>
  <field name="keywords">Cyber security, Ethics, Policy-making, Security, Surveillance</field>
  <field name="abstract">Urban space monitoring and surveillance systems are present almost everywhere in various forms of sensing devices such as closed-circuit television, smartphone, and camera. This requires a robust and easy-to-manage information and communication technology (ICT) infrastructure that is generally comprises sensors, protocols, networks, and steps. Smart adoption of such systems could influence, manage, direct, and protect human beings and property. Nevertheless, it may create problems of government support, data quality, privacy, and security. Today's computational world allows implementation of artificial intelligence models for big data analytics to bring cities smart (with intelligence and optimal improvement). This chapter will discuss the applications of urban space monitoring and surveillance systems via ICT. The typical limitations of the current research are discussed in detail.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-816639-0.00007-7</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 11 - BIG DATA AND DATABASES FOR METABOLIC PHENOTYPING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang Gao and Robert C. Glen</field>
  <field name="keywords">Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing, High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal, Social implications, ELSI</field>
  <field name="abstract">Metabolic phenotyping is entering the era of Big Data, leading to new opportunities and challenges. Cloud computing has been proposed as a novel paradigm, but as yet is not widely understood or used. In this chapter we introduce the concepts of Big Data and cloud computing, and discuss how they might change the landscape of metabolic phenotyping and analysis. We highlight some of the reasons for the increase in data size and explain advantages and disadvantages of large-scale computing in this context. We illustrate the area with a survey of software tools and databases currently available, and describe the newly developed cloud infrastructure “PhenoMeNal,” which will enable widespread use of these approaches. We conclude the chapter with a discussion of the important ethical, legal, and social implications (ELSI) of large-scale computing in this rapidly developing field.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-812293-8.00011-6</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INITIALIZING A HOSPITAL-WIDE DATA QUALITY PROGRAM. THE AP-HP EXPERIENCE.</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Christel Daniel and Patricia Serre and Nina Orlova and Stéphane Bréant and Nicolas Paris and Nicolas Griffon</field>
  <field name="keywords">Data accuracy, Data quality, Electronic health records, Data warehousing, Observational Studies as Topic</field>
  <field name="abstract">Background and Objectives
Data Quality (DQ) programs are recognized as a critical aspect of new-generation research platforms using electronic health record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical Data Repository aggregates EHR data from 37 hospitals to enable large-scale research and secondary data analysis. This paper describes the DQ program currently in place at AP-HP and the lessons learned from two DQ campaigns initiated in 2017.
Materials and Methods
As part of the AP-HP DQ program, two domains - patient identification (PI) and healthcare services (HS) - were selected for conducting DQ campaigns consisting of 5 phases: defining the scope, measuring, analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted in two data sets – the PI data set containing 8.8 M patients and the HS data set containing 13,099 consultation agendas and 2122 care units. Seventeen DQ measures were defined and DQ issues were classified using a unified DQ reporting framework. For each domain, actions plans were defined for improving and monitoring prioritized DQ issues.
Results
Eleven identified DQ issues (8 for the PI data set and 3 for the HS data set) were categorized into completeness (n = 6), conformance (n = 3) and plausibility (n = 2) DQ issues. DQ issues were caused by errors from data originators, ETL issues or limitations of the EHR data entry tool. The action plans included sixteen actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation, the DQ campaigns already resulted in significant improvement of DQ measures.
Conclusion
DQ assessments of hospital information systems are largely unpublished. The preliminary results of two DQ campaigns conducted at AP-HP illustrate the benefit of the engagement into a DQ program. The adoption of a unified DQ reporting framework enables the communication of DQ findings in a well-defined manner with a shared vocabulary. Dedicated tooling is needed to automate and extend the scope of the generic DQ program. Specific DQ checks will be additionally defined on a per-study basis to evaluate whether EHR data fits for specific uses.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cmpb.2018.10.016</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A PILOT INFRASTRUCTURE FOR SEARCHING RAINFALL METADATA AND GENERATING RAINFALL PRODUCT USING THE BIG DATA OF NEXRAD</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and Witold F. Krajewski</field>
  <field name="keywords">NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology</field>
  <field name="abstract">The Iowa Flood Center (IFC) developed a pilot infrastructure to explore rainfall metadata (descriptive statistics) and generate rainfall products over the Iowa domain based on the NEXRAD Level II data directly accessible through cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD radar data. Taking advantage of the cloud storage benefits (unlimited storage and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data exploration systems, which often lead to massive data acquisition/ingestion and rapid filling of limited system storage. Its map-based interface allows researchers to select a space-time domain of interest, retrieve and visualize pre-calculated rainfall metadata, and generate radar-derived rainfall products. Because the system provides generalized approaches to compute metadata and process data for rainfall estimation, the framework presented in this study would be readily transferrable to other geographic regions and larger scale applications.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.envsoft.2019.03.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TAQIH, A TOOL FOR TABULAR DATA QUALITY ASSESSMENT AND IMPROVEMENT IN THE CONTEXT OF HEALTH DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Roberto {Álvarez Sánchez} and Andoni {Beristain Iraola} and Gorka {Epelde Unanue} and Paul Carlin</field>
  <field name="keywords">Data quality, Exploratory data analysis, Data pre-processing</field>
  <field name="abstract">Background and Objectives
Data curation is a tedious task but of paramount relevance for data analytics and more specially in the health context where data-driven decisions must be extremely accurate. The ambition of TAQIH is to support non-technical users on 1) the exploratory data analysis (EDA) process of tabular health data, and 2) the assessment and improvement of its quality.
Methods
A web-based tool has been implemented with a simple yet powerful visual interface. First, it provides interfaces to understand the dataset, to gain the understanding of the content, structure and distribution. Then, it provides data visualization and improvement utilities for the data quality dimensions of completeness, accuracy, redundancy and readability.
Results
It has been applied in two different scenarios. (1) The Northern Ireland General Practitioners (GPs) Prescription Data, an open data set containing drug prescriptions. (2) A glucose monitoring tele health system dataset. Findings on (1) include: Features that had significant amount of missing values (e.g. AMP_NM variable 53.39%); instances that have high percentage of variable values missing (e.g. 0.21% of the instances with > 75% of missing values); highly correlated variables (e.g. Gross and Actual cost almost completely correlated (∼ + 1.0)). Findings on (2) include: Features that had significant amount of missing values (e.g. patient height, weight and body mass index (BMI) (> 70%), date of diagnosis 13%)); highly correlated variables (e.g. height, weight and BMI). Full detail of the testing and insights related to findings are reported.
Conclusions
TAQIH enables and supports users to carry out EDA on tabular health data and to assess and improve its quality. Having the layout of the application menu arranged sequentially as the conventional EDA pipeline helps following a consistent analysis process. The general description of the dataset and features section is very useful for the first overview of the dataset. The missing value heatmap is also very helpful in visually identifying correlations among missing values. The correlations section has proved to be supportive as a preliminary step before further data analysis pipelines, as well as the outliers section. Finally, the data quality section provides a quantitative value to the dataset improvements.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cmpb.2018.12.029</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA READINESS IN RADIATION ONCOLOGY: AN EFFICIENT APPROACH FOR RELABELING RADIATION THERAPY STRUCTURES WITH THEIR TG-263 STANDARD NAME IN REAL-WORLD DATA SETS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill and Brooke Evans and Blanca Gallego</field>
  <field name="keywords">nan</field>
  <field name="abstract">Purpose
To prepare for big data analyses on radiation therapy data, we developed Stature, a tool-supported approach for standardization of structure names in existing radiation therapy plans. We applied the widely endorsed nomenclature standard TG-263 as the mapping target and quantified the structure name inconsistency in 2 real-world data sets.
Methods and Materials
The clinically relevant structures in the radiation therapy plans were identified by reference to randomized controlled trials. The Stature approach was used by clinicians to identify the synonyms for each relevant structure, which was then mapped to the corresponding TG-263 name. We applied Stature to standardize the structure names for 654 patients with prostate cancer (PCa) and 224 patients with head and neck squamous cell carcinoma (HNSCC) who received curative radiation therapy at our institution between 2007 and 2017. The accuracy of the Stature process was manually validated in a random sample from each cohort. For the HNSCC cohort we measured the resource requirements for Stature, and for the PCa cohort we demonstrated its impact on an example clinical analytics scenario.
Results
All but 1 synonym group (“Hydrogel”) was mapped to the corresponding TG-263 name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures were exact matches (ie, following local naming convention), 284 were matched via a synonym, and 3 required manual matching. This original radiation therapy structure names therefore had a naming inconsistency rate of 4.81%. For the HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym, 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The clinician hours required were one-fifth of those that would be required for manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).
Conclusions
The Stature approach was highly accurate and had significant resource efficiencies compared with manual curation.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.adro.2018.09.013</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA QUALITY ASSESSMENT FOR IMPROVED DECISION-MAKING: A METHODOLOGY FOR SMALL AND MEDIUM-SIZED ENTERPRISES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lisa C. Günther and Eduardo Colangelo and Hans-Hermann Wiendahl and Christian Bauer</field>
  <field name="keywords">Data quality assessment, Data quality control, Information quality, Benchmarking, Production planning, control</field>
  <field name="abstract">Industrial enterprises rely on prediction of market behavior, monitoring of performance measures, evaluation of production processes and other data analyses to support strategic and operational decisions. However, although an adequate data quality (DQ) is essential for any data analysis and several methodologies for DQ assessment exist, not all organizations consider DQ in decision-making processes. E.g., inaccurate and delayed data acquisition leads to imprecise master data and poor knowledge of machine utilization. While these aspects should influence production planning and control, current approaches to data evaluation are too complex to use them on a-day-to-day basis. In this paper, we propose a methodology that simplifies the execution of DQ evaluations and improves the understandability of its results. One of its main concerns is to make DQ assessment usable to small and medium-sized enterprises (SME). The approach takes selected, context related structured or semi-structured data as input and uses a set of generic test criteria applicable to different tasks and domains. It combines data and domain driven aspects and can be partly executed automated and without context specific domain knowledge. The results of the assessment can be summarized into quality dimensions and used for benchmarking. The methodology is validated using data from the enterprise resource planning (ERP) and manufacturing execution system (MES) of a sheet metal manufacturer covering a year of time. The particular application aims at calculating logistic key performance indicators. Based on these conditions, data requirements are defined and the available data is evaluated considering domain specific characteristics.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.promfg.2019.02.114</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DYNAMIC ASSESSMENT OF PM2.5 EXPOSURE AND HEALTH RISK USING REMOTE SENSING AND GEO-SPATIAL BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood</field>
  <field name="keywords">Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health</field>
  <field name="abstract">In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5 μm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80 μg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.envpol.2019.06.057</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS A CONTENT AGNOSTIC COMPUTABLE KNOWLEDGE REPOSITORY FOR DATA QUALITY ASSESSMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen and Julio C. Facelli</field>
  <field name="keywords">Data Quality Metadata Repository, Knowledge representation, Data quality assessment, Data quality dimensions, Data quality framework</field>
  <field name="abstract">Background and objective
In recent years, several data quality conceptual frameworks have been proposed across the Data Quality and Information Quality domains towards assessment of quality of data. These frameworks are diverse, varying from simple lists of concepts to complex ontological and taxonomical representations of data quality concepts. The goal of this study is to design, develop and implement a platform agnostic computable data quality knowledge repository for data quality assessments.
Methods
We identified computable data quality concepts by performing a comprehensive literature review of articles indexed in three major bibliographic data sources. From this corpus, we extracted data quality concepts, their definitions, applicable measures, their computability and identified conceptual relationships. We used these relationships to design and develop a data quality meta-model and implemented it in a quality knowledge repository.
Results
We identified three primitives for programmatically performing data quality assessments: data quality concept, its definition, its measure or rule for data quality assessment, and their associations. We modeled a computable data quality meta-data repository and extended this framework to adapt, store, retrieve and automate assessment of other existing data quality assessment models.
Conclusion
We identified research gaps in data quality literature towards automating data quality assessments methods. In this process, we designed, developed and implemented a computable data quality knowledge repository for assessing quality and characterizing data in health data repositories. We leverage this knowledge repository in a service-oriented architecture to perform scalable and reproducible framework for data quality assessments in disparate biomedical data sources.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cmpb.2019.05.017</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SUSTAINABLE RESOURCE ALLOCATION FOR POWER GENERATION: THE ROLE OF BIG DATA IN ENABLING INTERINDUSTRY ARCHITECTURAL INNOVATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary</field>
  <field name="keywords">Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming</field>
  <field name="abstract">Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5 min of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.techfore.2018.04.031</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA QUALITY CHALLENGES FOR SUSTAINABLE FASHION SUPPLY CHAIN OPERATIONS IN EMERGING MARKETS: ROLES OF BLOCKCHAIN, GOVERNMENT SPONSORS AND ENVIRONMENT TAXES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Tsan-Ming Choi and Suyuan Luo</field>
  <field name="keywords">Fashion business operations, Supply chain centralization, Emerging markets, Sustainable operations, Social welfare</field>
  <field name="abstract">In emerging markets, there are data quality problems. In this paper, we establish theoretical models to explore how data quality problems affect sustainable fashion supply chain operations. We start with the decentralized supply chain and find that poor data quality lowers supply chain profit and social welfare. We consider the implementation of blockchain to help and identify the situation in which blockchain helps enhance social welfare but brings harm to supply chain profitability. We propose a government sponsor scheme as well as an environment taxation waiving scheme to help. We further extend the study to the centralized supply chain setting.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.tre.2019.09.019</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ANOMALY DETECTION AND REPAIR FOR ACCURATE PREDICTIONS IN GEO-DISTRIBUTED BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz</field>
  <field name="keywords">Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation, Neural networks, Gradient-boosting</field>
  <field name="abstract">The increasing presence of geo-distributed sensor networks implies the generation of huge volumes of data from multiple geographical locations at an increasing rate. This raises important issues which become more challenging when the final goal is that of the analysis of the data for forecasting purposes or, more generally, for predictive tasks. This paper proposes a framework which supports predictive modeling tasks from streaming data coming from multiple geo-referenced sensors. In particular, we propose a distance-based anomaly detection strategy which considers objects described by embedding features learned via a stacked auto-encoder. We then devise a repair strategy which repairs the data detected as anomalous exploiting non-anomalous data measured by sensors in nearby spatial locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast values assumed by a target variable of interest for the repaired newly arriving (unlabeled) data, using the original feature representation or the embedding feature representation learned via the stacked auto-encoder. The workflow is implemented with distributed Apache Spark programming primitives and tested on a cluster environment. We perform experiments to assess the performance of each module, separately and in a combined manner, considering the predictive modeling of one-day-ahead energy production, for multiple renewable energy sites. Accuracy results show that the proposed framework allows reducing the error up to 13.56%. Moreover, scalability results demonstrate the efficiency of the proposed framework in terms of speedup, scaleup and execution time under a stress test.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.bdr.2019.04.001</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 9 - INTELLIGENCE-BASED HEALTH RECOMMENDATION SYSTEM USING BIG DATA ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das</field>
  <field name="keywords">Big data analytics, Classification, Healthcare, Privacy preservation, Recommendation system</field>
  <field name="abstract">In today's digital world, healthcare is one of the core areas in the medical domain. A healthcare system is required to analyze a large amount of patient data, which helps to derive insights and predictions of disease. This system should be intelligent and able to predict the patient's health condition by analyzing the patient's lifestyle, physical health records, and social activities. The health recommendation system (HRS) is becoming an important platform for healthcare services. In this context, health intelligent systems have become indispensable tools in decision-making processes in the healthcare sector. The main objective is to ensure the availability of valuable information at the right time by ensuring information quality, trustworthiness, authentication, and privacy. As people use social networks to learn about their health condition, so the HRS is very important to derive outcomes such as recommending diagnosis, health insurance, clinical pathway-based treatment methods, and alternative medicines based on the patient's health profile. In this chapter, we discuss recent research that targeted utilization of large volumes of medical data while combining multimodal data from disparate sources, which reduces the workload and cost in healthcare. In the healthcare sector, big data analytics using a recommendation system has an important role in terms of decision-making processes regarding the patient's health. This chapter presents a proposed intelligent HRS that provides an insight into how to use big data analytics for implementing an effective health recommendation engine and shows how to transform the healthcare industry from the traditional scenario to more personalized paradigm in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE value when compared to existing approaches.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-818146-1.00009-X</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A HYBRID IT FRAMEWORK FOR IDENTIFYING HIGH-QUALITY PHYSICIANS USING BIG DATA ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang</field>
  <field name="keywords">Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis</field>
  <field name="abstract">Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijinfomgt.2019.01.005</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TERM STRUCTURE ANALYSIS WITH BIG DATA: ONE-STEP ESTIMATION USING BOND PRICES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch</field>
  <field name="keywords">Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson–Siegel model</field>
  <field name="abstract">Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jeconom.2019.04.019</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA QUALITY IN ETL PROCESS: A PRELIMINARY STUDY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and Sadok Ben Yahia</field>
  <field name="keywords">Business Intelligence & Analytics, ETL quality, Data, process quality, Talend Data Integration, Talend Data Quality</field>
  <field name="abstract">The accuracy and relevance of Business Intelligence & Analytics (BI&A) rely on the ability to bring high data quality to the data warehouse from both internal and external sources using the ETL process. The latter is complex and time-consuming as it manages data with heterogeneous content and diverse quality problems. Ensuring data quality requires tracking quality defects along the ETL process. In this paper, we present the main ETL quality characteristics. We provide an overview of the existing ETL process data quality approaches. We also present a comparative study of some commercial ETL tools to show how much these tools consider data quality dimensions. To illustrate our study, we carry out experiments using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated solution (Talend Data Quality). Based on our study, we identify and discuss quality challenges to be addressed in our future research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.09.223</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ARTIFICIAL INTELLIGENCE FOR INFECTIOUS DISEASE BIG DATA ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang</field>
  <field name="keywords">Infectious diseases modelling, Emergency response, Artificial Intelligence, Machine learning</field>
  <field name="abstract">Background
Since the beginning of the 21st century, the amount of data obtained from public health surveillance has increased dramatically due to the advancement of information and communications technology and the data collection systems now in place.
Methods
This paper aims to highlight the opportunities gained through the use of Artificial Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection in this information age.
Results and Conclusion
It is foreseeable that together with reliable data management platforms AI methods will enable analysis of massive infectious disease and surveillance data effectively to support government agencies, healthcare service providers, and medical professionals to response to disease in the future.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.idh.2018.10.002</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">GREEN INNOVATION AND ORGANIZATIONAL PERFORMANCE: THE INFLUENCE OF BIG DATA AND THE MODERATING ROLE OF MANAGEMENT COMMITMENT AND HR PRACTICES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Abdul-Nasser El-Kassar and Sanjay Kumar Singh</field>
  <field name="keywords">Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance</field>
  <field name="abstract">Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.techfore.2017.12.016</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA QUALITY PROGRAM MANAGEMENT FOR DIGITAL SHADOWS OF PRODUCTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Günther Schuh and Eric Rebentisch and Michael Riesener and Thorben Ipers and Christian Tönnes and Merle-Hendrikje Jank</field>
  <field name="keywords">data quality program, digital shadow, data quality management</field>
  <field name="abstract">Nowadays, companies are facing challenges due to increasingly dynamic market environments, a growing internal and external complexity, as well as globally intensifying competition. To keep pace, companies need to establish extensive knowledge about their business and its surroundings based on insights generated through the analysis of data. The digital shadow is a novel information system concept that integrates data of heterogeneous sources to provide product-related information to stakeholders across the company. The concept aims at improving the results of decision making, enabling advanced data analyses, and increasing information handling efficiency. As insufficient information quality has immediate effects on the utility of the information and induces significant costs, managing the quality of the digital shadow data basis is crucial. However, there are currently no comprehensive methodologies for the assessment and improvement of the data quality of digital shadows. Therefore, this paper introduces a methodology that supports the derivation of data quality projects aimed at optimizing the digital shadow data basis. The proposed methodology comprises four steps: First, digital shadow use cases along the product lifecycle are described. Next, the use cases are prioritized with regard to the expected benefits of applying the digital shadow. Third, quality deficiencies in the digital shadow data basis are assessed with respect to use case specific requirements. Finally, the prioritized use cases in relation with the identified quality deficits allow deriving needs for action, which are addressed by data quality projects. Together, the data quality projects constitute a data quality program. The methodology is applied in an industry case to prove the practical effectivity and efficiency.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procir.2020.01.027</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ENSURING SAFE SURGICAL CARE ACROSS RESOURCE SETTINGS VIA SURGICAL OUTCOMES DATA & QUALITY IMPROVEMENT INITIATIVES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and Michael D. Kluger</field>
  <field name="keywords">Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality improvement, Big data</field>
  <field name="abstract">Staggering statistics regarding the global burden of disease due to lack of surgical care worldwide has been gaining attention in the global health literature over the last 10 years. The Lancet Commission on Global Surgery reported that 16.9 million lives were lost due to an absence of surgical care in 2010, equivalent to 33% of all deaths worldwide. Although data from low- and middle-income countries (LMICs) are limited, recent investigations, such as the African Surgical Outcomes Study, highlight that despite operating on low risk patients, there is increased postoperative mortality in LMICs versus higher-resource settings, a majority of which occur secondary to seemingly preventable complications like surgical site infections. We propose that implementing creative, low-cost surgical outcomes monitoring and select quality improvement systems proven effective in high-income countries, such as surgical infection prevention programs and safety checklists, can enhance the delivery of safe surgical care in existing LMIC surgical systems. While efforts to initiate and expand surgical access and capacity continues to deserve attention in the global health community, here we advocate for creative modifications to current service structures, such as promoting a culture of safety, employing technology and mobile health (mHealth) for patient data collection and follow-up, and harnessing partnerships for information sharing, to create a framework for improving morbidity and mortality in responsible, scalable, and sustainable ways.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijsu.2019.07.036</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PITFALLS IN BIG DATA ANALYSIS: NEXT-GENERATION TECHNOLOGIES, LAST-GENERATION DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Susanna K.P. Lau and Patrick C.Y. Woo</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.diagmicrobio.2018.12.006</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 8 - HEALTHCARE DECISION-MAKING SUPPORT BASED ON THE APPLICATION OF BIG DATA TO ELECTRONIC MEDICAL RECORDS: A KNOWLEDGE MANAGEMENT CYCLE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Javier Carnicero and David Rojas</field>
  <field name="keywords">Big Data, Electronic medical record, Practice-based medicine, Learning health system, Semantic interoperability</field>
  <field name="abstract">Any given health system needs to increase efficiency and effectiveness up to the point of requiring a transformation of their current model to ensure their sustainability and continuity. The electronic medical record (EMR) is the main source of knowledge to improve the quality of healthcare, clinical research, epidemiological surveillance, patient empowerment, personalized medicine, and clinical decision-making support systems. There is also a huge amount of available information related to diseases and other medical conditions, such as drugs and therapies, omics data (genetic and proteomic), social networks, and wearable devices. Big Data technologies allow the processing of this data to reach the final goal, which is a learning health system. The great diversity of data, sources, structures, and uses requires a data linkage procedure to integrate and harmonize these data. This generation of knowledge allows the transition from evidence-based medicine, which still prevails, to practice-based medicine. The key points for any Big Data project based on EMRs and other medical information sources are semantic interoperability, data structure and granularity, information quality, patient privacy, legal framework, and bioethics.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-809556-0.00008-3</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">HOW BIG DATA ENRICHES MARITIME RESEARCH – A CRITICAL REVIEW OF AUTOMATIC IDENTIFICATION SYSTEM (AIS) DATA APPLICATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li</field>
  <field name="keywords">AIS data, data mining, navigation safety, ship behaviour analysis, environmental evaluation, advanced applications of AIS data</field>
  <field name="abstract">ABSTRACT
The information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1080/01441647.2019.1649315</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and H. Li</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jval.2019.04.303</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA FOR ECOLOGICAL MODELS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Marin M. Kress</field>
  <field name="keywords">Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery, Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine readable, Metadata, Remote sensing, Social media</field>
  <field name="abstract">The use of data repositories for parameterizing ecological models and storing model runs is becoming more common, yet often these data archives do not contain the appropriate metadata, nor are they maintained for others to use. Data archiving and sharing are additional steps in the scientific process that add value to a researcher׳s work, and more importantly, facilitate transparency and repeatability of a researcher׳s work. Historically, peer-reviewed publications did not allow for the full presentation of underlying datasets, which were only shared through personal contact with a scientist. However, with the expanding use of “supporting online material” (SOM) files that accompany digital publication there is an increased expectation that even large datasets can be made accessible to readers. Thus, researchers are faced with the additional task of becoming their own archivist and depositing data in a repository where it can be used by others. This article introduces basic concepts in data archiving and sharing, including major digital repositories for life science data, commonly used digital file formats, and why metadata is an essential element to successful data sharing when machine-readable data is increasingly used in large-scale studies.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-409548-9.10557-3</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 4 - BIG DATA, ARTIFICIAL INTELLIGENCE, AND MACHINE LEARNING IN NEUROTRAUMA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Denes V. Agoston</field>
  <field name="keywords">Big Data, Artificial intelligence and machine learning in neurotrauma</field>
  <field name="abstract">Rapid advances in the collection, storage, and analysis of large volumes of data—Big Data—offer the much-needed help to identify and treat the various pathological conditions triggered by traumatic brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and mostly unstructured data that cannot be analyzed using traditional approaches. BD can be only analyzed by using text mining (TM), artificial intelligence (AI), or machine learning (ML). These approaches can reveal patterns, trends, and associations, critical for understanding the “most complex disease of the most complex organ.” While powerful and successfully tested computational tools are available, using BD approaches in TBI is currently hampered by the limited availability of legacy and/or primary data, by incompatible data formats and standards. This chapter introduces Big Data and Big Data approaches such as text mining, artificial intelligence, and machine learning; outlines the benefits of using BD approaches; and suggests potential solutions that can help using the full potential of BD in TBI. It also identifies necessary changes of how researchers can help ushering in a new era of preclinical and clinical TBI research by recording and storing ALL the data generated and making ALL the data available for BD approaches—text mining, artificial intelligence, and machine learning so new correlations, relationships, and trends can be identified. In turn, these new information will help to develop novel diagnostics, evidence-based treatments, and improve outcomes.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-809556-0.00004-6</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEEP LEARNING IN BIG DATA ANALYTICS: A COMPARATIVE STUDY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon</field>
  <field name="keywords">Big data, Deep learning, Deep belief networks, Convolutional Neural Networks</field>
  <field name="abstract">Deep learning methods are extensively applied to various fields of science and engineering such as speech recognition, image classifications, and learning methods in language processing. Similarly, traditional data processing techniques have several limitations of processing large amount of data. In addition, Big Data analytics requires new and sophisticated algorithms based on machine and deep learning techniques to process data in real-time with high accuracy and efficiency. However, recently, research incorporated various deep learning techniques with hybrid learning and training mechanisms of processing data with high speed. Most of these techniques are specific to scenarios and based on vector space thus, shows poor performance in generic scenarios and learning features in big data. In addition, one of the reason of such failure is high involvement of humans to design sophisticated and optimized algorithms based on machine and deep learning techniques. In this article, we bring forward an approach of comparing various deep learning techniques for processing huge amount of data with different number of neurons and hidden layers. The comparative study shows that deep learning techniques can be built by introducing a number of methods in combination with supervised and unsupervised training techniques.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.compeleceng.2017.12.009</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE PERILS AND PITFALLS OF BIG DATA ANALYSIS IN MEDICINE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">C.J. Puranik and Sreenivasa Rao and S. Chennamaneni</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jtos.2019.07.010</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA AND MACHINE LEARNING IN CRITICAL CARE: OPPORTUNITIES FOR COLLABORATIVE RESEARCH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Antonio {Núñez Reiz} and Fernando {Martínez Sagasti} and Manuel {Álvarez González} and Antonio {Blesa Malpica} and Juan Carlos {Martín Benítez} and Mercedes {Nieto Cabrera} and Ángela {del Pino Ramírez} and José Miguel {Gil Perdomo} and Jesús {Prada Alonso} and Leo Anthony Celi and Miguel Ángel {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo {González Ferrer} and Ángel Rey and Henar {González Luengo} and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura López and Beatriz Merino and María Fernanda Cabrera and María Teresa Arredondo and María Bodí and Josep Gómez and Alejandro Rodríguez and Miguel {Sánchez García}</field>
  <field name="keywords">Big data, Machine learning, Artificial intelligence, Clinical databases, MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases de datos clínicos, MIMIC III, Datathon, Trabajo colaborativo</field>
  <field name="abstract">The introduction of clinical information systems (CIS) in Intensive Care Units (ICUs) offers the possibility of storing a huge amount of machine-ready clinical data that can be used to improve patient outcomes and the allocation of resources, as well as suggest topics for randomized clinical trials. Clinicians, however, usually lack the necessary training for the analysis of large databases. In addition, there are issues referred to patient privacy and consent, and data quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning experts, statisticians, epidemiologists and other information scientists may overcome these problems. A multidisciplinary event (Critical Care Datathon) was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of the Spanish Critical Care Society (SEMICYUC), the event was organized by the Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge, MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University. After presentations referred to big data in the critical care environment, clinicians, data scientists and other health data science enthusiasts and lawyers worked in collaboration using an anonymized database (MIMIC III). Eight groups were formed to answer different clinical research questions elaborated prior to the meeting. The event produced analyses for the questions posed and outlined several future clinical research opportunities. Foundations were laid to enable future use of ICU databases in Spain, and a timeline was established for future meetings, as an example of how big data analysis tools have tremendous potential in our field.
Resumen
La aparición de los sistemas de información clínica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad de almacenar una ingente cantidad de datos clínicos en formato electrónico durante el ingreso de los pacientes. Estos datos pueden ser empleados posteriormente para obtener respuestas a preguntas clínicas, para su uso en la gestión de recursos o para sugerir líneas de investigación que luego pueden ser explotadas mediante ensayos clínicos aleatorizados. Sin embargo, los médicos clínicos carecen de la formación necesaria para la explotación de grandes bases de datos, lo que supone un obstáculo para aprovechar esta oportunidad. Además, existen cuestiones de índole legal (seguridad, privacidad, consentimiento de los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta. El trabajo multidisciplinar con otros profesionales (analistas de datos, estadísticos, epidemiólogos, especialistas en derecho aplicado a grandes bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta para investigación clínica o análisis de resultados (benchmarking). Se describe la reunión multidisciplinar (Critical Care Datathon) realizada en Madrid los días 1, 2 y 3 de diciembre de 2017. Esta reunión, celebrada bajo los auspicios de la Sociedad Española de Medicina Intensiva, Crítica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada por el Massachusetts Institute of Technology (MIT), la Unidad de Innovación y el Servicio de Medicina Intensiva del Hospital Clínico San Carlos, así como el grupo de investigación «Life Supporting Technologies» de la Universidad Politécnica de Madrid. Tras unas ponencias de formación sobre big data, seguridad y calidad de los datos, y su aplicación al entorno de la medicina intensiva, un grupo de clínicos, analistas de datos, estadísticos, expertos en seguridad informática de datos realizaron sesiones de trabajo colaborativo en grupos utilizando una base de datos reales anonimizada (MIMIC III), para analizar varias preguntas clínicas establecidas previamente a la reunión. El trabajo colaborativo permitió establecer resultados relevantes con respecto a las preguntas planteadas y esbozar varias líneas de investigación clínica a desarrollar en el futuro. Además, se sentaron las bases para poder utilizar las bases de datos de las UCI con las que contamos en España, y se estableció un calendario de trabajo para planificar futuras reuniones contando con los datos de nuestras unidades. El empleo de herramientas de big data y el trabajo colaborativo con otros profesionales puede permitir ampliar los horizontes en aspectos como el control de calidad de nuestra labor cotidiana, la comparación de resultados entre unidades o la elaboración de nuevas líneas de investigación clínica.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.medin.2018.06.002</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA SHARING AND ANALYSIS TO ADVANCE RESEARCH IN POST-TRAUMATIC EPILEPSY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dominique Duncan and Paul Vespa and Asla Pitkänen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga</field>
  <field name="keywords">Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI</field>
  <field name="abstract">We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.nbd.2018.05.026</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA AND TARGETED MACHINE LEARNING IN ACTION TO ASSIST MEDICAL DECISION IN THE ICU</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon and Alan Hubbard</field>
  <field name="keywords">nan</field>
  <field name="abstract">Historically, personalised medicine has been synonymous with pharmacogenomics and oncology. We argue for a new framework for personalised medicine analytics that capitalises on more detailed patient-level data and leverages recent advances in causal inference and machine learning tailored towards decision support applicable to critically ill patients. We discuss how advances in data technology and statistics are providing new opportunities for asking more targeted questions regarding patient treatment, and how this can be applied in the intensive care unit to better predict patient-centred outcomes, help in the discovery of new treatment regimens associated with improved outcomes, and ultimately how these rules can be learned in real-time for the patient.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.accpm.2018.09.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">HOW PHYSICAL EXERCISE LEVEL AFFECTS SLEEP QUALITY? ANALYZING BIG DATA COLLECTED FROM WEARABLES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha Röning</field>
  <field name="keywords">Data analytics, wearables, sleep quality, statistical methods</field>
  <field name="abstract">Physical exercise and sleep have independent, yet synergistic, impacts on the health. However, the effects of acute exercise level on sleep quality have not been well investigated. We utilize statistical methods to investigate the differences of exercise level between the good and bad sleep nights. Our results present a complex interrelation between physical exercise and sleep quality with analyzing large personal data sets collected from wearables. As far as we know, this is the first study to investigate insights of interrelation of physical exercise and sleep quality based on a big volume of data collected from wearable devices of real users.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.08.035</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 8 - WHY BIG DATA AND WHAT IS IT? BASIC TO ADVANCED BIG DATA JOURNEY FOR THE MEDICAL INDUSTRY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Neha Sharma and Malini M. Patil and Madhavi Shamkuwar</field>
  <field name="keywords">Big data, Medical big data, Healthcare data, Medical big data analytics, Healthcare data analytics, Data analytics, Pharmacology data analytics</field>
  <field name="abstract">The idea of big data is mainly reflected in its dimensions, which are popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses, to focus on data analysis, hypothesis generation, and ascertaining the progressive strength of association. Preliminary study reveals that big data analytics adopts many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive analytics. This evolving technology has tremendous application in healthcare, such as surveillance of safety or disease, predictive modeling, public health, pharma data analytics, clinical data analytics, healthcare analytics, and research. Moreover, the journey of big data in the medical domain is proving to be one of the important research thrusts of recent times. Study reveals that medical data is very specific and heterogeneous due to varied data sources such as scanned images, CT scan reports, doctor prescriptions, electronic health records (EHRs), etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions, bias, and limitations of the study of patients through observation. Therefore, special big data techniques are required to handle them. Besides, many ethical, legal, social, clinical, and utility challenges are also a part of the data-handling process, which makes the role of big data in the medical field very challenging. Nevertheless, big data analytics is a fuel to the healthcare system that will provide a healthier life to patients; the issues and bottlenecks when removed from the system will be a boon for the entire human race. The chapter focuses on understanding the big data characteristics in medical big data, medical big data analytics, and its various applications in the interest of society.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-817356-5.00010-3</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA EXPLORATION, VISUALIZATION AND ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.bdr.2019.100123</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IS BIG DATA ABOUT TO RETIRE EXPERT KNOWLEDGE? A PREDICTIVE MAINTENANCE STUDY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Domingo Llorente Rivera and Markus R. Scholz and Christoph Bühl and Markus Krauss and Klaus Schilling</field>
  <field name="keywords">industrial analytics, anomaly detection, predictive maintenance, hydraulic pump, stochastic modeling</field>
  <field name="abstract">In this contribution, a data-driven approach towards the prediction of maintenance for the critical component of an injection molding machine is presented. We present our path from exploring and cleaning the data towards the implementation of a prediction algorithm based on kernel density estimation. We give first analytical evidence of the algorithms potential. Moreover, we compare the approach described here with our previous work where we went a model-based approach and present advantages and disadvantages of the two approaches. We try to contribute to a non-comprehensive guide on the implementation of predictive maintenance systems for industrial mass production facilities.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ifacol.2019.12.364</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">NEXT GENERATION SMART SUSTAINABLE AUDITING SYSTEMS USING BIG DATA ANALYTICS: UNDERSTANDING THE INTERACTION OF CRITICAL BARRIERS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Manish Shukla and Lana Mattar</field>
  <field name="keywords">Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive Structural Modelling</field>
  <field name="abstract">In the current scenario, sustainable auditing, for example roundtable of sustainable palm oil (RSPO), requires a huge amount of data to be manually collected and entered into paper forms by farmers. Such systems are inherently inefficient, time-consuming, and, prone to errors. Researchers have proposed Big Data Analytics (BDA) based framework for next-generation smart sustainable auditing systems. Though theoretically feasible, real-life implementation of such frameworks is extremely difficult. Thus, this paper aims to identify the critical barriers that hinder the application of BDA based smart sustainable auditing system. It also aims to explore the dynamic interrelations among the barriers. We applied Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates BDA adoption barriers and their relationships. The proposed model illustrates how barriers are spread over various levels and how specific barriers impact other barriers through direct and/or transitive links. This study provides practitioners with a roadmap to prioritise the interventions to facilitate the adoption of BDA in the sustainable auditing systems. Insights of this study could be used by academics to enhance understanding of the barriers to BDA applications.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cie.2018.04.055</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">OPPORTUNITIES AND CHALLENGES OF USING BIG DATA FOR GLOBAL HEALTH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.scib.2019.09.011</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ENERGY FORECASTING IN THE BIG DATA WORLD</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Tao Hong and Pierre Pinson</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijforecast.2019.05.004</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IMPLICATIONS OF BIG DATA ANALYTICS IN DEVELOPING HEALTHCARE FRAMEWORKS – A REVIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Venketesh Palanisamy and Ramkumar Thirunavukarasu</field>
  <field name="keywords">Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns, Tools</field>
  <field name="abstract">The domain of healthcare acquired its influence by the impact of big data since the data sources involved in the healthcare organizations are well-known for their volume, heterogeneous complexity and high dynamism. Though the role of big data analytical techniques, platforms, tools are realized among various domains, their impact on healthcare organization for implementing and delivering novel use-cases for potential healthcare applications shows promising research directions. In the context of big data, the success of healthcare applications solely depends on the underlying architecture and utilization of appropriate tools as evidenced in pioneering research attempts. Novel research works have been carried out for deriving application specific healthcare frameworks that offer diversified data analytical capabilities for handling sources of data ranging from electronic health records to medical images. In this paper, we have presented various analytical avenues that exist in the patient-centric healthcare system from the perspective of various stakeholders. We have also reviewed various big data frameworks with respect to underlying data sources, analytical capability and application areas. In addition, the implication of big data tools in developing healthcare eco system is also presented.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jksuci.2017.12.007</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EFFICIENT LEARNING FROM BIG DATA FOR CANCER RISK MODELING: A CASE STUDY WITH MELANOMA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Aaron N. Richter and Taghi M. Khoshgoftaar</field>
  <field name="keywords">Big data, Cloud computing, Machine learning, Electronic health records, Early detection of cancer</field>
  <field name="abstract">Background
Building cancer risk models from real-world data requires overcoming challenges in data preprocessing, efficient representation, and computational performance. We present a case study of a cloud-based approach to learning from de-identified electronic health record data and demonstrate its effectiveness for melanoma risk prediction.
Methods
We used a hybrid distributed and non-distributed approach to computing in the cloud: distributed processing with Apache Spark for data preprocessing and labeling, and non-distributed processing for machine learning model training with scikit-learn. Moreover, we explored the effects of sampling the training dataset to improve computational performance. Risk factors were evaluated using regression weights as well as tree SHAP values.
Results
Among 4,061,172 patients who did not have melanoma through the 2016 calendar year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted classifier achieved the best predictive performance with cross-validation (AUC = 0.799, Sensitivity = 0.753, Specificity = 0.688). Compared to a model built on the original data, a dataset two orders of magnitude smaller could achieve statistically similar or better performance with less than 1% of the training time and cost.
Conclusions
We produced a model that can effectively predict melanoma risk for a diverse dermatology population in the U.S. by using hybrid computing infrastructure and data sampling. For this de-identified clinical dataset, sampling approaches significantly shortened the time for model building while retaining predictive accuracy, allowing for more rapid machine learning model experimentation on familiar computing machinery. A large number of risk factors (>300) were required to produce the best model.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.compbiomed.2019.04.039</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE APPLICATION OF BIG DATA AND THE DEVELOPMENT OF NURSING SCIENCE: A DISCUSSION PAPER</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and Zhiguang Duan</field>
  <field name="keywords">Artificial intelligence, Data mining, Knowledge bases, Nursing</field>
  <field name="abstract">Based on the concept and research status of big data, we analyze and examine the importance of constructing the knowledge system of nursing science for the development of the nursing discipline in the context of big data and propose that it is necessary to establish big data centers for nursing science to share resources, unify language standards, improve professional nursing databases, and establish a knowledge system structure.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijnss.2019.03.001</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SPATIO-TEMPORALLY WEIGHTED HYBRID MODEL TO IMPROVE ESTIMATES OF PERSONAL PM2.5 EXPOSURE: INCORPORATING BIG DATA FROM MULTIPLE DATA SOURCES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong</field>
  <field name="keywords">Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai</field>
  <field name="abstract">An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5 μm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3 ± 27.1 μg/m3 (n = 365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1 ± 13.9 μg/m3 (n = 365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.envpol.2019.07.034</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IOT BIG DATA ANALYTICS FOR SMART HOMES WITH FOG AND CLOUD COMPUTING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam Muhammad</field>
  <field name="keywords">Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics, Energy management, Smart homes</field>
  <field name="abstract">Internet of Things (IoT) analytics is an essential mean to derive knowledge and support applications for smart homes. Connected appliances and devices inside the smart home produce a significant amount of data about consumers and how they go about their daily activities. IoT analytics can aid in personalizing applications that benefit both homeowners and the ever growing industries that need to tap into consumers profiles. This article presents a new platform that enables innovative analytics on IoT captured data from smart homes. We propose the use of fog nodes and cloud system to allow data-driven services and address the challenges of complexities and resource demands for online and offline data processing, storage, and classification analysis. We discuss in this paper the requirements and the design components of the system. To validate the platform and present meaningful results, we present a case study using a dataset acquired from real smart home in Vancouver, Canada. The results of the experiments show clearly the benefit and practicality of the proposed platform.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.future.2018.08.040</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SMART HEALTHCARE FRAMEWORK FOR AMBIENT ASSISTED LIVING USING IOMT AND BIG DATA ANALYTICS TECHNIQUES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi</field>
  <field name="keywords">Ambient Assisted Living (AAL), Big data analytics, Internet of Medical Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors</field>
  <field name="abstract">In the era of pervasive computing, human living has become smarter by the latest advancements in IoMT (Internet of Medical Things), wearable sensors and telecommunication technologies in order to deliver smart healthcare services. IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects wearable sensors, patients, healthcare providers and caregivers via software and ICT (Information and Communication Technology). AAL (Ambient Assisted Living) enables integration of new technologies to be part of our daily life activities. In this paper, we have provided a novel smart healthcare framework for AAL to monitor the physical activities of elderly people using IoMT and intelligent machine learning algorithms for faster analysis, decision making and better treatment recommendations. Data is collected from multiple wearable sensors placed on subject’s left ankle, right arm, and chest, is transmitted through IoMT devices to the integrated cloud and data analytics layer. To process huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial Naïve Bayes classifier, which fits into the MapReduce paradigm, is utilized to recognize the motion experienced by different body parts and provides higher scalability and better performance with parallel processing when compared to serial processor. Our proposed framework predicts 12 physical activities with an overall accuracy of 97.1%. This can be considered as an optimal solution for recognizing physical activities to remotely monitor health conditions of elderly people.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.future.2019.06.004</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHAPTER 8 - BIG DATA, PRIVACY AND SECURITY IN SMART GRIDS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ersan Kabalci and Yasin Kabalci</field>
  <field name="keywords">Internet of things (IoT), Machine-to-machine communication (M2M), Human to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining, Security, Privacy</field>
  <field name="abstract">The smart grid applications are related with monitoring and control operations of conventional power grid. The integration of information and communication technologies (ICT) to existing power network has leveraged interaction of different generators, controllers, monitoring and measurement devices, and intelligent loads. The two-way communication infrastructure is comprised by numerous sensor networks that increased deployment of massive data from measurement nodes to monitoring centers. Big data is a widespread concept which become a trend for massive data streams that are transferred and processed in an ecosystem. The enormous amount of data are generated, transferred and stored to improve operating and management quality of smart grid. The big data analytics are performed to improve quality of service in terms of grid operators and consumers. The big data acquisition, processing, storing, and clustering stages are widely researched by a wide variety of specialist. In this chapter, the all these stages, big data acquisition technologies, machine learning methods used in big data analytics, privacy and security of big data infrastructure are introduced in detail. The privacy preserving methods, big data processing technologies and firmware infrastructures are presented in the context of this chapter.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/B978-0-12-819710-3.00008-9</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ON MODELLING BIG DATA GUIDED SUPPLY CHAINS IN KNOWLEDGE-BASE GEOGRAPHIC INFORMATION SYSTEMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood</field>
  <field name="keywords">Supply Chain Management, Project Management, Laws of Geography, Domain Ontologies, Data Mining</field>
  <field name="abstract">We examine the existing goals of business- and geographic - information systems and their influence on logistics and supply chain management systems. Modelling supply chain management systems is held back because of lack of consistent and poorly aligned data with supply chain elements and processes. The issues constraining the decision-making process limit the connectivity between supply chains and geographically controlled database systems. The heterogeneous and unstructured data are added challenges to connectivity and integration processes. The research focus is on analysing the data heterogeneity and multidimensionality relevant to supply chain systems and geographically controlled databases. In pursuance of the challenges, a unified methodological framework is designed with data structuring, data warehousing and mining, visualization and interpretation artefacts to support connectivity and integration process. Multidimensional ontologies, ecosystem conceptualization and Big Data novelty are added motivations, facilitating the relationships between events of supply chain operations. The models construed for optimizing the resources are analysed in terms of effectiveness of the integrated framework articulations in global supply chains that obey laws of geography. The integrated articulations analysed with laws of geography can affect the operational costs, sure for better with reduced lead times and enhanced stock management.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.09.284</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">LITHIUM-ION BATTERY MODELING BASED ON BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang</field>
  <field name="keywords">electric vehicle, lithium-ion power battery, modeling, battery management, bigdata, deeplearning</field>
  <field name="abstract">Battery is the bottleneck technology of electric vehicles. The complex chemical reactions inside the battery are difficult to monitor directly. The establishment of a precise mathematical model for the battery is of great significance in ensuring the secure and stable operation of the battery management system. First of all, a data cleaning method based on machine learning is put forward, which is applicable to the characteristics of big data from batteries in electric vehicles. Secondly, this paper establishes a lithium-ion battery model based on deep learning algorithm and the error of model based on different algorithms is compared. The data of electric buses are used for validating the effectiveness of the model. The result shows that the data cleaning method achieves good results, in the case of the terminal voltage missing, the mean absolute percentage error of filling is within 4%, and the battery modeling method in this paper is able to simulate the battery characteristics accurately, and the mean absolute percentage error of the terminal voltage estimation is within 2.5%.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.egypro.2018.12.046</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE ELECTRONIC MEDICAL RECORD: BIG DATA, LITTLE INFORMATION?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Walter Verbrugghe and Kirsten Colpaert</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jcrc.2019.09.005</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AVERAGE RESTRAIN DIVIDER OF EVALUATION VALUE (ARDEV) IN DATA STREAM ALGORITHM FOR BIG DATA PREDICTION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ari Wibisono and Devvi Sarwinda</field>
  <field name="keywords">ARDEV, Big data prediction, FIMT-DD, Tree regression</field>
  <field name="abstract">Today, big data processing has become a challenging task due to the amount of data collected using various sensors increasingly significantly. To build knowledge and predict the data, traditional data mining methods calculate all numerical attributes into the memory simultaneously. The data stream method is a solution for processing and calculating data. The method streams incrementally in batch form; therefore, infrastructure memory is sufficient to develop knowledge. The existing method for data stream prediction is FIMT-DD (Fast Incremental Model Tree-Drift Detection). Using this method, knowledge is developed in tree form for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach with error evaluation, improvement in learning rate, modification of perceptron rule calculation, and utilization of activation function. Standard FIMT-DD separates the tree formation process and perceptron prediction. The proposed method evaluates and connects the development of the tree for knowledge formation and the perceptron rule for prediction. The prediction accuracy of the proposed method is measured using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV enhancement shows significant improvement in terms of accuracy prediction. Statistically, the overall accuracy prediction improvement is approximately 6.99 % compared to standard FIMT-DD with a traffic dataset.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.knosys.2019.03.019</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DOES BIG DATA ENHANCE FIRM INNOVATION COMPETENCY? THE MEDIATING ROLE OF DATA-DRIVEN INSIGHTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Maryam Ghasemaghaei and Goran Calic</field>
  <field name="keywords">Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency</field>
  <field name="abstract">Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jbusres.2019.07.006</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">MACHINE LEARNING AND BIG DATA: IMPLICATIONS FOR DISEASE MODELING AND THERAPEUTIC DISCOVERY IN PSYCHIATRY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger S. McIntyre</field>
  <field name="keywords">Big data, Machine learning, Precision medicine, AI, Mental health, Mental disease, Psychiatry, Data mining, RDoC, Research domain criteria, DSM-5. Schizophrenia, ADHD, Alzheimer, Depression, fMRI, MRI, Algorithms, IBM Watson, Neuro networking, Random forests, Decision trees, Support vector machines</field>
  <field name="abstract">Introduction
Machine learning capability holds promise to inform disease models, the discovery and development of novel disease modifying therapeutics and prevention strategies in psychiatry. Herein, we provide an introduction on how machine learning/Artificial Intelligence (AI) may instantiate such capabilities, as well as provide rationale for its application to psychiatry in both research and clinical ecosystems.
Methods
Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health, Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles selected for review were those that were determined to be aligned with the objective of this particular paper.
Results
Results indicate that AI is a viable option to build useful predictors of outcome while offering objective and comparable accuracy metrics, a unique opportunity, particularly in mental health research. The approach has also consistently brought notable insight into disease models through processing the vast amount of already available multi-domain, semi-structured medical data. The opportunity for AI in psychiatry, in addition to disease-model refinement, is in characterizing those at risk, and it is likely also relevant to personalizing and discovering therapeutics.
Conclusions
Machine learning currently provides an opportunity to parse disease models in complex, multi-factorial disease states (e.g. mental disorders) and could possibly inform treatment selection with existing therapies and provide bases for domain-based therapeutic discovery.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.artmed.2019.101704</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SYSTEMATIC LITERATURE REVIEW ON BIG DATA FOR SOLAR PHOTOVOLTAIC ELECTRICITY GENERATION FORECASTING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza</field>
  <field name="keywords">Systematic Literature Review, Solar energy forecasting, Machine learning, Data mining</field>
  <field name="abstract">Solar power is expected to play a substantial role globally, due to it being one of the leading renewable electricity sources for future use. Even though the use of solar irradiation to generate electricity is currently at a fast deployment pace and technological evolution, its natural variability still presents an important barrier to overcome. Machine learning and data mining techniques arise as alternatives to aid solar electricity generation forecast reducing the impacts of its natural inconstant power supply. This paper presents a literature review on big data models for solar photovoltaic electricity generation forecasts, aiming to evaluate the most applicable and accurate state-of-art techniques to the problem, including the motivation behind each project proposal, the characteristics and quality of data used to address the problem, among other issues. A Systematic Literature Review (SLR) method was used, in which research questions were defined and translated into search strings. The search returned 38 papers for final evaluation, affirming that the use of these models to predict solar electricity generation is currently an ongoing academic research question. Machine learning is widely used, and neural networks is considered the most accurate algorithm. Extreme learning machine learning has reduced time and raised precision.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.seta.2018.11.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ASSESSING RELIABILITY OF BIG DATA KNOWLEDGE DISCOVERY PROCESS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi</field>
  <field name="keywords">Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining</field>
  <field name="abstract">Extracting knowledge from Big Data is the process of transforming this data into actionable information. The exponential growth of data has initiated a myriad of new opportunities, and made data become the most valuable raw material of production for many organizations. Mining Big Data is coupled with some challenges, known as the 3V’s of Big Data: Volume, Variety and Velocity. However, a major challenge that needs to be addressed, and often is ignored in the literature, concerns reliability. Actually, data is agglomerated from multiple disparate sources, and each of Knowledge Discovery (KDD) process steps may be carried out by different organizations. These considerations lead us to ask a critical question that is weather the information we have at each step is reliable enough to proceed to the next one? This paper therefore aims to provide a framework that automatically assesses reliability of the knowledge discovery process. We focus on Linked Open Data (LOD) as a source of data, as it constitutes a relevant data provider in many Big Data applications. However, our framework can also be adapted for unstructured data. This framework will assist scientists to automatically and efficiently measure the reliability of each KDD process stage as well as detect unreliable steps that should be revised. Following this methodology, KDD process will be optimized and therefore produce knowledge with higher quality.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.01.005</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BARRIERS TO BIG DATA ANALYTICS IN MANUFACTURING SUPPLY CHAINS: A CASE STUDY FROM BANGLADESH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh Shukla</field>
  <field name="keywords">AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication technology (ICT), Manufacturing supply chains</field>
  <field name="abstract">Recently, big data (BD) has attracted researchers and practitioners due to its potential usefulness in decision-making processes. Big data analytics (BDA) is becoming increasingly popular among manufacturing companies as it helps gain insights and make decisions based on BD. However, there many barriers to the adoption of BDA in manufacturing supply chains. It is therefore necessary for manufacturing companies to identify and examine the nature of each barrier. Previous studies have mostly built conceptual frameworks for BDA in a given situation and have ignored examining the nature of the barriers to BDA. Due to the significance of both BD and BDA, this research aims to identify and examine the critical barriers to the adoption of BDA in manufacturing supply chains in the context of Bangladesh. This research explores the existing body of knowledge by examining these barriers using a Delphi-based analytic hierarchy process (AHP). Data were obtained from five Bangladeshi manufacturing companies. The findings of this research are as follows: (i) data-related barriers are most important, (ii) technology-related barriers are second, and (iii) the five most important components of these barriers are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy, (d) lack of availability of BDA tools and (e) high cost of investment. The findings can assist industrial managers to understand the actual nature of the barriers and potential benefits of using BDA and to make policy regarding BDA adoption in manufacturing supply chains. A sensitivity analysis was carried out to justify the robustness of the barrier rankings.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cie.2018.04.013</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CIRCULAR ECONOMY AND BIG DATA ANALYTICS: A STAKEHOLDER PERSPECTIVE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santibañez Gonzalez}</field>
  <field name="keywords">Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability</field>
  <field name="abstract">The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.techfore.2018.06.030</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA DRIVEN LITHIUM-ION BATTERY MODELING METHOD BASED ON SDAE-ELM ALGORITHM AND DATA PRE-PROCESSING TECHNOLOGY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shuangqi Li and Hongwen He and Jianwei Li</field>
  <field name="keywords">Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning</field>
  <field name="abstract">As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.apenergy.2019.03.154</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">UNLOCKING THE DRIVERS OF BIG DATA ANALYTICS VALUE IN FIRMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Nadine Côrte-Real and Pedro Ruivo and Tiago Oliveira and Aleš Popovič</field>
  <field name="keywords">IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage</field>
  <field name="abstract">Although big data analytics (BDA) is considered the next “frontier” in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jbusres.2018.12.072</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">MEDICAL DATA QUALITY ASSESSMENT: ON THE DEVELOPMENT OF AN AUTOMATED FRAMEWORK FOR MEDICAL DATA CURATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Vasileios C. Pezoulas and Konstantina D. Kourou and Fanis Kalatzis and Themis P. Exarchos and Aliki Venetsanopoulou and Evi Zampeli and Saviana Gandolfo and Fotini Skopouli and Salvatore {De Vita} and Athanasios G. Tzioufas and Dimitrios I. Fotiadis</field>
  <field name="keywords">Big data, Data quality, Data quality assessment, Data curation, Data standardization</field>
  <field name="abstract">Data quality assessment has gained attention in the recent years since more and more companies and medical centers are highlighting the importance of an automated framework to effectively manage the quality of their big data. Data cleaning, also known as data curation, lies in the heart of the data quality assessment and is a key aspect prior to the development of any data analytics services. In this work, we present the objectives, functionalities and methodological advances of an automated framework for data curation from a medical perspective. The steps towards the development of a system for data quality assessment are first described along with multidisciplinary data quality measures. A three-layer architecture which realizes these steps is then presented. Emphasis is given on the detection and tracking of inconsistencies, missing values, outliers, and similarities, as well as, on data standardization to finally enable data harmonization. A case study is conducted in order to demonstrate the applicability and reliability of the proposed framework on two well-established cohorts with clinical data related to the primary Sjögren's Syndrome (pSS). Our results confirm the validity of the proposed framework towards the automated and fast identification of outliers, inconsistencies, and highly-correlated and duplicated terms, as well as, the successful matching of more than 85% of the pSS-related medical terms in both cohorts, yielding more accurate, relevant, and consistent clinical data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.compbiomed.2019.03.001</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A COMPREHENSIVE REVIEW OF BIG DATA ANALYTICS THROUGHOUT PRODUCT LIFECYCLE TO SUPPORT SUSTAINABLE SMART MANUFACTURING: A FRAMEWORK, CHALLENGES AND FUTURE RESEARCH DIRECTIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh and Cecilia M.V.B. Almeida</field>
  <field name="keywords">Big data analytics, Smart manufacturing, Servitization, Sustainable production, Conceptual framework, Product lifecycle</field>
  <field name="abstract">Smart manufacturing has received increased attention from academia and industry in recent years, as it provides competitive advantage for manufacturing companies making industry more efficient and sustainable. As one of the most important technologies for smart manufacturing, big data analytics can uncover hidden knowledge and other useful information like relations between lifecycle decisions and process parameters helping industrial leaders to make more-informed business decisions in complex management environments. However, according to the literature, big data analytics and smart manufacturing were individually researched in academia and industry. To provide theoretical foundations for the research community to further develop scientific insights in applying big data analytics to smart manufacturing, it is necessary to summarize the existing research progress and weakness. In this paper, through combining the key technologies of smart manufacturing and the idea of ubiquitous servitization in the whole lifecycle, the term of sustainable smart manufacturing was coined. A comprehensive overview of big data in smart manufacturing was conducted, and a conceptual framework was proposed from the perspective of product lifecycle. The proposed framework allows analyzing potential applications and key advantages, and the discussion of current challenges and future research directions provides valuable insights for academia and industry.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jclepro.2018.11.025</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AN ADAPTIVE BIG DATA WEATHER SYSTEM FOR SURFACE TRANSPORTATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William P. Mahoney and Sue Ellen Haupt</field>
  <field name="keywords">Big data, Pikalert, Road weather, Surface transportation, Pavement condition, Weather forecasts</field>
  <field name="abstract">Operating modern multi-modal surface transportation systems are becoming increasingly automated and driven by decision support systems. One aspect necessary for successful, safe, reliable, and efficient operation of any transportation network is real-time and forecasted weather and pavement condition information. Providing such information requires an adaptive system capable of blending large amounts of observational and model data that arrives quickly, in disparate formats and times, and blends and optimizes their use via expert systems and machine-learning algorithms. Quality control of the data is also essential, and historical data is required to both develop expert-based empirical algorithms and train machine learning models. This paper reports on the open-source Pikalert® system that brings together weather information and real-time data from connected vehicles to provide crucial information to enhance the safety and efficiency of surface transportation systems. This robust framework can be applied to a diverse array of user community specifications and is designed to rapidly ingest more, unique data sets as they become available. Ultimately, the developmental framework of this system will provide critical environmental information necessary to promote the development, growth, refinement, and expanded adoption of automated and connected multi-modal vehicular systems globally.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.trip.2019.100071</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA SOURCE SELECTION FOR INFORMATION INTEGRATION IN BIG DATA ERA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao</field>
  <field name="keywords">Source selection, Data integration, Data cleaning</field>
  <field name="abstract">In big data era, information integration often requires abundant data extracted from massive data sources. Due to a large number of data sources, data source selection plays a crucial role in information integration, since it is costly and even impossible to access all data sources. Data Source selection should consider both efficiency and effectiveness issues. For efficiency, the approach should scale to large data source amount. From effectiveness aspect, data quality and overlapping of sources are to be considered. In this paper, we study source selection problem in Big Data and propose methods which can scale to datasets with up to millions of data sources and guarantee the quality of results. Motivated by this, we propose a new metric taking the expected number of true values a source can provide as a criteria to evaluate the contribution of a data source. Based on our proposed index, we present a scalable algorithm and two pruning strategies to improve the efficiency without sacrificing precision. Experimental results on both real world and synthetic data sets show that our methods can select sources providing a large proportion of true values efficiently and can scale to massive data sources.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ins.2018.11.029</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RATIONALITY AND POLITICS OF ALGORITHMS. WILL THE PROMISE OF BIG DATA SURVIVE THE DYNAMICS OF PUBLIC DECISION MAKING?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer</field>
  <field name="keywords">nan</field>
  <field name="abstract">Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data – including its new methods and functions – must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.giq.2018.10.011</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SPATIAL URBAN DATA SYSTEM: A CLOUD-ENABLED BIG DATA INFRASTRUCTURE FOR SOCIAL AND ECONOMIC URBAN ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh and Yeran Sun and David McArthur and Phil Mason and Rod Walpole</field>
  <field name="keywords">Urban big data infrastructure, Urban analytics, Spatial urban indicators, Small area assessment, Spatial big data</field>
  <field name="abstract">The Spatial Urban Data System (SUDS) is a spatial big data infrastructure to support UK-wide analytics of the social and economic aspects of cities and city-regions. It utilises data generated from traditional as well as new and emerging sources of urban data. The SUDS deploys geospatial technology, synthetic small area urban metrics, and cloud computing to enable urban analytics, and geovisualization with the goal of deriving actionable knowledge for better urban management and data-driven urban decision making. At the core of the system is a programme of urban indicators generated by using novel forms of data and urban modelling and simulation programme. SUDS differs from other similar systems by its emphasis on the generation and use of regularly updated spatially-activated urban area metrics from real or near-real time data sources, to enhance understanding of intra-city interactions and dynamics. By deploying public transport, labour market accessibility and housing advertisement data in the system, we were able to identify spatial variations of key urban services at intra-city levels as well as social and economically-marginalised output areas in major cities across the UK. This paper discusses the design and implementation of SUDS, the challenges and limitations encountered, and considerations made during its development. The innovative approach adopted in the design of SUDS will enable it to support research and analysis of urban areas, policy and city administration, business decision-making, private sector innovation, and public engagement. Having been tested with housing, transport and employment metrics, efforts are ongoing to integrate information from other sources such as IoT, and User Generated Content into the system to enable urban predictive analytics.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.future.2019.03.052</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">HEALTHCARE BIG DATA PROCESSING MECHANISMS: THE ROLE OF CLOUD COMPUTING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar</field>
  <field name="keywords">Cloud computing, Processing, Healthcare, Big data, Review</field>
  <field name="abstract">Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijinfomgt.2019.05.017</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DELINEATION OF NITROGEN SIGNALING NETWORKS: COMPUTATIONAL APPROACHES IN THE BIG DATA ERA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yoshiaki Ueda and Shuichi Yanagisawa</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.molp.2019.01.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SURVEY ON BIG DATA-DRIVEN DIGITAL PHENOTYPING OF MENTAL HEALTH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yunji Liang and Xiaolong Zheng and Daniel D. Zeng</field>
  <field name="keywords">Digital phenotyping, Big data, Mental health, Data mining, Information fusion</field>
  <field name="abstract">The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.inffus.2019.04.001</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">VALUES, CHALLENGES AND FUTURE DIRECTIONS OF BIG DATA ANALYTICS IN HEALTHCARE: A SYSTEMATIC REVIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">P. Galetsi and K. Katsaliaki and S. Kumar</field>
  <field name="keywords">Systematic review, Big data analytics, Health-medicine, Decision-making, Organizational and societal values, Preferred reporting items for systematic reviews and meta-analyses</field>
  <field name="abstract">The emergence of powerful software has created conditions and approaches for large datasets to be collected and analyzed which has led to informed decision-making towards tackling health issues. The objective of this study is to systematically review 804 scholarly publications related to big data analytics in health in order to identify the organizational and social values along with associated challenges. Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology were followed for conducting systematic reviews. Following a research path, we present the values, challenges and future directions of the scientific area using indicative examples from relevant published articles. The study reveals that one of the main values created is the development of analytical techniques which provides personalized health services to users and supports human decision-making using automated algorithms, challenging the power issues in the doctor-patient relationship and creating new working conditions. A main challenge to data analytics is data management and security when processing large volumes of sensitive, personal health data. Future research is directed towards the development of systems that will standardize and secure the process of extracting private healthcare datasets from relevant organizations. Our systematic literature review aims to provide to governments and health policy-makers a better understanding of how the development of a data driven strategy can improve public health and the functioning of healthcare organizations but also how can create challenges that need to be addressed in the near future to avoid societal malfunctions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.socscimed.2019.112533</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE ROLE OF BIG DATA ANALYTICS IN INDUSTRIAL INTERNET OF THINGS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad Imran and Prem Prakash Jayaraman and Charith Perera</field>
  <field name="keywords">Internet of Things, Cyber-physical systems, Cloud computing, Analytics, Big data</field>
  <field name="abstract">Big data production in industrial Internet of Things (IIoT) is evident due to the massive deployment of sensors and Internet of Things (IoT) devices. However, big data processing is challenging due to limited computational, networking and storage resources at IoT device-end. Big data analytics (BDA) is expected to provide operational- and customer-level intelligence in IIoT systems. Although numerous studies on IIoT and BDA exist, only a few studies have explored the convergence of the two paradigms. In this study, we investigate the recent BDA technologies, algorithms and techniques that can lead to the development of intelligent IIoT systems. We devise a taxonomy by classifying and categorising the literature on the basis of important parameters (e.g. data sources, analytics tools, analytics techniques, requirements, industrial analytics applications and analytics types). We present the frameworks and case studies of the various enterprises that have benefited from BDA. We also enumerate the considerable opportunities introduced by BDA in IIoT. We identify and discuss the indispensable challenges that remain to be addressed, serving as future research directions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.future.2019.04.020</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BRINGING BIG DATA ANALYTICS CLOSER TO PRACTICE: A METHODOLOGICAL EXPLANATION AND DEMONSTRATION OF CLASSIFICATION ALGORITHMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner</field>
  <field name="keywords">Congestive heart failure, Machine learning, Logistic regression, Boosted decision tree, Support vector machine, Neural network</field>
  <field name="abstract">Background
Big data analytics are becoming more prevalent due to the recent availability of health data. Yet in spite of evidence supporting the potential contribution of big data analytics to health policy makers and care providers, these tools are still too complex to be routinely used. Further, access to comprehensive datasets required for more accurate results is complex and costly. Consequently, big data analytics are mostly used by researchers and experts who are far removed from actual clinical practice. Hence, policy makers should allocate resources to encourage studies that clarify and simplify big data analytics so it can be used by non-experts (e.g., clinicians, practitioners and decision-makers who may not have advanced computer skills). It is also important to fund data collection and integration from various health IT, a pre-condition for any big data analytics project.
Objectives
To methodologically clarify the rationale and logic behind several analytics algorithms to help non-expert users employ big data analytics by understanding how to implement relatively easy to use platforms as Azure ML.
Methods
We demonstrate the predictive power of four known algorithms and compare their accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.
Results
The results of our models outperform those reported in the literature, attesting to the strength of some of the models, and the utility of comprehensive data.
Conclusions
The results support our call to policy makers to allocate resources to establishing comprehensive, integrated health IT systems, and to projects aimed at simplifying ML analytics.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.hlpt.2018.12.003</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">LINKING BIG DATA ANALYTICS AND OPERATIONAL SUSTAINABILITY PRACTICES FOR SUSTAINABLE BUSINESS MANAGEMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede</field>
  <field name="keywords">Big-data analytics, Ecological-economic-social sustainability, Green practices, SustainableOperations management, Structural equation modelling-artificial neural network, Emerging economies</field>
  <field name="abstract">Big data analytics is becoming very popular concept in academia as well as in industry. It has come up with new decision tools to design data-driven supply chains. The manufacturing industry is under huge pressure to integrate sustainable practices into their overall business for sustainbale operations management. The purpose of this study is to analyse the predictors of sustainable business performance through big data analytics in the context of developing countries. Data was collected from manufacturing firms those have adopted sustainable practices. A hybrid Structural Equation Modelling - Artificial Neural Network model is used to analyse 316 responses of Indian professional experts. Factor analysis results shows that management and leadership style, state and central-government policy, supplier integration, internal business process, and customer integration have a significant influence on big data analytics and sustainability practices. Furthermore, the results obtained from structural equation modelling were feed as input to the artificial neural network model. The study findings shows that management and leadership style, state and central-government policy as the two most important predictors of big data analytics and sustainability practices. The results provide unique insights into manufacturing firms to improve their sustainable business performance from an operations management viewpoint. The study provides theoretical and practical insights into big data implementation issues in accomplishing sustainability practices in business organisations of emerging economies.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jclepro.2019.03.181</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SOCIAL MEDIA BIG DATA ANALYTICS: A SURVEY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed</field>
  <field name="keywords">Big data, Social media, Machine learning, Analytics</field>
  <field name="abstract">Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as “big data” has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.chb.2018.08.039</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA AND UNDERSTANDING CHANGE IN THE CONTEXT OF PLANNING TRANSPORT SYSTEMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dave Milne and David Watling</field>
  <field name="keywords">nan</field>
  <field name="abstract">This paper considers the implications of so-called ‘big data’ for the analysis, modelling and planning of transport systems. The primary conceptual focus is on the needs of the practical context of medium-term planning and decision-making, from which perspective the paper seeks to achieve three goals: (i) to try to identify what is truly ‘special’ about big data; (ii) to provoke debate on the future relationship between transport planning and big data; and (iii) to try to identify promising themes for research and application. Differences in the information that can be derived from the data compared to more traditional surveys are discussed, and the respects in which they may impact on the role of models in supporting transport planning and decision-making are identified. It is argued that, over time, changes to the nature of data may lead to significant differences in both modelling approaches and in the expectations placed upon them. Furthermore, it is suggested that the potential widespread availability of data to commercial actors and travellers will affect the performance of the transport systems themselves, which might be expected to have knock-on effects for planning functions. We conclude by proposing a series of research challenges that we believe need to be addressed and warn against adaptations based on minimising change from the status quo.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jtrangeo.2017.11.004</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">UNDERSTANDING BIG DATA ANALYTICS FOR MANUFACTURING PROCESSES: INSIGHTS FROM LITERATURE REVIEW AND MULTIPLE CASE STUDIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and Said {El fezazi}</field>
  <field name="keywords">Big Data Analytics, Manufacturing process, Big Data Analytics capabilities, Business intelligence, Literature review, Multiple case study</field>
  <field name="abstract">Today, we are undoubtedly in the era of data. Big Data Analytics (BDA) is no longer a perspective for all level of the organization. This is of special interest in the manufacturing process with their high capital intensity, time constraints and given the huge amount of data already captured. However, there is a paucity in past literature on BDA to develop better understanding of the capabilities and strategic implications to extract value from BDA. In that vein, the central aim of this paper is to develop a novel model that summarizes the main capabilities of BDA in the context of manufacturing process. This is carried out by relying on the findings of a review of the ongoing research along with a multiple case studies within a leading phosphate derivatives manufacturer to point out the capabilities of BDA in manufacturing processes and outline recommendations to advance research in the field. The findings will help companies to understand the big data analytics capabilities and its potential implications for their manufacturing processes and support them seeking to design more effective BDA-enabler infrastructure.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.cie.2019.106099</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INDUSTRIAL APPLICATIONS OF BIG DATA IN DISRUPTIVE INNOVATIONS SUPPORTING ENVIRONMENTAL REPORTING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Esa Hämäläinen and Tommi Inkinen</field>
  <field name="keywords">Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography</field>
  <field name="abstract">Disruptive innovations are usually identified as ideas that are created ‘outside the box’. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jii.2019.100105</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA DRIVEN DECISION-MAKING FOR BATCH-BASED PRODUCTION SYSTEMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y Zhong and Ting Qu and Zhiwu Li</field>
  <field name="keywords">Big data, Smart Product-Service System, Sales predict, Economic batch quantity, production plan</field>
  <field name="abstract">The era of big data has brought new challenges to chemical enterprises. In order to maximize the benefits, enterprises are considering to implement intelligent service technology into traditional production systems to improve the level of intelligence in business. This paper proposes a service framework based on big data driven prediction, which includes information perception layer, information application layer and big data service layer. In this paper, the composition of big data service layer is described in detail, and a sales predicting method based on neural network is introduced. The salability of products is divided, and the qualitative economic production volume mechanism is finally given. Based on the framework, an intelligent service system for enterprises with the characteristics of mass production is implemented. Experimental results show that the big data service framework can support chemical enterprises to make decisions to reduce costs, and provides an effective method for Smart Product Service System (PSS).</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procir.2019.05.023</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">HOW BIG DATA SCIENCE CAN IMPROVE LINKAGE AND RETENTION IN CARE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Aadia I. Rana and Michael J. Mugavero</field>
  <field name="keywords">HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.idc.2019.05.009</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CAPABILITIES AND READINESS FOR BIG DATA ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jenifer Pedro and Irwin Brown and Mike Hart</field>
  <field name="keywords">Big data analytics, organisational readiness, organisational capabilities, frameworks, business analytics, thematic analysis</field>
  <field name="abstract">Despite some of the initial hype from marketers and consultants, the use of big data is now firmly established in many organisations worldwide. Big data analytics (BDA) is making use of huge volumes of data from a wide range of structured and unstructured sources. Surveys have however reported a number of barriers to organisational effectiveness with BDA. This research aims to determine what capabilities large organisations require to be ready for a successful BDA initiative. Drawing mainly on relevant results of two published research articles, key informed stakeholders from a large South African telecommunications company were interviewed on this topic. Thematic analysis identified the key themes and sub-themes relating to capabilities needed for the organization to be ready for effective BDA. These proved to be very similar to those given in the earlier research, although a new capability of legal compliance for data protection was now added.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.12.147</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA ANALYTICS AND FIRM PERFORMANCE: FINDINGS FROM A MIXED-METHOD APPROACH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie</field>
  <field name="keywords">Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty</field>
  <field name="abstract">Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jbusres.2019.01.044</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">REAL-TIME BIG DATA PROCESSING FOR ANOMALY DETECTION: A SURVEY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran</field>
  <field name="keywords">Real-time, Big data processing, Anomaly detection and machine learning algorithms</field>
  <field name="abstract">The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ijinfomgt.2018.08.006</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WHAT WE TALK ABOUT WHEN WE TALK ABOUT (BIG) DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Matthew Jones</field>
  <field name="keywords">nan</field>
  <field name="abstract">In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between “data in principle” as they are recorded, and the “data in practice” as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jsis.2018.10.005</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EXPERIENCE AND REFLECTION FROM CHINA’S XIANGYA MEDICAL BIG DATA PROJECT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan</field>
  <field name="keywords">Medical big data, Data sharing, Information security, Cooperation mechanism, Medical data acquisition, Medical data centre</field>
  <field name="abstract">The construction of medical big data includes several problems that need to be solved, such as integration and data sharing of many heterogeneous information systems, efficient processing and analysis of large-scale medical data with complex structure or low degree of structure, and narrow application range of medical data. Therefore, medical big data construction is not only a simple collection and application of medical data but also a complex systematic project. This paper introduces China's experience in the construction of a regional medical big data ecosystem, including the overall goal of the project; establishment of policies to encourage data sharing; handling the relationship between personal privacy, information security, and information availability; establishing a cooperation mechanism between agencies; designing a polycentric medical data acquisition system; and establishing a large data centre. From the experience gained from one of China's earliest established medical big data projects, we outline the challenges encountered during its development and recommend approaches to overcome these challenges to design medical big data projects in China more rationally. Clear and complete top-level design of a project requires to be planned in advance and considered carefully. It is essential to provide a culture of information sharing and to facilitate the opening of data, and changes in ideas and policies need the guidance of the government. The contradiction between data sharing and data security must be handled carefully, that is not to say data openness could be abandoned. The construction of medical big data involves many institutions, and high-level management and cooperation can significantly improve efficiency and promote innovation. Compared with infrastructure construction, it is more challenging and time-consuming to develop appropriate data standards, data integration tools and data mining tools.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jbi.2019.103149</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CHALLENGES, OPPORTUNITIES AND PARADIGM OF APPLYING BIG DATA TO PRODUCTION SAFETY MANAGEMENT: FROM A THEORETICAL PERSPECTIVE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lang Huang and Chao Wu and Bing Wang</field>
  <field name="keywords">Big data, Production safety management, Big-data-driven, Challenges, Opportunities</field>
  <field name="abstract">Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jclepro.2019.05.245</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PROSPECTS FOR ENERGY ECONOMY MODELLING WITH BIG DATA: HYPE, ELIMINATING BLIND SPOTS, OR REVOLUTIONISING THE STATE OF THE ART?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan</field>
  <field name="keywords">Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data</field>
  <field name="abstract">Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.apenergy.2019.02.002</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ARCHITECTURAL TACTICS FOR BIG DATA CYBERSECURITY ANALYTICS SYSTEMS: A REVIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Faheem Ullah and Muhammad {Ali Babar}</field>
  <field name="keywords">Big data, Cybersecurity, Quality attribute, Architectural tactic</field>
  <field name="abstract">Context
Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies for analyzing security events data to protect organizational networks, computers, and data from cyber attacks.
Objective
We aimed at identifying the most frequently reported quality attributes and architectural tactics for BDCA systems.
Method
We used Systematic Literature Review (SLR) method for reviewing 74 papers.
Result
Our findings are twofold: (i) identification of 12 most frequently reported quality attributes for BDCA systems; and (ii) identification and codification of 17 architectural tactics for addressing the identified quality attributes. The identified tactics include six performance tactics, four accuracy tactics, two scalability tactics, three reliability tactics, and one security and usability tactic each.
Conclusion
Our study reveals that in the context of BDCA (a) performance, accuracy and scalability are the most important quality concerns (b) data analytics is the most critical architectural component (c) despite the significance of interoperability, modifiability, adaptability, generality, stealthiness, and privacy assurance, these quality attributes lack explicit architectural support (d) empirical investigation is required to evaluate the impact of the codified tactics and explore the quality trade-offs and dependencies among the tactics and (e) the reported tactics need to be modelled using a standardized modelling language such as UML.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jss.2019.01.051</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE LAST FIVE YEARS OF BIG DATA RESEARCH IN ECONOMICS, ECONOMETRICS AND FINANCE: IDENTIFICATION AND CONCEPTUAL ANALYSIS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">José Ricardo López-Robles and Marisela Rodríguez-Salvador and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jesús Cobo</field>
  <field name="keywords">Type your keywords here, separated by semicolons</field>
  <field name="abstract">Today, the Big Data term has a multidimensional approach where five main characteristics stand out: volume, velocity, veracity, value and variety. It has changed from being an emerging theme to a growing research area. In this respect, this study analyses the literature on Big Data in the Economics, Econometrics and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated using SciMAT as a bibliometric and network analysis software. SciMAT offers a complete approach of the field and evaluates the most cited and productive authors, countries and subject areas related to Big Data. Lastly, a science map is performed to understand the intellectual structure and the main research lines (themes).</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.12.044</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TECHNO-OPTIMISM AND POLICY-PESSIMISM IN THE PUBLIC SECTOR BIG DATA DEBATE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Simon Vydra and Bram Klievink</field>
  <field name="keywords">Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance</field>
  <field name="abstract">Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core ‘techno-optimist’ tenets from a more ‘policy-pessimist’ angle. In the conclusion we have these two narratives meet ‘eye-to-eye’, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.giq.2019.05.010</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">APPLICATION OF BIG DATA TECHNOLOGY IN SCIENTIFIC RESEARCH DATA MANAGEMENT OF MILITARY ENTERPRISES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wang Kun and Liu Tong and Xie Xiaodan</field>
  <field name="keywords">big data technology, scientific research data, data analysis, decision</field>
  <field name="abstract">Scientific research data has an important strategic position for the development of enterprises and countries, and is an important basis for management to conduct strategic research and decision-making. Compared with the Internet industry, big data technology started late in the military enterprises, while military enterprises research data often has the characteristics of decentralization, low relevance, and diverse data types. It cannot fully utilize the advantages of data resources to enhance the core competitiveness of enterprises. To this end, this paper deeply explores the application methods of big data technology in military scientific research data management, and lays a foundation for the construction of scientific research big data platform.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.01.221</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A NEW ARCHITECTURE FOR COGNITIVE INTERNET OF THINGS AND BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari Fourati</field>
  <field name="keywords">Internet of Things, Big-Data, Architecture, Cognitive, Data-flow</field>
  <field name="abstract">Big data and the Internet of Things (IoT) are considered as the main paradigms when defining new information architecture projects. Accordingly, technologies that make up these solutions could have an important role to play in business information architecture. Solutions that have approached big data and the IoT as unique technology initiatives, struggle in finding value in such efforts and in the technology itself. A connection to the requirements (volume, velocity, and variety) is mandatory to reach the potential business goals. In this context, we propose a new architecture for Cognitive Internet of Things (CIoT) and big data. The proposed architecture benefits computing mechanisms by combining the data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous data collection.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.09.208</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">STATE OF THE ART IN BIG DATA APPLICATIONS IN MICROGRID: A REVIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Karim Moharm</field>
  <field name="keywords">Big data, Microgrid</field>
  <field name="abstract">The prospering Big data era is emerging in the power grid. Multiple world-wide studies are emphasizing the big data applications in the microgrid due to the huge amount of produced data. Big data analytics can impact the design and applications towards safer, better, more profitable, and effective power grid. This paper presents the recognition and challenges of the big data and the microgrid. The construction of big data analytics is introduced. The data sources, big data opportunities, and enhancement areas in the microgrid like stability improvement, asset management, renewable energy prediction, and decision-making support are summarized. Diverse case studies are presented including different planning, operation control, decision making, load forecasting, data attacks detection, and maintenance aspects of the microgrid. Finally, the open challenges of big data in the microgrid are discussed.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.aei.2019.100945</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SYSTEMATIC REVIEW OF THE LITERATURE ON BIG DATA IN THE TRANSPORTATION DOMAIN: CONCEPTS AND APPLICATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra</field>
  <field name="keywords">Big Data, Smart city, Intelligent transportation system, Connected vehicle, Road traffic safety, Vision Zero</field>
  <field name="abstract">Research in Big Data and analytics offers tremendous opportunities to utilize evidence in making decisions in many application domains. To what extent can the paradigms of Big Data and analytics be used in the domain of transport? This article reports on an outcome of a systematic review of published articles in the last five years that discuss Big Data concepts and applications in the transportation domain. The goal is to explore and understand the current research, opportunities, and challenges relating to the utilization of Big Data and analytics in transportation. The review shows the potential of Big Data and analytics to garner insights and improve transportation systems through the analysis of various forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing, and social media. We discuss some platforms and software architecture for the transport domain, along with a wide array of storage, processing, and analytical techniques, and describe challenges associated with the implementation of Big Data and analytics. This review contributes broadly to the various ways in which cities can utilize Big Data in transportation to guide the creation of sustainable and safer traffic systems. Since research in Big Data and transportation is, by and large, at infancy, this article does not prescribe recommendations to the various challenges identified, which also constitutes the limitation of the article.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.bdr.2019.03.001</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA IN CANCER RESEARCH: REAL-WORLD RESOURCES FOR PRECISION ONCOLOGY TO IMPROVE CANCER CARE DELIVERY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez</field>
  <field name="keywords">nan</field>
  <field name="abstract">In oncology, the term “big data” broadly describes the rapid acquisition and generation of massive amounts of information, typically from population cancer registries, electronic health records, or large-scale genetic sequencing studies. The challenge of using big data in cancer research lies in interdisciplinary collaboration and information processing to unify diverse data sources and provide valid analytics to harness meaningful information. This article provides an overview of how big data approaches can be applied in cancer research, and how they can be used to translate information into new ways to ultimately make informed decisions that improve cancer care and delivery.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.semradonc.2019.05.002</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ROLE OF BIG DATA MANAGEMENT IN ENHANCING BIG DATA DECISION-MAKING CAPABILITY AND QUALITY AMONG CHINESE FIRMS: A DYNAMIC CAPABILITIES VIEW</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan</field>
  <field name="keywords">Big data management, Dynamic capabilities, Big data decision-making capability, Decision-making quality, China</field>
  <field name="abstract">This study examines the antecedents and influence of big data decision-making capabilities on decision-making quality among Chinese firms. We propose that such capabilities are influenced by big data management challenges such as leadership, talent management, technology, and organisational culture. By using primary data from 108 Chinese firms and utilising partial least squares, we tested the antecedents of big data decision-making capability and its impact on decision-making quality. Findings suggest that big data management challenges are the key antecedents of big data decision-making capability. Furthermore, the latter is vital for big data decision-making quality.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.im.2018.12.003</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">STRATEGIC ORIENTATIONS, DEVELOPMENTAL CULTURE, AND BIG DATA CAPABILITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Canchu Lin and Anand Kunnathur</field>
  <field name="keywords">Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture</field>
  <field name="abstract">Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.jbusres.2019.07.016</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ENABLING SMART DATA: NOISE FILTERING IN BIG DATA CLASSIFICATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Diego García-Gil and Julián Luengo and Salvador García and Francisco Herrera</field>
  <field name="keywords">Big Data, Smart Data, Classification, Class noise, Label noise.</field>
  <field name="abstract">In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ins.2018.12.002</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA ADOPTION: STATE OF THE ART AND RESEARCH CHALLENGES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi</field>
  <field name="keywords">Big data adoption, Technology–Organization–Environment, Diffusion of Innovations</field>
  <field name="abstract">Big data adoption is a process through which businesses find innovative ways to enhance productivity and predict risk to satisfy customers need more efficiently. Despite the increase in demand and importance of big data adoption, there is still a lack of comprehensive review and classification of the existing studies in this area. This research aims to gain a comprehensive understanding of the current state-of-the-art by highlighting theoretical models, the influence factors, and the research challenges of big data adoption. By adopting a systematic selection process, twenty studies were identified in the domain of big data adoption and were reviewed in order to extract relevant information that answers a set of research questions. According to the findings, Technology–Organization–Environment and Diffusion of Innovations are the most popular theoretical models used for big data adoption in various domains. This research also revealed forty-two factors in technology, organization, environment, and innovation that have a significant influence on big data adoption. Finally, challenges found in the current research about big data adoption are represented, and future research directions are recommended. This study is helpful for researchers and stakeholders to take initiatives that will alleviate the challenges and facilitate big data adoption in various fields.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.ipm.2019.102095</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIGSEA: A BIG DATA ANALYTICS PLATFORM FOR PUBLIC TRANSPORTATION INFORMATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and Carlos Eduardo Pires and Nádia P. Kozievitch and Walter dos Santos and Paulo Silva and Marco Vieira</field>
  <field name="keywords">nan</field>
  <field name="abstract">Analysis of public transportation data in large cities is a challenging problem. Managing data ingestion, data storage, data quality enhancement, modelling and analysis requires intensive computing and a non-trivial amount of resources. In EUBra-BIGSEA (Europe–Brazil Collaboration of Big Data Scientific Research Through Cloud-Centric Applications) we address such problems in a comprehensive and integrated way. EUBra-BIGSEA provides a platform for building up data analytic workflows on top of elastic cloud services without requiring skills related to either programming or cloud services. The approach combines cloud orchestration, Quality of Service and automatic parallelisation on a platform that includes a toolbox for implementing privacy guarantees and data quality enhancement as well as advanced services for sentiment analysis, traffic jam estimation and trip recommendation based on estimated crowdedness. All developments are available under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/).</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.future.2019.02.011</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA ANALYTICS CAPABILITY AND CO-INNOVATION: AN EMPIRICAL STUDY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Nelson Lozada and Jose Arias-Pérez and Geovanny Perdomo-Charry</field>
  <field name="keywords">Business, Economics, Information science, Big data analytics capabilities, Co-innovation, Big data, Co-creation</field>
  <field name="abstract">There are numerous emerging studies addressing big data and its application in different organizational aspects, especially regarding its impact on the business innovation process. This study in particular aims at analyzing the existing relationship between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis model, structural equations by the partial least squares method were used in a sample of 112 Colombian firms. The main findings allow to positively relate Big Data Analytics Capabilities with better and more agile processes of product and service co-creation and with more robust collaboration networks with stakeholders internal and external to the firm.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.heliyon.2019.e02541</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ORCHESTRATING BIG DATA ANALYTICS CAPABILITY FOR SUSTAINABILITY: A STUDY OF AIR POLLUTION MANAGEMENT IN CHINA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu</field>
  <field name="keywords">Big data, Big data analytics, Sustainability, Air pollution, Resource orchestration</field>
  <field name="abstract">Under rapid urbanization, cities are facing many societal challenges that impede sustainability. Big data analytics (BDA) gives cities unprecedented potential to address these issues. As BDA is still a new concept, there is limited knowledge on how to apply BDA in a sustainability context. Thus, this study investigates a case using BDA for sustainability, adopting the resource orchestration perspective. A process model is generated, which provides novel insights into three aspects: data resource orchestration, BDA capability development, and big data value creation. This study benefits both researchers and practitioners by contributing to theoretical developments as well as by providing practical insights.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.im.2019.103231</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">KEY QUESTIONS ON THE USE OF BIG DATA IN FARMING: AN ACTIVITY THEORY APPROACH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and Marcello {De Rosa}</field>
  <field name="keywords">Big data, Smart farming, Value, Farmers, Cyber-physical-social systems, Activity theory</field>
  <field name="abstract">Big data represent a pioneering development in the field of agriculture. By producing intuition, intelligence, and insights, these data have the potential to recast conventional process-driven agriculture, plotting the course for a smarter, data-driven farming. However, many open issues about the use of big data in agriculture remain unanswered. In this work, conceptualizing smart agricultural systems as cyber-physical-social systems, and building upon activity theory, we aim at highlighting some key questions that need to be addressed. To our view, big data constitute a tool reciprocally produced by all the actors involved in the agrifood supply chains. The constant flux of this tool and the intricate nature of the interactions among the actors who share it complicate the translation of big data into value. Moreover, farmers’ limited capacity to deal with data complexity, along with their dual role as producers and users of big data, impedes the institutionalization of this tool at the farm level. Although the approach used left us with more questions than answers, we suggest that unraveling the institutional arrangements that govern value co-creation, capturing the motivations of farmers and other actors, and detailing the direct and indirect effects that big data (and the technologies used to generate them) have in farms are important preconditions for setting forth rules that facilitate the extraction and equal exchange of value from big data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.njas.2019.04.003</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">FUNCTIONAL NEUROIMAGING IN THE NEW ERA OF BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xiang Li and Ning Guo and Quanzheng Li</field>
  <field name="keywords">Big data, Neuroimaging, Machine learning, Health informatics, fMRI</field>
  <field name="abstract">The field of functional neuroimaging has substantially advanced as a big data science in the past decade, thanks to international collaborative projects and community efforts. Here we conducted a literature review on functional neuroimaging, with focus on three general challenges in big data tasks: data collection and sharing, data infrastructure construction, and data analysis methods. The review covers a wide range of literature types including perspectives, database descriptions, methodology developments, and technical details. We show how each of the challenges was proposed and addressed, and how these solutions formed the three core foundations for the functional neuroimaging as a big data science and helped to build the current data-rich and data-driven community. Furthermore, based on our review of recent literature on the upcoming challenges and opportunities toward future scientific discoveries, we envisioned that the functional neuroimaging community needs to advance from the current foundations to better data integration infrastructure, methodology development toward improved learning capability, and multi-discipline translational research framework for this new era of big data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.gpb.2018.11.005</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">USE OF BIG DATA FOR QUALITY ASSURANCE IN RADIATION THERAPY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright</field>
  <field name="keywords">nan</field>
  <field name="abstract">The application of big data to the quality assurance of radiation therapy is multifaceted. Big data can be used to detect anomalies and suboptimal quality metrics through both statistical means and more advanced machine learning and artificial intelligence. The application of these methods to clinical practice is discussed through examples of guideline adherence, contour integrity, treatment delivery mechanics, and treatment plan quality. The ultimate goal is to apply big data methods to direct measures of patient outcomes for care quality. The era of big data and machine learning is maturing and the implementation for quality assurance promises to improve the quality of care for patients.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.semradonc.2019.05.006</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CAN BIG DATA IMPROVE FIRM DECISION QUALITY? THE ROLE OF DATA QUALITY AND DATA DIAGNOSTICITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Maryam Ghasemaghaei and Goran Calic</field>
  <field name="keywords">Big data utilization, Data quality, Decision quality, Data diagnosticity</field>
  <field name="abstract">Anecdotal evidence suggests that, despite the large variety of data, the huge volume of generated data, and the fast velocity of obtaining data (i.e., big data), quality of big data is far from perfect. Therefore, many firms defer collecting and integrating big data as they have concerns regarding the impact of utilizing big data on data diagnosticity (i.e., retrieval of valuable information from data) and firm decision making quality. In this study, we use the Organizational Learning Theory and Wang and Strong's data quality framework to explore the impact of processing big data on firm decision quality and the mediating role of data quality (DQ) and data diagnosticity on this relationship. We validate the proposed research model using survey data from 130 firms, obtained from data analysts and IT managers. Results confirm the critical role of DQ in increasing data diagnosticity and improving firm decision quality when processing big data; suggesting important implications for practice and theory. Findings also reveal that while big data utilization positively impacts contextual DQ, accessibility DQ, and representational DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show that while intrinsic DQ, contextual DQ, and representational DQ significantly increase data diagnosticity, accessibility DQ does not influence it. Most importantly, the findings show that big data utilization does not significantly impact the quality of firm decisions and it is fully mediated through DQ and data diagnosticity. The results of this study contribute to practice by providing important guidelines for managers to improve firm decision quality through the use of big data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.dss.2019.03.008</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA: TRADE-OFF BETWEEN DATA QUALITY AND DATA SECURITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI</field>
  <field name="keywords">Big Data, Data Quality, Data Security, Trade-off between Quality, Security</field>
  <field name="abstract">The essence of an information system lies in the data; if it is not of good quality or not sufficiently protected, the consequences will undoubtedly be harmful. Quality and Security are two essential aspects that add value to data and their implementation has become a real need and must be adopted before any data exploitation. Due to the high volume of data, their diversity and their rapid generation, effective implementation of such systems requires well thought out mechanisms and strategies. This paper provides an overview of Data Quality and Data Security in a Big Data context. We want through this paper to highlight the conflicts that may exist during the implementation of data security and data quality management systems. Such a conflict makes the complexity greater and requires new adapted solutions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.04.127</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE IMPACT OF BIG DATA QUALITY ON SENTIMENT ANALYSIS APPROACHES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Imane El Alaoui and Youssef Gahi</field>
  <field name="keywords">Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data, Sentiment Analysis, Opinion Mining</field>
  <field name="abstract">Human beings share their good or bad opinions about subjects, products, and services through internet and social networks. The ability to effectively analyze this kind of information is now seen as a key competitive advantage to better inform decisions. In order to do so, organizations employ Sentiment Analysis (SA) techniques on these data. However, the usage of social media around the world is ever-increasing, which considerably accelerates massive data generation and makes traditional SA systems unable to deliver useful insights. Such volume of data can be efficiently analyzed using the combination of SA techniques and Big Data technologies. In fact, big data is not a luxury but an essential necessary to make valuable predictions. However, there are some challenges associated with big data such as quality that could highly affect the SA systems’ accuracy that use huge volume of data. Thus, the quality aspect should be addressed in order to build reliable and credible systems. For this, the goal of our research work is to consider Big Data Quality Metrics (BDQM) in SA that rely of big data. In this paper, we first highlight the most eloquent BDQM that should be considered throughout the Big Data Value Chain (BDVC) in any big data project. Then, we measure the impact of BDQM on a novel SA method accuracy in a real case study by giving simulation results.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.11.007</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RISK ANALYSIS OF USING BIG DATA IN COMPUTER SCIENCES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin Solano and Claudia Fernández</field>
  <field name="keywords">Data management, data quality, decision making, data analysis</field>
  <field name="abstract">Today, as technologies mature and people are encouraged to contribute data to organizations’ databases, more transactions are being captured than ever before. Meanwhile, improvements in data storage technologies have made the cost of evaluating, selecting, and destroying legacy data considerably greater than simply letting it accumulate. On the one hand, the excess of stored data has considerably increased the opportunities to interrelate and analyze them, while the moderate enthusiasm generated by data warehousing and data mining in the 1990s has been replaced by a rampant euphoria about big data and data analytics. But, is this as wonderful as seems? This paper presents a risk analysis of Big Data and Big Data Analytics based on a review of quality factors.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.11.052</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">NUMERICAL, SECONDARY BIG DATA QUALITY ISSUES, QUALITY THRESHOLD ESTABLISHMENT, & GUIDELINES FOR JOURNAL POLICY DEVELOPMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Anita Lee-Post and Ram Pakath</field>
  <field name="keywords">Data quality, Big data, Secondary data, Numerical data, Quality threshold</field>
  <field name="abstract">An IS researcher may obtain Big Data from primary or secondary data sources. Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility, cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based IS research and discuss ways in which one may, post hoc, establish quality thresholds for numerical Big Data obtained from secondary sources. We also present guidelines for developing journal policies aimed at ensuring the veracity and verifiability of such data when used for research purposes.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.dss.2019.113135</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A REVIEW ON DATA CLEANSING METHODS FOR BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}</field>
  <field name="keywords">data cleansing, big data, data quality</field>
  <field name="abstract">Massive amounts of data are available for the organization which will influence their business decision. Data collected from the various resources are dirty and this will affect the accuracy of prediction result. Data cleansing offers a better data quality which will be a great help for the organization to make sure their data is ready for the analyzing phase. However, the amount of data collected by the organizations has been increasing every year, which is making most of the existing methods no longer suitable for big data. Data cleansing process mainly consists of identifying the errors, detecting the errors and corrects them. Despite the data need to be analyzed quickly, the data cleansing process is complex and time-consuming in order to make sure the cleansed data have a better quality of data. The importance of domain expert in data cleansing process is undeniable as verification and validation are the main concerns on the cleansed data. This paper reviews the data cleansing process, the challenge of data cleansing for big data and the available data cleansing methods.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">https://doi.org/10.1016/j.procs.2019.11.177</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA QUALITY METRICS FOR SENTIMENT ANALYSIS APPROACHES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi</field>
  <field name="keywords">Big data quality metrics, Big data, Sentiment analysis</field>
  <field name="abstract">In a world increasingly connected, and in which information flows quickly and affects a very large number of people, sentiment analysis has seen a spectacular development over the past ten years. This is due to the fact that the explosion of social networks has allowed anyone with internet access to publicly express his opinion. Moreover, the emergence of big data has brought enormous opportunities and powerful storage and analytics tools to the field of sentiment analysis. However, big data introduces new variables and constraints that could radically affect the traditional models of sentiment analysis. Therefore, new concerns, such as big data quality, have to be addressed to get the most out of big data. To the best of our knowledge, no contributions have been published so far which address big data quality in SA throughout its different processes. In this paper, we first highlight the most important big data quality metrics to consider in any big data project. Then, we show how these metrics could be specifically considered in SA approaches and this for each phase in the big data value chain.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3341620.3341629</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EVALUATION OF LARGE-SCALE COMPLEX SYSTEMS EFFECTIVENESS BASED ON BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang</field>
  <field name="keywords">big data, large-scale complex systems, evaluation, effectiveness</field>
  <field name="abstract">With the advent of the information age, big data technology came into being. The wide application of big data brings new opportunities and challenges to the construction of national defense and military information. Under the background of information-based joint operations characterized by large complex systems, how to scientifically and rationally plan the construction of large complex systems, and maximize the effectiveness of the complex system has become a key concern for system construction decision makers and researchers. This paper combines the application of big data in the construction of large complex systems, and focuses on the evaluation of the effectiveness of large complex systems based on big data, which can be used for reference by relevant researchers.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3335484.3335545</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA SOURCE SELECTION IN BIG DATA CONTEXT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim</field>
  <field name="keywords">Source reliability, Big Data integration, Data quality, Big Data Source Selection</field>
  <field name="abstract">Big Data presents promising technological and economical opportunities. In fact, it has become the raw material of production for many organizations. Data is available in large quantities, and it continues generating abundantly. However, not all the data will have valuable knowledge. Unreliable sources provide misleading and biased information, and even reliable sources could suffer from low data quality.In this paper, we propose a novel methodology for the selectability of data sources, by both considering the presence and the absence of users' preferences. The proposed model integrates multiple factors that affect the reliability of data sources, including their quality, gain, cost and coverage. Experimental results on real world data-sets, show its capability to find the subset of relevant and reliable sources with the lowest cost.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3366030.3366121</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CO-DESIGNING LEARNING MATERIALS TO EMPOWER LAYPERSONS TO BETTER UNDERSTAND BIG DATA AND BIG DATA METHODS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral, Janet and Kwan, Bethany and Ressalam, Julie</field>
  <field name="keywords">community engagement, big data, co-design, education</field>
  <field name="abstract">University of Colorado Anschutz Medical Campus' Data Science to Patient Value Program and 2040 Partners for Health sought to create open learning materials for engaged citizens and community leaders regarding big data and big data methods to support their collaboration in patient-centered and participatorybased community research and evaluation. 2040 is a local nonprofit organization that cultivates partnerships in Aurora, Colorado neighborhoods to tackle critical health needs. Our goal was to co-design and co-create a series of big data learning modules accessible to community laypeople, so they might better understand big data topics and be empowered more actively engage in health research and evaluation that uses big data methods.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3331651.3331659</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ASSESSING RELIABILITY OF BIG DATA STREAM FOR SMART CITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Puangpontip, Supadchaya and Hewett, Rattikorn</field>
  <field name="keywords">IoT, Data Reliability, Theory of evidence, Smart City</field>
  <field name="abstract">Proliferation of IoT (Internet of Things) and sensor technology has expedited the realization of Smart City. To enable necessary functions, sensors distributed across the city generate a huge volume of stream data that are crucial for controlling Smart City devices. However, due to conditions such as wears and tears, battery drain, or malicious attacks, not all data are reliable even when they are accurately measured. These data could lead to invalid and devastating consequences (e.g., failed utility or transportation services). The assessment of data reliability is necessary and challenging especially for Smart City, as it has to keep up with velocity of big data stream to provide up-to-date results. Most research on data reliability has focused on data fusion and anomaly detection that lack a quantified measure of how much the data over a period of time are adequately reliable for decision-makings. This paper alleviates these issues and presents an online approach to assessing Big stream data reliability in a timely manner. By employing a well-studied evidence-based theory, our approach provides a computational framework that assesses data reliability in terms of belief likelihoods. The framework is lightweight and easy to scale, deeming fit for streaming data. We evaluate the approach using a real application of light sensing data of 1,323,298 instances. The preliminary results are consistent with logical rationales, confirming validity of the approach.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3372454.3372478</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA IN HEALTHCARE: ARE WE GETTING USEFUL INSIGHTS FROM THIS AVALANCHE OF DATA?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga, Rahmat O. and Solihudeen, Muhammad J.</field>
  <field name="keywords">Big Data, Challenges, Benefits, Analytics</field>
  <field name="abstract">The benefits of deriving useful insights from avalanche of data available everywhere cannot be overemphasized. Big Data analytics can revolutionize the healthcare industry. It can also ensure functional productivity, help forecast and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize healthcare expenditure which cuts across all stakeholders in healthcare sectors. Notwithstanding these immense capabilities available in the general application of big data; studies on derivation of useful insights from healthcare data that can enhance medical practice have received little academic attention. Therefore, this study highlighted the possibility of making very insightful healthcare outcomes with big data through a simple classification problem which classifies the tendency of individuals towards specific drugs based on personality measures. Our model though trained with less than 2000 samples and with a simple neural network architecture achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the 0.15 and 0.05 validation sets respectively. The relatively acceptable performance recorded by our model despite the small dataset could largely be attributed to number of attributes in our dataset. It is essential to uncover some of the many complexities in our societies in relations to healthcare; and through many machine learning architectures like the neural networks these complex relationships can be discovered</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3328833.3328841</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CONCEPTUAL ARCHITECTURE OF GATE BIG DATA PLATFORM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova, Irena</field>
  <field name="keywords">Big Data Value Chain, GATE Platform, Smart City, Emerging Architectures, Big Data</field>
  <field name="abstract">Today we experience a data-driven society. All human activities, industrial processes and research lead to data generation of unprecedented scale, spurring new products, services and businesses. Big Data and its application have been a target for European Commission -- with more than 100 FP7 and about 50 H2020 funded projects under Big Data domain. GATE project aims to establish and sustain in the long run a Centre of Excellence as collaborative environment for conducting Big Data research and innovation, facilitated by GATE platform and Innovation Labs. This paper proposes a conceptual architecture of GATE platform, that is holistic, symbiotic, open, evolving and data-integrated. It is also modular and with component-based design that allows to position a mix of products and tools from different providers. GATE platform will enable start-ups, SMEs and large enterprises, as well as other organizations in a wide range of sectors, to build advanced Data driven services and applications. The usability of the proposed architecture is proven through a development of a sample time series data visualization application. Its architecture follows the proposed one through implementation of required components using open technology stack.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3345252.3345282</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA CHALLENGES AND ACHIEVEMENTS: APPLICATIONS ON SMART CITIES AND ENERGY SECTOR</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh, Shadi</field>
  <field name="keywords">machin learning, data mining, data processing, OLAP, big data</field>
  <field name="abstract">In this paper, the Big Data challenges and the processing is analyzed, recently great attention has been paid to the challenges for great data, largely due to the wide spread of applications and systems used in real life, such as presentation, modeling, processing and large (often unlimited) data storage. Mass Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection. Consequently, we focus on further research trends and, as a default, we will explore a future research challenge research project in this area of research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3368691.3368717</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE 51 V'S OF BIG DATA: SURVEY, TECHNOLOGIES, CHARACTERISTICS, OPPORTUNITIES, ISSUES AND CHALLENGES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri Noorulhasan and Ahmad, Naim and Qamar, Shamimul</field>
  <field name="keywords">data storage, data characteristics, data generation, Big Data</field>
  <field name="abstract">Currently Big Data is the biggest buzzword, and definitely, we believe that Big Data is changing the world. Some researchers say Big Data will be even bigger buzzword than the Internet. With fast-growing computing resources, information and knowledge a new digital globe has emerged. Information is being created and stored at a fast rate and is being accessed by a vast range of applications through scientific computing, commercial workloads, and social media. In 2018, over 28 billion devices globally, are connected to the internet. In 2020, more than 50 billion smart appliances will be connected worldwide and internet traffic flow will be 92 times greater than it was in 2005. The usage of such a massive number of connected devices not only increase the data volume but also the velocity of data addition with speed of light on fiber optic and various wireless networks. This fast generation of enormous data creates numerous threats and challenges. There exist various approaches that are addressing issues and challenges of Big Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this work is to explore and investigate the status of the current Big Data domain. Further, a comprehensive overview of Big Data, its characteristics, opportunities, issues, and challenges have been explored and described with the help of 51 V's. The outcome of this research will help in understanding the Big Data in a systematic way.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3312614.3312623</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CONSTRUCTION AND APPLICATION OF BIG DATA ANALYSIS PLATFORM FOR ENTERPRISE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Shen, Shaoyi and Li, Bin and Li, Situo</field>
  <field name="keywords">Construction, Shared, Analysis of big data, Distributed, Data asset</field>
  <field name="abstract">A data revolution has been leading by big data, which have the extremely profound influence on the economic, social development and public life. This paper introduces the meaning of big data, and discusses the innovation and opportunity of enterprise under the perspective of big data. According to the information architecture, this paper supplies the basic construction of enterprise big data analysis platform, and suggests the strategy of application, which have certain realistic directive significance.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3374587.3374650</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA INFORMATIZATION APPLIED TO OPTIMIZATION OF HUMAN RESOURCE PERFORMANCE MANAGEMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang</field>
  <field name="keywords">Big data, human resources, performance management</field>
  <field name="abstract">With the development of technology in the era of digital big data in the network and the promotion of network technology, big data is simultaneously integrated into different industry sectors to achieve Internet performance management, and enhance the new perspective of enterprise human resources performance management activities. Today's Internet, cloud computing, Internet of Things and other industrial technologies have undergone repeated changes, showing an unprecedented picture. At present, the subjective awareness of enterprise human resources performance management is too strong, lack of objective data understanding, and the theoretical framework of big data human resource management is not fully applied. This paper reconstructs the data system from four aspects: data source, collection, integration and analysis. Innovate the human resources performance management method from the system to provide more scientific and specific ideas for human resource performance management.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3357292.3357302</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INTEGRATING DATA QUALITY REQUIREMENTS TO CITIZEN SCIENCE APPLICATION DESIGN</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Musto, Jiri and Dahanayake, Ajantha</field>
  <field name="keywords">Data Quality requirements, Data Quality Characteristics, Data quality, Conceptual model, Citizen science</field>
  <field name="abstract">Data quality is an important aspect in many fields. In citizen science application databases, data quality is often found lacking, which is why there needs to be a method of integrating data quality into the design. This paper tackles the problem by dividing data quality into separate characteristics according to the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual model of the system and data model for citizen science applications. Furthermore, the paper describes a way to measure data quality using the data quality characteristics. The models and measuring methods are theoretical and can be adapted into case specific designs.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3297662.3365797</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RESEARCH ON THE CONSTRUCTION OF BIG DATA TRADING PLATFORM IN CHINA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yu, Bangbo and Zhao, Haijun</field>
  <field name="keywords">big data trading platform, regulatory construction, Data assets</field>
  <field name="abstract">As a new type of asset, the value of big data resources can only be realized in the transaction circulation. Establishing and improving the big data trading platform market system is a systematic project to transform data from resources into assets. Through the comparative analysis of several typical big data trading platform construction practices in China, this research finds some problems, such as unclear positioning of some platforms leading to overlapping functions, extensive data transaction, lack of unified data pricing methods, unclear data ownership. In addition, there is no difference between the data escrow transaction mode and the aggregate transaction mode, and the rights of the data supply parties cannot be guaranteed. And it also discusses how to promote the construction and improvement of China's big data trading market more systematically, normally and institutionally. Finally, it proposes to build local big data trading platforms according to local conditions, establish a data transaction system based on blockchain, establish a big data transaction pricing index system, and establish a big data standard system.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3321454.3321474</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">USER STUDIES ON END-USER SERVICE COMPOSITION: A LITERATURE REVIEW AND A DESIGN FRAMEWORK</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo, Keletso J.</field>
  <field name="keywords">review framework, design guideline, service-oriented computing, empirical studies, end-user service composition, systematic review, mapshups, web services, qualitative studies, User studies</field>
  <field name="abstract">Context: End-user service composition (EUSC) is a service-oriented paradigm that aims to empower end users and allow them to compose their own web applications from reusable service components. User studies have been used to evaluate EUSC tools and processes. Such an approach should benefit software development, because incorporating end users’ feedback into software development should make software more useful and usable. Problem: There is a gap in our understanding of what constitutes a user study and how a good user study should be designed, conducted, and reported. Goal: This article aims to address this gap. Method: The article presents a systematic review of 47 selected user studies for EUSC. Guided by a review framework, the article systematically and consistently assesses the focus, methodology and cohesion of each of these studies. Results: The article concludes that the focus of these studies is clear, but their methodology is incomplete and inadequate, their overall cohesion is poor. The findings lead to the development of a design framework and a set of questions for the design, reporting, and review of good user studies for EUSC. The detailed analysis and the insights obtained from the analysis should be applicable to the design of user studies for service-oriented systems as well and indeed for any user studies related to software artifacts.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3340294</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEPENDENCIES FOR GRAPHS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Fan, Wenfei and Lu, Ping</field>
  <field name="keywords">validation, built-in predicates, TGDs, disjunction, satisfiability, axiom system, keys, Graph dependencies, conditional functional dependencies, implication, EGDs</field>
  <field name="abstract">This article proposes a class of dependencies for graphs, referred to as graph entity dependencies (GEDs). A GED is defined as a combination of a graph pattern and an attribute dependency. In a uniform format, GEDs can express graph functional dependencies with constant literals to catch inconsistencies, and keys carrying id literals to identify entities (vertices) in a graph. We revise the chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability and implication, and establish the complexity of these problems and the validation problem for GEDs, in the presence and absence of constant literals and id literals. We also develop a sound, complete and independent axiom system for finite implication of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions, to strike a balance between the expressive power and complexity. We settle the complexity of the satisfiability, implication, and validation problems for these extensions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3287285</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SURVEY ON COLLECTING, MANAGING, AND ANALYZING PROVENANCE FROM SCRIPTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Pimentel, Jo\~{a}o Felipe and Freire, Juliana and Murta, Leonardo and Braganholo, Vanessa</field>
  <field name="keywords">managing, Provenance, survey, collecting, scripts, analyzing</field>
  <field name="abstract">Scripts are widely used to design and run scientific experiments. Scripting languages are easy to learn and use, and they allow complex tasks to be specified and executed in fewer steps than with traditional programming languages. However, they also have important limitations for reproducibility and data management. As experiments are iteratively refined, it is challenging to reason about each experiment run (or trial), to keep track of the association between trials and experiment instances as well as the differences across trials, and to connect results to specific input data and parameters. Approaches have been proposed that address these limitations by collecting, managing, and analyzing the provenance of scripts. In this article, we survey the state of the art in provenance for scripts. We have identified the approaches by following an exhaustive protocol of forward and backward literature snowballing. Based on a detailed study, we propose a taxonomy and classify the approaches using this taxonomy.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3311955</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WWW '19: COMPANION PROCEEDINGS OF THE 2019 WORLD WIDE WEB CONFERENCE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">It is our great pleasure to welcome you to &lt;I&gt;The Web Conference 2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding the current state and the evolution of the Web through the lens of computer science, computational social science, economics, policy, and many other disciplines. The 2019 edition of the conference is a reflection point as we celebrate the 30th anniversary of the Web.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BEYOND OPEN VS. CLOSED: BALANCING INDIVIDUAL PRIVACY AND PUBLIC ACCOUNTABILITY IN DATA SHARING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa, Boyang and Whittington, Jan and Howe, Bill</field>
  <field name="keywords">data sharing, privacy, data ethics, data governance, algorithmic bias</field>
  <field name="abstract">Data too sensitive to be "open" for analysis and re-purposing typically remains "closed" as proprietary information. This dichotomy undermines efforts to make algorithmic systems more fair, transparent, and accountable. Access to proprietary data in particular is needed by government agencies to enforce policy, researchers to evaluate methods, and the public to hold agencies accountable; all of these needs must be met while preserving individual privacy and firm competitiveness. In this paper, we describe an integrated legal-technical approach provided by a third-party public-private data trust designed to balance these competing interests. Basic membership allows firms and agencies to enable low-risk access to data for compliance reporting and core methods research, while modular data sharing agreements support a wide array of projects and use cases. Unless specifically stated otherwise in an agreement, all data access is initially provided to end users through customized synthetic datasets that offer a) strong privacy guarantees, b) removal of signals that could expose competitive advantage, and c) removal of biases that could reinforce discriminatory policies, all while maintaining fidelity to the original data. We find that using synthetic data in conjunction with strong legal protections over raw data strikes a balance between transparency, proprietorship, privacy, and research objectives. This legal-technical framework can form the basis for data trusts in a variety of contexts.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3287560.3287577</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">GAIT-BASED PERSON RE-IDENTIFICATION: A SURVEY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C.</field>
  <field name="keywords">gait analysis, biometrics, person re-identification, machine learning, computer vision, Video surveillance</field>
  <field name="abstract">The way people walk is a strong correlate of their identity. Several studies have shown that both humans and machines can recognize individuals just by their gait, given that proper measurements of the observed motion patterns are available. For surveillance applications, gait is also attractive, because it does not require active collaboration from users and is hard to fake. However, the acquisition of good-quality measures of a person’s motion patterns in unconstrained environments, (e.g., in person re-identification applications) has proved very challenging in practice. Existing technology (video cameras) suffer from changes in viewpoint, daylight, clothing, accessories, and other variations in the person’s appearance. Novel three-dimensional sensors are bringing new promises to the field, but still many research issues are open. This article presents a survey of the work done in gait analysis for re-identification in the past decade, looking at the main approaches, datasets, and evaluation methodologies. We identify several relevant dimensions of the problem and provide a taxonomic analysis of the current state of the art. Finally, we discuss the levels of performance achievable with the current technology and give a perspective of the most challenging and promising directions of research for the future.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3243043</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE CIVIC DATA DELUGE: UNDERSTANDING THE CHALLENGES OF ANALYZING LARGE-SCALE COMMUNITY INPUT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mahyar, Narges and Nguyen, Diana V. and Chan, Maggie and Zheng, Jiayi and Dow, Steven P.</field>
  <field name="keywords">public inpu, community engagement, qualitative dataanalysis, digital civics</field>
  <field name="abstract">Advancements in digital civics have enabled leaders to engage and gather input from a broader spectrum of the public. However, less is known about the analysis process around community input and the challenges faced by civic leaders as engagement practices scale up. To understand these challenges, we conducted 21 interviews with leaders on civic-oriented projects. We found that at a small-scale, civic leaders manage to facilitate sensemaking through collaborative or individual approaches. However, as civic leaders scale engagement practices to account for more diverse perspectives, making sense of the large quantity of qualitative data becomes a challenge. Civic leaders could benefit from training in qualitative data analysis and simple, scalable collaborative analysis tools that would help the community form a shared understanding. Drawing from these insights, we discuss opportunities for designing tools that could improve civic leaders' ability to utilize and reflect public input in decisions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3322276.3322354</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA DEDUPLICATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DETECTING AREAS OF POTENTIAL HIGH PREVALENCE OF CHAGAS IN ARGENTINA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Vazquez Brust, Antonio and Olego, Tom\'{a}s and Rosati, Germ\'{a}n and Lang, Carolina and Bozzoli, Guillermo and Weinberg, Diego and Chuit, Roberto and Minnoni, Martin and Sarraute, Carlos</field>
  <field name="keywords">migrations, neglected tropical diseases, epidemics, health vulnerability, Chagas disease, call detail records, social network analysis</field>
  <field name="abstract">A map of potential prevalence of Chagas disease (ChD) with high spatial disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion (hyperendemic for the ChD), characterized by high affinity with ChD and high health vulnerability.To quantify potential prevalence, we developed several indicators: an Affinity Index which quantifies the degree of linkage between endemic areas of ChD and the rest of the country. We also studied favorable habitability conditions for Triatoma infestans, looking for areas where the predominant materials of floors, roofs and internal ceilings favor the presence of the disease vector.We studied determinants of a more general nature that can be encompassed under the concept of Health Vulnerability Index. These determinants are associated with access to health providers and the socio-economic level of different segments of the population.Finally we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the affinity index, the health vulnerability index, and the population density. We show and discuss the maps obtained. These maps are intended to assist public health specialists, decision makers of public health policies and public officials in the development of cost-effective strategies to improve access to diagnosis and treatment of ChD.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3308560.3316485</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DWELLING ON WIKIPEDIA: INVESTIGATING TIME SPENT BY GLOBAL ENCYCLOPEDIA READERS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">TeBlunthuis, Nathan and Bayer, Tilman and Vasileva, Olga</field>
  <field name="keywords">quantitative methods, peer production, readership, Wikipedia, web analytics, dwell time, digital divides</field>
  <field name="abstract">Much existing knowledge about global consumption of peer-produced information goods is supported by data on Wikipedia page view counts and surveys. In 2017, the Wikimedia Foundation began measuring the time readers spend on a given page view (dwell time), enabling a more detailed understanding of such reading patterns. In this paper, we validate and model this new data source and, building on existing findings, use regression analysis to test hypotheses about how patterns in reading time vary between global contexts. Consistent with prior findings from self-report data, our complementary analysis of behavioral data provides evidence that Global South readers are more likely to use Wikipedia to gain in-depth understanding of a topic. We find that Global South readers spend more time per page view and that this difference is amplified on desktop devices, which are thought to be better suited for in-depth information seeking tasks.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3306446.3340829</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EVALUATING DOMAIN ONTOLOGIES: CLARIFICATION, CLASSIFICATION, AND CHALLENGES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">McDaniel, Melinda and Storey, Veda C.</field>
  <field name="keywords">domain ontology, ontology application, evaluation, ontology development, applied ontology, Ontology, task-ontology fit, assessment, metrics</field>
  <field name="abstract">The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3329124</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RAHA: A CONFIGURATION-FREE ERROR DETECTION SYSTEM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez, Raul and Madden, Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan</field>
  <field name="keywords">label propagation, data cleaning, semi-supervised learning, error detection, machine learning, classification, clustering, historical data</field>
  <field name="abstract">Detecting erroneous values is a key step in data cleaning. Error detection algorithms usually require a user to provide input configurations in the form of rules or statistical parameters. However, providing a complete, yet correct, set of configurations for each new dataset is not trivial, as the user has to know about both the dataset and the error detection algorithms upfront. In this paper, we present Raha, a new configuration-free error detection system. By generating a limited number of configurations for error detection algorithms that cover various types of data errors, we can generate an expressive feature vector for each tuple value. Leveraging these feature vectors, we propose a novel sampling and classification scheme that effectively chooses the most representative values for training. Furthermore, our system can exploit historical data to filter out irrelevant error detection algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art error detection techniques with no more than 20 labeled tuples on each dataset.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3299869.3324956</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE HANDBOOK OF MULTIMODAL-MULTISENSOR INTERFACES: LANGUAGE PROCESSING, SOFTWARE, COMMERCIALIZATION, AND EMERGING DIRECTIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">OPENING PRIVACY SENSITIVE MICRODATA SETS IN LIGHT OF GDPR</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak, Susan and Schirm, Walter and Choenni, Sunil</field>
  <field name="keywords">Open data, Microdata, GDPR, Data protection, Criminal justice data, Privacy, Justice domain data</field>
  <field name="abstract">To enhance the transparency, accountability and efficiency of the Dutch Ministry of Justice and Security, the ministry has set up an open data program to proactively stimulate sharing its (publicly funded) data sets with the public. Disclosure of personal data is considered as one of the main threats for data opening. In this contribution we argue that, according to Dutch laws, the criminal data within the Dutch justice domain are sensitive data in GDPR terms and that the criminal data can only be opened if these sensitive data are transformed to have no personal information. Subsequently, having no personal information in data sets is related to two GDPR concepts: the data being anonymous in its GDPR sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts, i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished in our setting based on whether the data controller cannot or can revert the data protection process, respectively. (Note that the terms anonymous and pseudonymized are interpreted differently in the technical domain.) We examine realizing these GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently argue that pseudonymized data in a GDPR sense delivers a better data utility than the other. At the end, we present a number of the consequences of adopting either of these concepts, which can inform legislators and policymakers to define their strategy for opening privacy sensitive microdata sets, like those pertaining to the Dutch criminal justice domain.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3325112.3325222</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WAVEFORM SIGNAL ENTROPY AND COMPRESSION STUDY OF WHOLE-BUILDING ENERGY DATASETS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Kriechbaumer, Thomas and Jorde, Daniel and Jacobsen, Hans-Arno</field>
  <field name="keywords">file format, waveform compression, high sampling rate, non-intrusive load monitoring, Energy dataset, electricity aggregate</field>
  <field name="abstract">Electrical energy consumption has been an ongoing research area since the coming of smart homes and Internet of Things. Consumption characteristics and usages profiles are directly influenced by building occupants and their interaction with electrical appliances. Data analysis together with machine learning models can be utilized to extract valuable information for the benefit of occupants themselves (conserve energy and increase comfort levels), power plants (maintenance), and grid operators (stability). Public energy datasets provide a scientific foundation to develop and benchmark these algorithms and techniques. With datasets exceeding tens of terabytes, we present a novel study of five whole-building energy datasets with high sampling rates, their signal entropy, and how a well-calibrated measurement can have a significant effect on the overall storage requirements. We show that some datasets do not fully utilize the available measurement precision, therefore leaving potential accuracy and space savings untapped. We benchmark a comprehensive list of 365 file formats, transparent data transformations, and lossless compression algorithms. The primary goal is to reduce the overall dataset size while maintaining an easy-to-use file format and access API. We show that with careful selection of file format and encoding scheme, we can reduce the size of some datasets by up to 73%.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3307772.3328285</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SECURE TRANSMISSION SCHEME OF SENSITIVE POWER INFORMATION IN UBIQUITOUS POWER IOT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Luo, Jingtang and Yao, Shiying and Gou, Jijun and Shuai, Lisha and Cao, Yu</field>
  <field name="keywords">Ubiquitous Power IoT, Data Splitting, Information Security</field>
  <field name="abstract">As one of the national key infrastructures, the ubiquitous power Internet of Things (IoT) provides a convenient method for large-scale power information collection. The widespread transmission of massive power information using data mining techniques for large amounts of data can yield valuable information. Therefore, hacker attacks are endless, posing a threat to the security of the state, society, collectives and individuals. In this paper, we propose a secure transmission scheme of power information, named "SSD" (Split &amp; Signature &amp; Disturbing). In the scheme, after the data is split, it will be anonymized and selected for different paths to be transferred to the destination node. After recombination, the data will be restored. The SSD ensures the indistinguishability and security of the sensitive data by data splitting and disturbing method, and protects the anonymity of individual identities by group signature. The experimental results show that the individual prediction/actual data similarity approaches 0%, and the similarity ratio of the category data (three types in the experiment) is 37.32%, which can be judged to be basically non-correlated.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3374587.3374631</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA TRANSFORMATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEEPFUSION: A DEEP LEARNING FRAMEWORK FOR THE FUSION OF HETEROGENEOUS SENSORY DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong and Su, Lu</field>
  <field name="keywords">Sensor Fusion, Deep Learning, Internet of Things</field>
  <field name="abstract">In recent years, significant research efforts have been spent towards building intelligent and user-friendly IoT systems to enable a new generation of applications capable of performing complex sensing and recognition tasks. In many of such applications, there are usually multiple different sensors monitoring the same object. Each of these sensors can be regarded as an information source and provides us a unique "view" of the observed object. Intuitively, if we can combine the complementary information carried by multiple sensors, we will be able to improve the sensing performance. Towards this end, we propose DeepFusion, a unified multi-sensor deep learning framework, to learn informative representations of heterogeneous sensory data. DeepFusion can combine different sensors' information weighted by the quality of their data and incorporate cross-sensor correlations, and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed DeepFusion model, we set up two real-world human activity recognition testbeds using commercialized wearable and wireless sensing devices. Experiment results show that DeepFusion can outperform the state-of-the-art human activity recognition methods.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3323679.3326513</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A SURVEY ON BIG MULTIMEDIA DATA PROCESSING AND MANAGEMENT IN SMART CITIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Usman, Muhammad and Jan, Mian Ahmad and He, Xiangjian and Chen, Jinjun</field>
  <field name="keywords">IoMT, smart cities, management, machine learning, multimedia</field>
  <field name="abstract">Integration of embedded multimedia devices with powerful computing platforms, e.g., machine learning platforms, helps to build smart cities and transforms the concept of Internet of Things into Internet of Multimedia Things (IoMT). To provide different services to the residents of smart cities, the IoMT technology generates big multimedia data. The management of big multimedia data is a challenging task for IoMT technology. Without proper management, it is hard to maintain consistency, reusability, and reconcilability of generated big multimedia data in smart cities. Various machine learning techniques can be used for automatic classification of raw multimedia data and to allow machines to learn features and perform specific tasks. In this survey, we focus on various machine learning platforms that can be used to process and manage big multimedia data generated by different applications in smart cities. We also highlight various limitations and research challenges that need to be considered when processing big multimedia data in real-time.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3323334</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SERVERLESS WORKFLOWS FOR INDEXING LARGE SCIENTIFIC DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Skluzacek, Tyler J. and Chard, Ryan and Wong, Ryan and Li, Zhuozhao and Babuji, Yadu N. and Ward, Logan and Blaiszik, Ben and Chard, Kyle and Foster, Ian</field>
  <field name="keywords">materials science, serverless, file systems, metadata extraction, data lakes</field>
  <field name="abstract">The use and reuse of scientific data is ultimately dependent on the ability to understand what those data represent, how they were captured, and how they can be used. In many ways, data are only as useful as the metadata available to describe them. Unfortunately, due to growing data volumes, large and distributed collaborations, and a desire to store data for long periods of time, scientific "data lakes" quickly become disorganized and lack the metadata necessary to be useful to researchers. New automated approaches are needed to derive metadata from scientific files and to use these metadata for organization and discovery. Here we describe one such system, Xtract, a service capable of processing vast collections of scientific files and automatically extracting metadata from diverse file types. Xtract relies on function as a service models to enable scalable metadata extraction by orchestrating the execution of many, short-running extractor functions. To reduce data transfer costs, Xtract can be configured to deploy extractors centrally or near to the data (i.e., at the edge). We present a prototype implementation of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data repository.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3366623.3368140</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEEP LEARNING FOR SENTIMENT ANALYSIS OF ARABIC TEXT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Soufan, Ayah</field>
  <field name="keywords">nan</field>
  <field name="abstract">Deep learning has been very successful in the past decades, especially in Computer Vision and Speech Recognition fields. It has been also used successfully in the Natural Language Processing field because of the availability of an enormous amount of online text data, such as social networks and reviews websites, which have gained a lot of popularity and success in the past years. Sentiment Analysis is one of the hottest applications of Natural Language Processing (NLP). Many researchers have done excellent work on Sentiment Analysis for English language. However, the amount of work on Sentiment Analysis for Arabic language is, in comparison, very limited due to the complexity of the Arabic language's morphology and orthography. Unlike the English language, Arabic has many different dialects which makes Sentiment Analysis for Arabic more difficult and challenging, especially when working on data collected from social networks, which is known to be unstructured and noisy. Most of the work that has been done on Sentiment Analysis of Arabic language, focused on using lexicons and basic machine learning models. In addition, most of the work has been done on small datasets because of the limited number of the available annotated datasets for Arabic language. This paper proposes state-of-the-art research for Sentiment Analysis of Arabic microblogging using new techniques, and a sophisticated Arabic text data preprocessing.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3333165.3333185</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS PROFIT OPTIMIZATION DURING ONLINE PARTICIPANT SELECTION IN COMPRESSIVE MOBILE CROWDSENSING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Chen, Yueyue and Guo, Deke and Bhuiyan, MD Zakirul Alam and Xu, Ming and Wang, Guojun and Lv, Pin</field>
  <field name="keywords">Compressive mobile crowdsensing, data reconstruction, online participant selection</field>
  <field name="abstract">A mobile crowdsensing (MCS) platform motivates employing participants from the crowd to complete sensing tasks. A crucial problem is to maximize the profit of the platform, i.e., the charge of a sensing task minus the payments to participants that execute the task. In this article, we improve the profit via the data reconstruction method, which brings new challenges, because it is hard to predict the reconstruction quality due to the dynamic features and mobility of participants. In particular, two Profit-driven Online Participant Selection (POPS) problems under different situations are studied in our work: (1) for S-POPS, the sensing cost of the different parts within the target area is the Same. Two mechanisms are designed to tackle this problem, including the ProSC and ProSC+. An exponential-based quality estimation method and a repetitive cross-validation algorithm are combined in the former mechanism, and the spatial distribution of selected participants are further discussed in the latter mechanism; (2) for V-POPS, the sensing cost of different parts within the target area is Various, which makes it the NP-hard problem. A heuristic mechanism called ProSCx is proposed to solve this problem, where the searching space is narrowed and both the participant quantity and distribution are optimized in each slot. Finally, we conduct comprehensive evaluations based on the real-world datasets. The experimental results demonstrate that our proposed mechanisms are more effective and efficient than baselines, selecting the participants with a larger profit for the platform.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3342515</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RULE-BASED DATA CLEANING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EXTRACTING 3D MAPS FROM CROWDSOURCED GNSS SKYVIEW DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Rodrigues, Jo\~{a}o G. P. and Aguiar, Ana</field>
  <field name="keywords">crowdsensing, 3d mapping, gnss snr measurements</field>
  <field name="abstract">3D maps of urban environments are useful in various fields ranging from cellular network planning to urban planning and climatology. These models are typically constructed using expensive techniques such as manual annotation with 3D modeling tools, extrapolated from satellite or aerial photography, or using specialized hardware with depth sensing devices. In this work, we show that 3D urban maps can be extracted from standard GNSS data, by analyzing the received satellite signals that are attenuated by obstacles, such as buildings. Furthermore, we show that these models can be extracted from low-accuracy GNSS data, crowdsourced opportunistically from standard smartphones during their user's uncontrolled daily commute trips, unleashing the potential of applying the principle to wide areas. Our proposal incorporates position inaccuracies in the calculations, and accommodates different sources of variability of the satellite signals' SNR. The diversity of collection conditions of crowdsourced GNSS positions is used to mitigate bias and noise from the data. A binary classification model is trained and evaluated on multiple urban scenarios using data crowdsourced from over 900 users. Our results show that the generalization accuracy for a Random Forest classifier in typical urban environments lies between 79% and 91% on 4 m wide voxels, demonstrating the potential of the proposed method for building 3D maps for wide urban areas.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3300061.3345456</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">MACHINE LEARNING AND PROBABILISTIC DATA CLEANING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WORLD OF CODE: AN INFRASTRUCTURE FOR MINING THE UNIVERSE OF OPEN SOURCE VCS DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and Mockus, Audris</field>
  <field name="keywords">software supply chain, software ecosystem, software mining</field>
  <field name="abstract">Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are tens of millions of projects in the periphery interconnected through technical dependencies, code sharing, or knowledge flows? To answer such questions we a) create a very large and frequently updated collection of version control data for FLOSS projects named World of Code (WoC) and b) provide basic tools for conducting research that depends on measuring interdependencies among all FLOSS projects. Our current WoC implementation is capable of being updated on a monthly basis and contains over 12B git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/MSR.2019.00031</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS AN END-TO-END HUMAN-CENTRIC DATA CLEANING FRAMEWORK</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Rezig, El Kindi and Ouzzani, Mourad and Elmagarmid, Ahmed K. and Aref, Walid G. and Stonebraker, Michael</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data Cleaning refers to the process of detecting and fixing errors in the data. Human involvement is instrumental at several stages of this process such as providing rules or validating computed repairs. There is a plethora of data cleaning algorithms addressing a wide range of data errors (e.g., detecting duplicates, violations of integrity constraints, and missing values). Many of these algorithms involve a human in the loop, however, this latter is usually coupled to the underlying cleaning algorithms. In a real data cleaning pipeline, several data cleaning operations are performed using different tools. A high-level reasoning on these tools, when combined to repair the data, has the potential to unlock useful use cases to involve humans in the cleaning process. Additionally, we believe there is an opportunity to benefit from recent advances in active learning methods to minimize the effort humans have to spend to verify data items produced by tools or humans. There is currently no end-to-end data cleaning framework that systematically involves humans in the cleaning pipeline regardless of the underlying cleaning algorithms. In this paper, we present opportunities that this framework could offer, and highlight key challenges that need to be addressed to realize this vision. We present a design vision and discuss scenarios that motivate the need for this framework to judiciously assist humans in the cleaning process.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3328519.3329133</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ENABLING DATA TRUSTWORTHINESS AND USER PRIVACY IN MOBILE CROWDSENSING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang, Dejun</field>
  <field name="keywords">nan</field>
  <field name="abstract">Ubiquitous mobile devices with rich sensors and advanced communication capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities of mobile users and the openness of sensing paradigms raise concerns for data trustworthiness, user privacy, and incentive provision. Instead of considering these issues as isolated modules in most existing researches, we comprehensively capture both conflict and inner-relationship among them. In this paper, we propose a holistic solution for trustworthy and privacy-aware mobile crowdsensing with no need of a trusted third party. Specifically, leveraging cryptographic technologies, we devise a series of protocols to enable benign users to request tasks, contribute their data, and earn rewards anonymously without any data linkability. Meanwhile, an anonymous trust/reputation model is seamlessly integrated into our scheme, which acts as reference for our fair incentive design, and provides evidence to detect malicious users who degrade the data trustworthiness. Particularly, we first propose the idea of limiting the number of issued pseudonyms which serves to efficiently tackle the anonymity abuse issue. Security analysis demonstrates that our proposed scheme achieves stronger security with resilience against possible collusion attacks. Extensive simulations are presented which demonstrate the efficiency and practicality of our scheme.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/TNET.2019.2944984</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IS YELP ACTUALLY CLEANING UP THE RESTAURANT INDUSTRY? A RE-ANALYSIS ON THE RELATIVE USEFULNESS OF CONSUMER REVIEWS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Altenburger, Kristen M. and Ho, Daniel E.</field>
  <field name="keywords">food safety, regulation, replication, Yelp, consumer reviews</field>
  <field name="abstract">Social media provides the government with novel methods to improve regulation. One leading case has been the use of Yelp reviews to target food safety inspections. While previous research on data from Seattle finds that Yelp reviews can predict unhygienic establishments, we provide a more cautionary perspective. First, we show that prior results are sensitive to what we call “Extreme Imbalanced Sampling”: extreme because the dataset was restricted from roughly 13k inspections to a sample of only 612 inspections with only extremely high or low inspection scores, and imbalanced by not accounting for class imbalance in the population. We show that extreme imbalanced sampling is responsible for claims about the power of Yelp information in the original classification setup. Second, a re-analysis that utilizes the full dataset of 13k inspections and models the full inspection score (regression instead of classification) shows that (a) Yelp information has lower predictive power than prior inspection history and (b) Yelp reviews do not significantly improve predictions, given existing information about restaurants and inspection history. Contrary to prior claims, Yelp reviews do not appear to aid regulatory targeting. Third, this case study highlights critical issues when using social media for predictive models in governance and corroborates recent calls for greater transparency and reproducibility in machine learning.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3308558.3313683</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">MODELLING AND LINKING COMPANY DATA IN THE EUBUSINESSGRAPH PLATFORM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and Roman, Dumitru</field>
  <field name="keywords">Record Linkage, RDF, Entity Matching, Company data</field>
  <field name="abstract">In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3336499.3338012</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA COLLECTION SCHEME WITH MINIMUM COST AND LOCATION OF EMOTIONAL RECOGNITION EDGE DEVICES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang</field>
  <field name="keywords">Data acquisition, Location, Edge devices, Emotional recognition, Data collection, Collection cost</field>
  <field name="abstract">This paper develops a real-time and reliable data collection system for big scale emotional recognition systems. Based on the data sample set collected in the initialization stage and by considering the dynamic migration of emotional recognition data, we design an adaptive Kth average device clustering algorithm for migration perception. We define a sub-modulus weight function, which minimizes the sum of the weights of the subsets covered by a cover to achieve high-precision device positioning. Combining the energy of the data collection devices and the energy of the wireless emotional device, we balance the data collection efficiency and energy consumption, and define a minimum access number problem based on energy and storage space constraints. By designing an approximate algorithm to solve the approximate minimum Steiner point problem, the continuous collection of emotional recognition data and the connectivity of data acquisition devices are guaranteed under the energy constraint of wireless devices. We validate the proposed algorithms through simulation experiments using different emotional recognition systems and different data scale. Furthermore, we analyze the proposed algorithms in terms of topology for devices classification, location accuracy, and data collection efficiency by comparing with the Bayesian classifier-based expectation maximization algorithm, the background difference-based moving target detection arithmetic averaging algorithm, and the Hungarian algorithm for solving the assignment problem.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1007/s00779-019-01217-0</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AI INNOVATION FOR ADVANCING PUBLIC SERVICE: THE CASE OF CHINA'S FIRST ADMINISTRATIVE APPROVAL BUREAU</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Chen, Tao and Ran, Longya and Gao, Xian</field>
  <field name="keywords">nan</field>
  <field name="abstract">The adoption of artificial intelligence (AI) is becoming increasingly popular in the public sector, but there is a severe lack of relevant theoretical research. The government of China also has high expectations for AI innovation. This paper proposes a four-stage model for AI development in public sectors to help public administrators think about the impact of AI on their organizations. We empirically investigate a case of AI adoption for delivering public services in local government in China. The findings improve our understanding of not only the status of AI innovation but also the factors motivating and challenging public sectors that are intending to adopt AI. Given that AI application in public sectors is still in its infancy, this study provides us with an opportunity to conduct longitudinal tracking of AI innovation in local government in China. This could help public administrators to think more comprehensively about the changes and transformations that AI may bring to the public sector.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3325112.3325243</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WHEN THE POWER OF THE CROWD MEETS THE INTELLIGENCE OF THE MIDDLEWARE: THE MOBILE PHONE SENSING CASE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise</field>
  <field name="keywords">nan</field>
  <field name="abstract">The data gluttony of AI is well known: Data fuels the artificial intelligence. Technologies that help to gather the needed data are then essential, among which the IoT. However, the deployment of IoT solutions raises significant challenges, especially regarding the resource and financial costs at stake. It is our view that mobile crowdsensing, aka phone sensing, has a major role to play because it potentially contributes massive data at a relatively low cost. Still, crowdsensing is useless, and even harmful, if the contributed data are not properly analyzed. This paper surveys our work on the development of systems facing this challenge, which also illustrates the virtuous circles of AI. We specifically focus on how intelligent crowdsensing middleware leverages on-device machine learning to enhance the reported physical observations. Keywords: Crowdsensing, Middleware, Online learning.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3352020.3352033</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEPENDENCIES FOR GRAPHS: CHALLENGES AND OPPORTUNITIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Fan, Wenfei</field>
  <field name="keywords">dependency discovery, implication, error detection, certain fixes, satisfiability, Dependencies, validation, graphs</field>
  <field name="abstract">What are graph dependencies? What do we need them for? What new challenges do they introduce? This article tackles these questions. It aims to incite curiosity and interest in this emerging area of research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3310230</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">OUTLIER DETECTION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PRIVACY AND INFORMATION PROTECTION FOR A NEW GENERATION OF CITY SERVICES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dominguez, Hector and Mowry, Judith and Perez, Elisabeth and Kendrick, Christine and Martin, Kevin</field>
  <field name="keywords">Digital equity, government services, Automatic decision systems, Privacy, Digital Inclusion</field>
  <field name="abstract">This paper will showcase the work that the City of Portland has done around developing Privacy and Information Protection Principles considering the current state of technology, the social digital age, and advance inference algorithms like machine learning or other Artificial Intelligence tools. By creating more responsible data stewardship in the public sector, municipalities are set to build trusted information networks involving communities and complex social issues. Particularly, the promotion of data privacy can lead to the emergence of anti-poverty and economic development strategies.The City of Portland has developed seven Privacy and Information Protection Principles: Transparency and accountability, full lifecycle stewardship, equitable data management, ethical and non-discriminatory use of data, data openness, automated decision systems, and data utility. These principles have implications in social equity and the future of technology management in smart cities projects. Principle implementation involves the collaboration of different agencies, particularly focused on ethics and human rights supporting sustainable development.This work is part of emergent strategies for a new generation of city services based on data and information, which aim to improve civic engagement, social benefits to communities in city neighborhoods and better collaboration with partners and other government agencies.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3357492.3358628</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PRACTICE OF CONSTRUCTING NAME AUTHORITY DATABASE BASED ON MULTI-SOURCE DATA INTEGRATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang, Yongwen and Jianhua, Liu</field>
  <field name="keywords">multi-source, name disambiguation, NSTL, name authority</field>
  <field name="abstract">Name authority is a common issue in digital library. This paper mainly summarizes the practice of constructing name authority database based on multi-source data in NSTL. Firstly, we load, integrate different source data and convert them into unified structure. Then, we extract scientific entities and relationships from these data, according to metadata model. For different entities, we use different disambiguation rules and algorithms. As a result, we have constructed author name authority database, institution authority database, journal authority database, and fund authority database. Compared with Incites, taking six institutions name authority data as a sample, the result shows that the average accuracy can reach 86.8%.1</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/JCDL.2019.00088</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RESOLVING DATA INTEROPERABILITY IN UBIQUITOUS HEALTH PROFILE USING SEMI-STRUCTURED STORAGE AND PROCESSING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad Masood and Lee, Sungyoung</field>
  <field name="keywords">ACM proceedings, text tagging</field>
  <field name="abstract">Advancements in the field of healthcare information management have led to the development of a plethora of software, medical devices and standards. As a consequence, the rapid growth in quantity and quality of medical data has compounded the problem of heterogeneity; thereby decreasing the effectiveness and increasing the cost of diagnostics, treatment and follow-up. However, this problem can be resolved by using a semi-structured data storage and processing engine, which can extract semantic value from a large volume of patient data, produced by a variety of data sources, at variable rates and conforming to different abstraction levels. Going beyond the traditional relational model and by re-purposing state-of-the-art tools and technologies, we present, the Ubiquitous Health Profile (UHPr), which enables a semantic solution to the data interoperability problem, in the domain of healthcare1.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3297280.3297354</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PASSAGE CHALLENGES FROM DATA-INTENSIVE SYSTEM TO KNOWLEDGE-INTENSIVE SYSTEM RELATED TO PROCESS MINING FIELD</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lamghari, Zineb and Radgui, Maryam and Saidi, Rajaa and Rahmani, Moulay Driss</field>
  <field name="keywords">Business Process Management, Adaptive Case Management, Data-intensive, Knowledge-intensive, Process Mining, Process mining challenges</field>
  <field name="abstract">Process mining has emerged as a research field that focuses on the analysis of processes using event data. Process mining is a current topic that reveals several challenges, the most important of which have defined in the Process Mining Manifesto [1]. However, none of the published works have mentioned the progress of process challenges from data-intensive system to knowledge-intensive system related to process mining field. Therefore, the objective of this paper is to provide researchers with the recent challenges emerged during the passage from data-intensive system to knowledge-intensive system.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3333165.3333168</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BRINGING OPEN DATA INTO DANISH SCHOOLS AND ITS POTENTIAL IMPACT ON SCHOOL PUPILS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Saddiqa, Mubashrah and Rasmussen, Lise and Magnussen, Rikke and Larsen, Birger and Pedersen, Jens Myrup</field>
  <field name="keywords">impact, educational themes, educational resource, open data, school pupils</field>
  <field name="abstract">Private and public institutions are using open and public data to provide better services, which increases the impact of open data on daily life. With the advancement of technology, it becomes also important to equip our younger generation with the essential skills for future challenges. In order to bring up a generation equipped with 21st century skills, open data could facilitate educational processes at school level as an educational resource. Open data could acts as a key resource to enhance the understanding of data through critical thinking and ethical vision among the youth and school pupils. To bring open data into schools, it is important to know the teacher's perspective on open data literacy and its possible impact on pupils. As a research contribution, we answered these questions through a Danish public school teacher's survey where we interviewed 10 Danish public school teachers of grade 5-7th and analyzed their views about the impact of open data on pupils' learning development. After analyzing Copenhagen city's open data, we identified four open data educational themes that could facilitate different subjects, e.g. geography, mathematics, basic science and social science. The survey includes interviews, open discussions, questionnaires and an experiment with the grade 7th pupils, where we test the pupils' understanding with open data. The survey concluded that open data cannot only empower pupils to understand real facts about their local areas, improve civics awareness and develop digital and data skills, but also enable them to come up with the ideas to improve their communities.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3306446.3340821</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WORKSHOP ON BARRIERS TO DATA SCIENCE ADOPTION: WHY EXISTING FRAMEWORKS AREN'T WORKING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Alexander, Rohan and Lyons, Kelly and Alexopoulos, Michelle and Austin, Lisa</field>
  <field name="keywords">organisational, data science adoption, legal, business practices, challenges</field>
  <field name="abstract">Data science is an interdisciplinary scientific approach that provides methods to understand and solve problems in an evidence-based manner, using data and experience. Despite the clear benefits from adoption, many firms face challenges, be that legal, organisational, or business practices, when seeking to implement and embed data science within an existing framework. In this workshop, panel and audience members drew on their experiences to elaborate on the challenges encountered when attempting to deploying data science within existing frameworks. Panel and audience members were drawn from business, academia, and think-tanks. For discussion purposes the challenges were grouped within three themes: regulatory; investment; and workforce.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">LDC '19: INTERNATIONAL WORKSHOP ON LONGITUDINAL DATA COLLECTION IN HUMAN SUBJECT STUDIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Manea, Vlad and Berrocal, Allan and De Masi, Alexandre and M\o{}ller, Naja Holten and Wac, Katarzyna and Bayer, Hannah and Lehmann, Sune and Ashley, Euan</field>
  <field name="keywords">panel technique, human sensing, human subject studies, mobile devices, longitudinal studies, in situ</field>
  <field name="abstract">Individuals increasingly use mobile, wearable, and ubiquitous devices capable of unobtrusive collection of vast amounts of scientifically rich personal data over long periods (months to years), and in the context of their daily life. However, numerous human and technological factors challenge longitudinal data collection, often limiting research studies to very short data collection periods (days to weeks), spawning recruitment biases, and affecting participant retention over time. This workshop is designed to bring together researchers involved in longitudinal data collection studies to foster an insightful exchange of ideas, experiences, and discoveries to improve the studies' reliability, validity, and perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection for the participants.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3341162.3347758</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A QUANTITATIVE EVALUATION OF TRUST IN THE QUALITY OF CYBER THREAT INTELLIGENCE SOURCES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Schaberreiter, Thomas and Kupfersberger, Veronika and Rantos, Konstantinos and Spyros, Arnolnt and Papanikolaou, Alexandros and Ilioudis, Christos and Quirchmayr, Gerald</field>
  <field name="keywords">quality parameters, cyber threat information sharing, trust indicators, Cooperative and collaborative cybersecurity, cyber threat intelligence source evaluation</field>
  <field name="abstract">Threat intelligence sharing has become a cornerstone of cooperative and collaborative cybersecurity. Sources providing such data have become more widespread in recent years, ranging from public entities (driven by legislatorial changes) to commercial companies and open communities that provide threat intelligence in order to help organisations and individuals to better understand and assess the cyber threat landscape putting their systems at risk. Tool support to automatically process this information is emerging concurrently. It has been observed that the quality of information received by the sources varies significantly and that in order to assess the quality of a threat intelligence source it is not sufficient to only consider qualitative indications of the source itself, but it is necessary to monitor the data provided by the source continuously to be able to draw conclusions about the quality of information provided by a source. In this paper, we propose a methodology for evaluating cyber threat information sources based on quantitative parameters. The methodology aims to facilitate trust establishment to threat intelligence sources, based on a weighted evaluation method that allows each entity to adapt it to its own needs and priorities. The approach facilitates automated tools utilising threat intelligence, since information to be considered can be prioritised based on which source is trusted the most at the time the intelligence arrives.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3339252.3342112</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">REAL-TIME DYNAMIC DATA DESENSITIZATION METHOD BASED ON DATA STREAM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Tian, Bing and Lv, Shuqing and Yin, Qilin and Li, Ning and Zhang, Yue and Liu, Ziyan</field>
  <field name="keywords">data desensitization, stream data, dynamic desensitization</field>
  <field name="abstract">With the rapid development of the data mining industry, the value hidden in the massive data has been discovered, but at the same time it has also raised concerns about privacy leakage, leakage of sensitive data and other issues. These problems have also become numerous studies. Among the methods for solving these problems, data desensitization technology has been widely adopted for its outstanding performance. However, with the increasing scale of data and the increasing dimension of data, the traditional desensitization method for static data can no longer meet the requirements of various industries in today's environment to protect sensitive data. In the face of ever-changing data sets of scale and dimension, static desensitization technology relies on artificially designated desensitization rules to grasp the massive data, and it is difficult to control the loss of data connotation. In response to these problems, this paper proposes a real-time dynamic desensitization method based on data flow, and combines the data anonymization mechanism to optimize the data desensitization strategy. Experiments show that this method can efficiently and stably perform real-time desensitization of stream data, and can save more information to support data mining in the next steps.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3373477.3373499</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">REFERENCES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS A SEAMLESS COORDINATION OF CLOUD AND FOG: ILLUSTRATION THROUGH THE INTERNET-OF-THINGS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Maamar, Zakaria and Baker, Thar and Faci, Noura and Ugljanin, Emir and Khafajiy, Mohammed Al and Bur\'{e}gio, Vanilson</field>
  <field name="keywords">healthcare, cloud, coordination, internet-of-things, fog</field>
  <field name="abstract">With the increasing popularity of the Internet-of-Things (IoT), organizations are revisiting their practices as well as adopting new ones so they can deal with an ever-growing amount of sensed and actuated data that IoT-compliant things generate. Some of these practices are about the use of cloud and/or fog computing. The former promotes "anything-as-a-service" and the latter promotes "process data next to where it is located". Generally presented as competing models, this paper discusses how cloud and fog could work hand-in-hand through a seamless coordination of their respective "duties". This coordination stresses out the importance of defining where the data of things should be sent (either cloud, fog, or cloud&amp;fog concurrently) and in what order (either cloud then fog, fog then cloud, or fog&amp;cloud concurrently). Applications' concerns with data such as latency, sensitivity, and freshness dictate both the appropriate recipients and the appropriate orders. For validation purposes, a healthcare-driven IoT application along with an in-house testbed, that features real sensors and fog and cloud platforms, have permitted to carry out different experiments that demonstrate the technical feasibility of the coordination model.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3297280.3297477</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A NOVEL EXPERIENCE-BASED INCENTIVE MECHANISM FOR MOBILE CROWDSENSING SYSTEM</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Tan, Wenan and Jiang, Zihui</field>
  <field name="keywords">mobile crowdsensing, sensor network, fairness competition, incentive mechanism</field>
  <field name="abstract">While sensor networks have been pervasively deployed in the real world, more and more mobile crowdsensing (MCS) applications have come into realization to collaboratively detect events and collect data. This paper aims to design a novel incentive mechanism to achieve good services for mobile crowdsensing applications. Responding to insufficient participants, we propose a novel Experience-Based incentive mechanism using Reverse Auction (EBRA). Additionally, it can also guarantee fair competition while maximizing the total profit of the service platform. Through strictly proving, our proposed EBRA incentive mechanism satisfies four properties: computational efficiency, individual rationality, profitability, and truthfulness. The extensive simulations show that the proposed EBRA method has a better performance over 20% than other benchmark mechanisms.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3371425.3371459</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">COMPARING GRAY BOX METHODS TO DERIVE BUILDING PROPERTIES FROM SMART THERMOSTAT DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Baasch, Gaby and Wicikowski, Adam and Faure, Ga\"{e}lle and Evins, Ralph</field>
  <field name="keywords">thermal characteristics, gray box models, smart thermostats, Buildings</field>
  <field name="abstract">The development of quantitative techniques for determining the amount of heat lost through the building envelope is essential for targeted retrofits. This type of evaluation is traditionally a resource intensive process that involves onsite appraisal and in-situ measurements. In order to build more efficient and scalable methods for retrofit analysis, new sources of data could be used. Smart thermostat data, for example, provide a valuable resource, however they often lack detailed information about the building characteristics and energy loads. This paper presents and compares three methods for assessing heating characteristics of households using a dataset that does not contain heating power. The three methods are based on: (1) balance point plots, (2) the extraction of indoor temperature decay curves, and (3) the classic differential equation for indoor temperature. These methods all take a gray box approach in which physics-based and machine learning models are combined. The dataset used for this study consists of over 4,000 houses in Ontario and New York. The three methods are applied to each building and the resulting data is analyzed to determine whether the results are statistically sound. It is found that there is a positive linear correlation between characteristics derived for each method, although there is uncertainty about absolute values. This result indicates that the methods can be used to ascertain relative values for the thermal characteristics of a building. The methods suggested in this paper may therefore be used to filter heating profiles to target potential retrofit measures or other stock-level decisions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3360322.3360836</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CONCLUSION AND FUTURE THOUGHTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CLEANITS: A DATA CLEANING SYSTEM FOR INDUSTRIAL TIME SERIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Li, Zijue and Li, Jianzhong and Gao, Hong</field>
  <field name="keywords">nan</field>
  <field name="abstract">The great amount of time series generated by machines has enormous value in intelligent industry. Knowledge can be discovered from high-quality time series, and used for production optimization and anomaly detection in industry. However, the original sensors data always contain many errors. This requires a sophisticated cleaning strategy and a well-designed system for industrial data cleaning. Motivated by this, we introduce Cleanits, a system for industrial time series cleaning. It implements an integrated cleaning strategy for detecting and repairing three kinds of errors in industrial time series. We develop reliable data cleaning algorithms, considering features of both industrial time series and domain knowledge. We demonstrate Cleanits with two real datasets from power plants. The system detects and repairs multiple dirty data precisely, and improves the quality of industrial time series effectively. Cleanits has a friendly interface for users, and result visualization along with logs are available during each cleaning process.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.14778/3352063.3352066</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">UNDERSTANDING HUMAN MOBILITY: A MULTI-MODAL AND INTELLIGENT MOVING OBJECTS DATABASE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xu, Jianqiu and Lu, Hua and G\"{u}ting, Ralf Hartmut</field>
  <field name="keywords">nan</field>
  <field name="abstract">The research field of moving objects has been quite active in the past 20 years. The recording of position data becomes easy and huge amounts of mobile data are collected. Moving objects databases represent time-dependent objects and support queries with spatial and temporal constraints. In this paper we provide the vision of a multi-model and intelligent moving objects database. The goal is to enhance the data management of moving objects by providing extensive data models for different applications and fusing artificial intelligence techniques. Toward this goal, we propose how to develop corresponding modules and integrate them into the system to achieve the next-generation moving objects database.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3340964.3340975</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEBE FEEDBACK FOR LARGE LECTURE CLASSROOM ANALYTICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mitra, Ritayan and Chavan, Pankaj</field>
  <field name="keywords">Large lectures, quantified self, learning analytics, live feedback, mobile application</field>
  <field name="abstract">Learning Analytics (LA) research has demonstrated the potential of LA in detecting and monitoring cognitive-affective parameters and improving student success. But most of it has been applied to online and computerized learning environments whereas physical classrooms have largely remained outside the scope of such research. This paper attempts to bridge that gap by proposing a student feedback model in which they report on the difficult/easy and engaging/boring aspects of their lecture. We outline the pedagogical affordances of an aggregated time-series of such data and discuss it within the context of LA research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3303772.3303821</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DIGITAL TRANSFORMATION IN THE INDIAN GOVERNMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Verma, Neeta and Dawar, Savita</field>
  <field name="keywords">nan</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3349629</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PRESENTING A DATA IMPUTATION CONCEPT TO SUPPORT THE CONTINUOUS ASSESSMENT OF HUMAN VITAL DATA AND ACTIVITIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Haescher, Marian and Matthies, Denys J. C. and Krause, Silvio and Bieber, Gerald</field>
  <field name="keywords">controlled data creation, data fusion, data imputation, sensor fusion, mobile device, accelerometer, coherent database, smartwatch</field>
  <field name="abstract">Data acquisition of mobile tracking devices often suffers from invalid and non-continuous input data streams. This issue especially occurs with current wearables tracking the user's activity and vital data. Typical reasons include the short battery life and the fact that the body-worn tracking device may be doffed. Other reasons, such as technical issues, can corrupt the data and render it unusable. In this paper, we introduce a data imputation concept which complements and thus fixes incomplete datasets by using a new merging approach that is particularly suitable for assessing activities and vital data. Our technique enables the dataset to become coherent and comprehensive so that it is ready for further analysis. In contrast to previous approaches, our technique enables the controlled creation of continuous data sets that also contain information on the level of uncertainty for possible reconversions, approximations, or later analysis.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3316782.3322785</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ENTERPRISE KNOWLEDGE GRAPH FROM SPECIFIC BUSINESS TASK TO ENTERPRISE KNOWLEDGE MANAGEMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Duan, Rong and Xiao, Yanghua</field>
  <field name="keywords">enterprise knowledge management, entity recognition, ontology, relation extraction, knowledge graph</field>
  <field name="abstract">Data driven Knowledge Graph is rapidly adapted by different societies. Many open domain and specific domain knowledge graphs have been constructed, and many industries have benefited from knowledge graph. Currently, enterprise related knowledge graph is classified as specific domain, but the applications span from solving a narrow specific problem to Enterprise Knowledge Management system. With the digital transform of traditional industry, Enterprise knowledge becomes more and more complicated, it involves knowledge from common domain, multiple specific domains, and corporate-specific in general. This tutorial provides an overview of current Enterprise Knowledge Graph(EKG). It distinguishes the EKG from specific domain according to the knowledge it covers, and provides the examples to illustrate the difference between EKG and specific domain KG. The tutorial further summarizes EKG into three types: Specific Business Task Enterprise KG, Specific Business Unit Enterprise KG and Cross Business Unit Enterprise KG, and illustrates the characteristics, steps, challenges, and future research in constructing and consuming of each of these three types of EKG .</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3357384.3360314</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">GOVERNMENT MANAGEMENT MODEL OF NON-PROFIT ORGANIZATIONS BASED ON E-GOVERNMENT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lin, Yuting</field>
  <field name="keywords">E-government, management model, non-profit organization</field>
  <field name="abstract">With the development and popularization of Internet technology, our country is increasingly aware of the importance of e-government, and continuously expands the channels and means of e-government development in policy, such as the application of e-government to the management of non-profit organizations. However, in practice, "e-government + NPO (non-profit organization) management" still has problems such as digital divide, information sharing and insufficient disclosure, and information security. Therefore, this paper proposes a more complete non-profit organization management model based on e-government. From the perspectives of optimization services, information sharing, network supervision and information security, it is explained how to effectively realize the efficient management of non-profit organizations based on e-government.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3348445.3348464</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WHAT IS THE ROLE OF NEW GENERATION OF ICTS IN TRANSFORMING GOVERNMENT OPERATION AND REDEFINING STATE-CITIZEN RELATIONSHIP IN THE LAST DECADE?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liu, Shuhua Monica and Pan, Liting and Lei, Yupei</field>
  <field name="keywords">Information and communication technology (ICT), Transformative governance, E-governance</field>
  <field name="abstract">This article first introduce a new government initiative emerging after the US presidential election in 2008. Comparing to the more descriptive definitions of e-government, supporters of these new government initiatives emphasize the transformative and normative aspect of the newest generation of Information and Communication Technology (ICTs). They argue that the new initiative redefines how government should operate and transform state-citizen relationships. To understand the core of this initiative and whether it offers new opportunities to solve public problems, we collected and analyzed research papers published in the e-governance area between 2008 and 2017. Our analysis demonstrates that the use of new generation of ICTs has promoted the government information infrastructure. In other words, the application of new ICTs enables the government to accumulate and use a large amount of data, so that the government makes better decisions. The advancement of open data, the wide use of social media, and the potential of data analytics have also generated pressure to address challenging questions and issues in e-democracy. However, the analysis leads us to deliberate on whether the use of new generation of ICTs worldwide have actually achieved their goal. In the conclusion, we present challenges to be addressed before new innovative ICTs realize their potential towards better public governance.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3326365.3326374</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PANEL: THE COMPUTING IN DATA SCIENCE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Cassel, Lillian and Hongzhi, Wang</field>
  <field name="keywords">computing curriculum, computing for data science, data science</field>
  <field name="abstract">This panel brings the workings and results of the ACM Education Council Task Force on Data Science Education. The task force has gathered information on existing programs and has reviewed documents such as the result of the National Academies deliberations on data science. The task force is charged with exploring the role of computer science in data science education, understanding that data science is an inherently interdisciplinary field and not exclusively a computer science field. The panel will present a summary of the task force findings by two members of the task force and perspectives from leaders in data-intensive applications from China. The goal of the panel is to present the findings, but also to obtain perspectives from the attendees in order to enrich the task force's work.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3300115.3312508</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA SCIENCE COMPETENCY IN ORGANISATIONS: A SYSTEMATIC REVIEW AND UNIFIED MODEL</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Hattingh, Mari\'{e} and Marshall, Linda and Holmner, Marlene and Naidoo, Rennie</field>
  <field name="keywords">Data Science, Systematic Literature Review, Competency, Skills</field>
  <field name="abstract">The paper presents a systematic literature review of the literature on the competencies that are essential to develop a globally competitive workforce in the field of data science. The systematic review covers a wide range of literature but focuses primarily, but not exclusively, on the computing, information systems, management, and organisation science literature. The paper uses a broad research search strategy covering four separate electronic databases. The search strategy led the researchers to scan 139 titles, abstracts and keywords. Sixty potentially relevant articles were identified, of which 42 met the quality criteria and contributed to the analysis. A critical appraisal checklist assessed the validity of each empirical study. The researchers grouped the findings under six broad competency themes: organisational, technical, analytical, ethical and regulatory, cognitive and social. Thematic analysis was used to develop a unified model of data science competency based on the evidence of the findings. This model will be applied to case studies and survey research in future studies. A unified data science competency model, supported by empirical evidence, is crucial in closing the skills gap, thereby improving the quality and competitiveness of the South Africa's data science workforce. Researchers are encouraged to contribute to the further conceptual development of data science competency.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3351108.3351110</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A RECENT SURVEY ON CHALLENGES IN SECURITY AND PRIVACY IN INTERNET OF THINGS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Aljawarneh, Shadi and Radhakrishna, Vangipuram and Kumar, Gunupudi Rajesh</field>
  <field name="keywords">challenges in IoT, research issues, S/W weakness, IoT classification, IoT services, IoT architecture, security and privacy, vulnerability</field>
  <field name="abstract">Computing environment in IoT (Internet of Things) is surrounded with huge amounts of heterogeneous data fulfilling many services in everyone's daily life. Since, communication process in IoT takes place using different devices such as smart phones, sensors, mobile devices, household devices, embedded equipment etc. With the use of these variety of devices, the exchange of data in open internet environment is prone to vulnerabilities. The main cause for these vulnerabilities is the weaknesses in the design of software components and hardware components. Bridging communications gaps in the IoT is a complex process as the data is from heterogeneous sources. An effort is made in this paper to discuss various challenges that are being faced in security and privacy of data. This will be very much helpful for researchers who want to pursue research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3330431.3330457</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA LAKE MANAGEMENT: CHALLENGES AND OPPORTUNITIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Nargesian, Fatemeh and Zhu, Erkang and Miller, Ren\'{e}e J. and Pu, Ken Q. and Arocena, Patricia C.</field>
  <field name="keywords">nan</field>
  <field name="abstract">The ubiquity of data lakes has created fascinating new challenges for data management research. In this tutorial, we review the state-of-the-art in data management for data lakes. We consider how data lakes are introducing new problems including dataset discovery and how they are changing the requirements for classic problems including data extraction, data cleaning, data integration, data versioning, and metadata management.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.14778/3352063.3352116</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE DIGITALIZATION OF THE SUPPLY CHAIN MANAGEMENT OF SERVICE COMPANIES: A PROSPECTIVE APPROACH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Bentalha, Badr and Hmioui, Aziz and Alla, Lhoussaine</field>
  <field name="keywords">prospective approach, service company, supply chain, SCM, digital</field>
  <field name="abstract">Supply Chain Management (SCM) was born and developed first in an industrial context. In the field of services, little research has addressed the issue of the company's SCM. According to [1] "service logistics is an approach that stabilizes and guarantees the continuity of flows: it is then oriented more towards the service provided than towards reducing traffic costs". The SCM of services is of increasing interest to companies facing strong competition, market globalization and rapid changes in information and communication technologies. This evolution has led to a rapid integration of new digital practices in this field.So, how is the digitalization of the SCM of service companies looking today and what will be the future trends? On the one hand, with the help of the literature review, we seek to identify the concept of the SCM in services and its specificities, then that of digitization of the SCM and its organizational dimension. On the other hand, we are attempting a prospective approach to the current practices and digitalization prospects of the service company's SCM.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3368756.3369005</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">WORKSHOP SUMMARY: 2019 IEEE / ACM FOURTH INTERNATIONAL WORKSHOP ON METAMORPHIC TESTING (MET 2019)</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Xie, Xiaoyuan and Poon, Pak-Lok and Pullum, Laura L.</field>
  <field name="keywords">software verification and validation, metamorphic testing, software engineering, software testing</field>
  <field name="abstract">MET is a relatively new workshop on metamorphic testing for academic researchers and industry practitioners. The first international workshop on MET (MET 2016) was co-located with the 38th International Conference on Software Engineering (ICSE 2016) in Austin TX, USA on May 16, 2016. Since then the workshop has become an annual event. This paper reports on the fourth International Workshop on Metamorphic Testing (MET 2019) held in Montr\'{e}al, Canada on May 26, 2019, as part of the 41st International Conference on Software Engineering (ICSE 2019). We first outline the aims of the workshop, followed by a discussion of its keynote speech and technical program.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3356773.3356810</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AN IN-DEPTH LOOK OF BFT CONSENSUS IN BLOCKCHAIN: CHALLENGES AND OPPORTUNITIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad</field>
  <field name="keywords">nan</field>
  <field name="abstract">Since the introduction of Bitcoin---the first wide-spread application driven by blockchains---the interest of the public and private sector in blockchains has skyrocketed. At the core of this interest are the ways in which blockchains can be used to improve data management, e.g., by enabling federated data management via decentralization, resilience against failure and malicious actors via replication and consensus, and strong data provenance via a secured immutable ledger.In practice, high-performance blockchains for data management are usually built in permissioned environments in which the participants are vetted and can be identified. In this setting, blockchains are typically powered by Byzantine fault-tolerant consensus protocols. These consensus protocols are used to provide full replication among all honest blockchain participants by enforcing an unique order of processing incoming requests among the participants.In this tutorial, we take an in-depth look at Byzantine fault-tolerant consensus. First, we take a look at the theory behind replicated computing and consensus. Then, we delve into how common consensus protocols operate. Finally, we take a look at current developments and briefly look at our vision moving forward.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3366625.3369437</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SCORING MATRIX COMBINED WITH MACHINE LEARNING FOR HETEROGENEOUSLY STRUCTURED ENTITY RESOLUTION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Li, Xinming and Talburt, John R. and Li, Ting and Liu, Xiangwen</field>
  <field name="keywords">nan</field>
  <field name="abstract">This paper describes how machine learning works with "coring matrix", which is designed for measuring the similarity between heterogeneously structured references, to get a better performance in Entity Resolution (ER). In the scoring matrix, each entity reference is tokenized and all pairs of tokens between the references are scored by a similarity scoring function such as the Levenshtein edit distance. In so doing, a similarity score vector can measure the similarity between references. With the similarity score vector, machine learning is used to make the linking decision. Our experiments show that machine learning based on score vector outperforms TF-IDF and FuzzyWuzzy benchmarks. One possible explanation is that a similarity score vector conveys much more information than a single similarity score. Random forest and neural network even get better performance with raw score vector input than with the statistic characteristic input.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SHOULD RESEARCHERS USE DATA FROM SECURITY BREACHES?</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Douglas, David M.</field>
  <field name="keywords">nan</field>
  <field name="abstract">Evaluating the arguments for and against using digital data derived from security breaches.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3368091</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AIOPS: REAL-WORLD CHALLENGES AND RESEARCH INNOVATIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dang, Yingnong and Lin, Qingwei and Huang, Peng</field>
  <field name="keywords">AIOps, software analytics, DevOps</field>
  <field name="abstract">AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and applications at scale with artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help improve service quality and customer satisfaction, boost engineering productivity, and reduce operational cost. In this technical briefing, we first summarize the real-world challenges in building AIOps solutions based on our practice and experience in Microsoft. We then propose a roadmap of AIOps related research directions, and share a few successful AIOps solutions we have built for Microsoft service products.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/ICSE-Companion.2019.00023</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">UNLOCKING DATA TO IMPROVE PUBLIC POLICY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Hastings, Justine S. and Howison, Mark and Lawless, Ted and Ucles, John and White, Preston</field>
  <field name="keywords">nan</field>
  <field name="abstract">When properly secured, anonymized, and optimized for research, administrative data can be put to work to help government programs better serve those in need.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3335150</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">MICHAEL FRANKLIN SPEAKS OUT ON DATA SCIENCE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Winslett, Marianne and Braganholo, Vanessa</field>
  <field name="keywords">nan</field>
  <field name="abstract">Welcome to ACM SIGMOD Record series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we're at the 2017 SIGMOD and PODS conference in Chicago. I have here with me Mike Franklin, who is the chair of the Computer Science department at the University of Chicago. Before that, for many years, Mike was a professor at Berkeley where he also served as a chair of the Computer Science division. Mike was a co-founder and director of the Algorithms, Machines, and People Lab, better known as the AMPLab. He is an ACM fellow, a two-time winner of the SIGMOD Ten Year Test of Time Award, and a founder of the successful startup, Truviso. Mike's Ph.D. is from the University of Wisconsin Madison. So, Mike, welcome!</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3377391.3377398</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">COMPREHENSIVE DATA MANAGEMENT AND ANALYTICS FOR GENERAL SOCIETY SURVEY DATASET</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Pan, Zhiwen and Zhao, Shuangye and Pacheco, Jesus and Zhang, Yuxin and Song, Xiaofan and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun</field>
  <field name="keywords">Data management, Decision support systems, Society survey, Data mining, Knowledge discovery</field>
  <field name="abstract">The General Society Survey(GSS) is a kind of government-funded survey which aims at examining the Socio-economic status, quality of life, and structure of contemporary society. GSS dataset is regarded as one of the authoritative source for the government and organization practitioners to make data-driven policies. The previous analytic approaches for GSS dataset are designed by combining expert knowledges and simple statistics. In this paper, we proposed a comprehensive data management and data mining approach for GSS datasets. The approach is designed to be operated in a two-phase manner: a data management phase which can improve the quality of GSS data by performing attribute preprocessing and filter-based attribute selection; a data mining phase which can extract hidden knowledges from the dataset by performing data mining analysis including prediction analysis, classification analysis, association analysis and clustering analysis. By leveraging the power of data mining techniques, our proposed approach can explore knowledges in a fine-grained manner with minimum human interference. Experiments on Chinese General Social Survey dataset are conducted at the end to evaluate the performance of our approach.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3371238.3371269</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DNA SEQUENCING TECHNOLOGIES: SEQUENCING DATA PROTOCOLS AND BIOINFORMATICS TOOLS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin, Qiuzhen and Kwong, Sam and Liang, Cheng</field>
  <field name="keywords">bioinformatics, DNA sequencing, tools, third-generation sequencing (TGS), technology, software, data protocols, history, computational biology</field>
  <field name="abstract">The recent advances in DNA sequencing technology, from first-generation sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed the genome research landscape. Its data throughput is unprecedented and severalfold as compared with past technologies. DNA sequencing technologies generate sequencing data that are big, sparse, and heterogeneous. This results in the rapid development of various data protocols and bioinformatics tools for handling sequencing data.In this review, a historical snapshot of DNA sequencing is taken with an emphasis on data manipulation and tools. The technological history of DNA sequencing is described and reviewed in thorough detail. To manipulate the sequencing data generated, different data protocols are introduced and reviewed. In particular, data compression methods are highlighted and discussed to provide readers a practical perspective in the real-world setting. A large variety of bioinformatics tools are also reviewed to help readers extract the most from their sequencing data in different aspects, such as sequencing quality control, genomic visualization, single-nucleotide variant calling, INDEL calling, structural variation calling, and integrative analysis. Toward the end of the article, we critically discuss the existing DNA sequencing technologies for their pitfalls and potential solutions.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3340286</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PICTURE MANAGEMENT OF POWER SUPPLY SAFETY MANAGEMENT SYSTEM BASED ON DEEP LEARNING TECHNOLOGY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yuwei, Sun and Jianbao, Zhu and Qingshan, Ma and Xinchun, Yu and Ye, Shi and Yu, Chen</field>
  <field name="keywords">deep learning, convolutional neural network, Picture management</field>
  <field name="abstract">With the advent of the era of big data, power supply security management systems will get a lot of picture data. In the face of massive image data, this paper studies the image management technology based on convolutional neural network. Aiming at the high repetition rate of self built image database samples and the problem that many sample classes contain uncorrelated images, two algorithms are proposed to improve the quality of the database: de duplication and de uncorrelation. By using the depth convolution neural network, the Embedding represented by the corresponding image is taken, and the distance between Embedding is calculated in the Euclidean space to achieve the purpose of de duplication and de uncorrelation. In this paper, "time" and "accuracy" are used to evaluate the performance of de duplication and de uncorrelation algorithms. The comparison examples of some sample classes before and after removing repetition and before and after removing uncorrelation are shown. The Recall-value of the database after removing duplicate and uncorrelated is tested based on the GoogLe Netplus-model respectively, which proves the effectiveness of the two filtering algorithms and overcomes the complexity of the traditional filtering process.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3377458.3377464</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">UNDERSTANDING LAW ENFORCEMENT STRATEGIES AND NEEDS FOR COMBATING HUMAN TRAFFICKING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy</field>
  <field name="keywords">law enforcement, human trafficking, qualitative, needs analysis</field>
  <field name="abstract">In working to rescue victims of human trafficking, law enforcement officers face a host of challenges. Working in complex, layered organizational structures, they face challenges of collaboration and communication. Online information is central to every phase of a human-trafficking investigation. With terabytes of available data such as sex work ads, policing is increasingly a big-data research problem. In this study, we interview sixteen law enforcement officers working to rescue victims of human trafficking to try to understand their computational needs. We highlight three major areas where future work in human-computer interaction can help. First, combating human trafficking requires advances in information visualization of large, complex, geospatial data, as victims are frequently forcibly moved across jurisdictions. Second, the need for unified information databases raises critical research issues of usable security and privacy. Finally, the archaic nature of information systems available to law enforcement raises policy issues regarding resource allocation for software development.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3290605.3300561</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEEPMM: DEEP LEARNING BASED MAP MATCHING WITH DATA AUGMENTATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhao, Kai and Feng, Jie and Xu, Zhao and Xia, Tong and Chen, Lin and Sun, Funing and Guo, Diansheng and Jin, Depeng and Li, Yong</field>
  <field name="keywords">map matching, deep learning, data driven system</field>
  <field name="abstract">Map matching is important in many trajectory based applications like route optimization and traffic schedule, etc. As the widely used methods, Hidden Markov Model and its variants are well studied to provide accurate and efficient map matching service. However, HMM based methods fail to utilize the value of enormous trajectory big data, which are useful for the map matching task. Furthermore, with many following-up works, they are still easily influenced by the noisy records, which are very common in the real system. To solve these problems, we revisit the map matching task from the data perspective, and propose to utilize the great power of data to help solve these problems. We build a deep learning based model to utilize all the trajectory data for joint training and knowledge sharing. With the help of embedding techniques and sequence learning model with attention enhancement, our system does the map matching in the latent space, which is tolerant to the noise in the physical space. Extensive experiments demonstrate that our model outperforms the widely used HMM based methods more than 10% (absolute accuracy) and works robustly in the noisy settings in the meantime.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3347146.3359090</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RECOGNIZING EXPERTS ON SOCIAL MEDIA: A HEURISTICS-BASED APPROACH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel</field>
  <field name="keywords">social media, expertise location, data analytics.</field>
  <field name="abstract">Knowing who is an expert on social media is a challenging yet important task, especially in a world where misleading information is commonplace and where social media is an important information source for knowledge seekers. In this paper we investigate expertise heuristics by comparing features of experts versus non-experts in big data settings. We employ a large set of features to classify experts and non-experts using data collected on two social media platform (Twitter and reddit). Our results show a good ability to predict who is an expert, especially using language-based features, validating that heuristics can be developed to differentiate experts from novices organically, based on social media use. Our results contribute to the development of expertise location and identification systems as well as our understanding on how experts present themselves on social media.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3353401.3353406</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">POLYFLOW: A SOA FOR ANALYZING WORKFLOW HETEROGENEOUS PROVENANCE DATA IN DISTRIBUTED ENVIRONMENTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel</field>
  <field name="keywords">polystore, heterogeneous provenance data integration, Workflows interoperability</field>
  <field name="abstract">In the last decade the (big) data-driven science paradigm became a wide-spread reality. However, this approach has some limitations such as a performance dependency on the quality of the data and the lack of reproducibility of the results. In order to enable this reproducibility, many tools such as Workflow Management Systems were developed to formalize process pipelines and capture execution traces. However, interoperating data generated by these solutions became a problem, since most systems adopted proprietary data models. To support interoperability across heterogeneous provenance data, we propose a Service Oriented Architecture with a polystore storage design in which provenance is conceptually represented utilizing the ProvONE model. A wrapper layer is responsible for transforming data described by heterogeneous formats into ProvONE-compliant. Moreover, we propose a query layer that provides location and access transparency to users. Furthermore, we conduct two feasibility studies, showcasing real usecase scenarios. Firstly, we illustrate how two research groups can compare their processes and results. Secondly, we show how our architecture can be used as a queriable provenance repository. We show Polyflow's viability for both scenarios using the Goal-Question-Metric methodology. Finally, we show our solution usability and extensibility appeal by comparing it to similar approaches.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3330204.3330259</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SAFETY MONITORING OF POWER INDUSTRIAL CONTROL TERMINALS BASED ON DATA CLEANING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lv, Zhining and Hu, Ziheng and Ning, Baifeng and Li, Wei and Yan, Gangfeng and Ding, Lifu and Shi, Xiasheng and Guo, Ningxuan</field>
  <field name="keywords">Chebyshev theory, Power monitoring, Data cleaning, Proximity data averaging</field>
  <field name="abstract">Stable and high-quality electric energy is the main driving force for the development of social science, technology, and the national economic leap. The assessment and monitoring of electrical safety rely on the generation, collection and statistics of large amounts of data by the power system. For the possible problems and impurities in these data, this paper uses the 'local Chebyshev theorem' and the 'near data averaging method' for the attribute values. The error is cleaned, and the 'sorting neighbor algorithm' is used to clean the duplicate data, thereby improving the data quality and realizing the accuracy of the safety monitoring of the power grid of the smart grid.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3357777.3357781</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">APPLICATION OF ATTRIBUTE CORRELATION IN UNSUPERVISED DATA CLEANING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Li, Pei and Dai, Chaofan and Wang, Wenqian</field>
  <field name="keywords">Unsupervised data cleaning, minimum repair cost, weak logic errors, attribute correlation, machine learning</field>
  <field name="abstract">Referring to the supervised learning and unsupervised learning in machine learning, we divide the data cleaning processes into supervised and unsupervised two forms too, and then, we reclassify the data quality problems into canonicalization error, redundancy error, strong logic error and weak logic error according to the characteristics of unsupervised cleaning. For the weak logic errors, we propose a repair framework AC-Framework and an algorithm AC-Repair based on the attribute correlation. When repairing, we first establish a priority queue(PQ) for elements to be repaired according to the minimum cost idea and take the corresponding conflict-free data set(Icf) as a training set to learn the correlation among attributes. Then, we select the first element in PQ list as the candidate element to repair, and recompute the PQ list after one repair round to improve the efficiency. Finally, in order to prevent the algorithm from endless loops, we set a label flag to mark the repaired elements, in this way, every error element will be repaired at most once. In the experimental part, we compare the AC-Repair algorithm with the interpolation-based repair algorithm to verify its validity.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3312714.3312717</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DEDUCING CERTAIN FIXES TO GRAPHS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Fan, Wenfei and Lu, Ping and Tian, Chao and Zhou, Jingren</field>
  <field name="keywords">nan</field>
  <field name="abstract">This paper proposes to deduce certain fixes to graphs G based on data quality rules Σ and ground truth Γ (i.e., validated attribute values and entity matches). We fix errors detected by Σ in G such that the fixes are assured correct as long as Σand Γ are correct. We deduce certain fixes in two paradigms. (a) We interact with users and "incrementally" fix errors online. Whenever users pick a small set V0 of nodes in G, we fix all errors pertaining to V0 and accumulate ground truth in the process. (b) Based on accumulated Γ, we repair the entire graph G offline; while this may not correct all errors in G, all fixes are guaranteed certain.We develop techniques for deducing certain fixes. (1) We define data quality rules to support conditional functional dependencies, recursively defined keys and negative rules on graphs, such that we can deduce fixes by combining data repairing and object identification. (2) We show that deducing certain fixes is Church-Rosser, i.e., the deduction converges at the same fixes regardless of the order of rules applied. (3) We establish the complexity of three fundamental problems associated with certain fixes. (4) We provide (parallel) algorithms for deducing certain fixes online and offline, and guarantee to reduce running time when given more processors. Using real-life and synthetic data, we experimentally verify the effectiveness and scalability of our methods.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.14778/3317315.3317318</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">GENERATING SYNTHETIC DATA TO SUPPORT ENTITY RESOLUTION EDUCATION AND RESEARCH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ye, Yumeng and Talburt, John R.</field>
  <field name="keywords">nan</field>
  <field name="abstract">Almost all organizations use some type of Entity Resolution (ER) methods to uniquely identify their customers and vendors across different channels of contact. In the case of persons, this requires the use of personally identifying information (PII) such as name, address, phone number, and email address. Because of the growing concerns over data privacy and identity theft, organizations are reluctant to release personally-identifiable customer information even for education and research purposes. An alternative is to generate synthetic data to use in student exercises and for research related to entity resolution methods and techniques. One advantage of synthetically generated data for ER is it can be fully annotated with the correct linking making it very easy to calculate the precision and recall of linking operations. This paper discusses a simple method to generate synthetic data as input for ER processes. The method allows the user to randomly assign certain types and levels of data quality errors along with other types of non-error variations to the data, such as nicknames, different date formats, and changes in address. For ER research in particular, the method can create introduce data redundancy by copying records referencing the same person into the same file or into different files with different record layouts.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA CLEANING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ilyas, Ihab F. and Chu, Xu</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EFFICIENT USER GUIDANCE FOR VALIDATING PARTICIPATORY SENSING DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong and Stantic, Bela and Hung, Nguyen Quoc Viet</field>
  <field name="keywords">trust management, probabilistic database, Participatory sensing</field>
  <field name="abstract">Participatory sensing has become a new data collection paradigm that leverages the wisdom of the crowd for big data applications without spending cost to buy dedicated sensors. It collects data from human sensors by using their own devices such as cell phone accelerometers, cameras, and GPS devices. This benefit comes with a drawback: human sensors are arbitrary and inherently uncertain due to the lack of quality guarantee. Moreover, participatory sensing data are time series that exhibit not only highly irregular dependencies on time but also high variance between sensors. To overcome these limitations, we formulate the problem of validating uncertain time series collected by participatory sensors. In this article, we approach the problem by an iterative validation process on top of a probabilistic time series model. First, we generate a series of probability distributions from raw data by tailoring a state-of-the-art dynamical model, namely &lt;u&gt;G&lt;/u&gt;eneralised &lt;u&gt;A&lt;/u&gt;uto &lt;u&gt;R&lt;/u&gt;egressive &lt;u&gt;C&lt;/u&gt;onditional &lt;u&gt;H&lt;/u&gt;eteroskedasticity (GARCH), for our joint time series setting. Second, we design a feedback process that consists of an adaptive aggregation model to unify the joint probabilistic time series and an efficient user guidance model to validate aggregated data with minimal effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness of our approach on both real data and synthetic data. Highlights from our experiences include the fast running time of a probabilistic model, the robustness of an aggregation model to outliers, and the significant effort saving of a guidance model.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3326164</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">USING A NOVEL NEGATIVE SELECTION INSPIRED ANOMALY DETECTION ALGORITHM TO IDENTIFY CORRUPTED RIBO-SEQ AND RNA-SEQ SAMPLES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Perkins, Patrick and Heber, Steffen</field>
  <field name="keywords">negative selection algorithm, rna-seq, ribosome profiling, sample quality, machine learning, anomaly detection</field>
  <field name="abstract">RNA-seq and Ribo-seq are popular techniques for quantifying cellular transcription and translation. These experiments use next-generation sequencing to produce genome-wide high-resolution snapshots of the total populations of mRNAs and translating ribosomes within the investigated samples. When performed in concert, these experiments yield valuable information about protein synthesis rates and translational efficiency. Due to their intricate experimental protocols and demanding data processing requirements, quality control and analysis of such experiments are often challenging. Therefore, methods for accurately assessing data quality, and for identifying contaminated samples, are greatly needed. In the following we use a novel negative selection inspired algorithm called Boundary Detection Using Nearest Neighbors (BDUNN), for the identification of corrupted samples. Our algorithm constructs a detector set and reduced training set that defines the boundaries between normal data points and potential anomalies. Subsequently, a nearest neighbor algorithm is used to classify unseen observations. We compare the performance of BDUNN with other popular negative selection and one-class classification algorithms, and show that BDUNN is capable of accurately and efficiently detecting anomalies in standard anomaly detection datasets and simulated RNA-seq and Ribo-seq data sets. Furthermore, we have implemented our method within an existing R Shiny platform for analyzing RNA-seq an Ribo-seq datasets, which permits downstream analysis of anomalous samples.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3307339.3342169</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EVALUATING MODEL-FREE DIRECTIONAL DEPENDENCY METHODS ON SINGLE-CELL RNA SEQUENCING DATA WITH SEVERE DROPOUT</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dvo\v{r}\'{a}kov\'{a}, Eli\v{s}ka and Kumar, Sajal and Kl\'{e}ma, Ji\v{r}\'{\i} and \v{Z}elezn\'{y}, Filip and Drbal, Karel and Song, Mingzhou</field>
  <field name="keywords">model-free, directional dependency, single-cell sequencing</field>
  <field name="abstract">As severe dropout in single-cell RNA sequencing (scRNA-seq) degrades data quality, current methods for network inference face increased uncertainty from such data. To examine how dropout influences directional dependency inference from scRNA-seq data, we thus studied four methods based on discrete data that are model-free without parametric model assumptions. They include two established methods: conditional entropy and Kruskal-Wallis test, and two recent methods: causal inference by stochastic complexity and function index. We also included three non-directional methods for a contrast. On simulated data, function index performed most favorably at varying dropout rates, sample sizes, and discrete levels. On an scRNA-seq dataset from developing mouse cerebella, function index and Kruskal-Wallis test performed favorably over other methods in detecting expression of developmental genes as a function of time. Overall among the four methods, function index is most resistant to dropout for both directional and dependency inference. The next best choice, Kruskal-Wallis test, carries a directional bias towards a uniformly distributed variable. We conclude that a method robust to marginal distributions with a sufficiently large sample size can reap benefits of single-cell over bulk RNA sequencing in understanding molecular mechanisms at the cellular resolution.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3383783.3383793</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">OPEN GOVERNMENT DATA: TOWARDS A COMPARISON OF DATA LIFECYCLE MODELS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Elmekki, Hanae and Chiadmi, Dalila and Lamharhar, Hind</field>
  <field name="keywords">Open Government Data, data lifecycle, data value creation</field>
  <field name="abstract">Government, through Open Government Data "OGD, becomes one of the important producers of open data. OGD is an opportunity to create valuable services and innovative products useful for citizens as a primarily targeted consumer. However, the expected benefits of OGD are not yet met. That is to say, several research communities' studies insist on the necessity of creating valuable data in order to generate valuable services. These studies are still insufficient for a shared understanding of how OGD contribute to the creation of value. For this purpose, this paper presents a review of a set of data lifecycle models compared against their contribution to the creation of value in the context of OGD.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3333165.3333180</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">STUDY OF SMART CITY DATA: CATEGORIES AND QUALITY CHALLENGES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Rhazal, Oumaima El and Tomader, Mazri</field>
  <field name="keywords">internet of things (IoT), quality of information (QoI), smart city</field>
  <field name="abstract">Lately, the world attention is directed to transforming daily life to a smarter one, we cannot deny the smart city concept that became pervading. This concept will give every device the chance to communicate with other devices, it will simply create the smarter version of everything. However, data heterogeneity and quality changes are one of the best priorities and challenges that should be handled in this promising concept.In this paper we present a review about data categories circulating in a smart city depending on its required services. We also study the quality of information as one of both, major challenges and treasures in a smart city.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3368756.3368965</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">TOWARDS RELIABLE DATA COLLECTION AND ANNOTATION TO EXTRACT PULMONARY DIGITAL BIOMARKERS USING MOBILE SENSORS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Rahman, Md Mahbubur and Nathan, Viswam and Nemati, Ebrahim and Vatanparvar, Korosh and Ahmed, Mohsin and Kuang, Jilong</field>
  <field name="keywords">Digital Biomarkers, Breathlessness, Breathing, Data Quality, Cough, mHealth, Crowdsourced Annotation</field>
  <field name="abstract">Proliferation of sensors embedded in smartphones and smartwatches helps capture rich dataset for machine learning algorithms to extract meaningful digital bio-markers on consumer devices for monitoring disease progression and treatment response. However, development and validation of machine learning algorithms depend on gathering high fidelity sensor data and reliable ground-truth. We conduct a study, called mLungStudy, with 131 subjects with varying pulmonary conditions to collect mobile sensor data including audio, accelerometer, gyroscope using a smartphone and a smartwatch, in order to extract pulmonary biomarkers such as breathing, coughs, spirometry, and breathlessness. Our study shows that commonly used breathing ground-truth data from chestband may not always be reliable as a gold-standard. Our analysis shows that breathlessness biomarkers such as pause time and pause frequency from 2.15 minutes of audio can be as reliable as those extracted from 5 minutes' worth of speech data. This finding can be useful for future studies to trade-off between the reliability of breathlessness data and patient comfort in generating continuous speech data. Furthermore, we use crowdsourcing techniques to annotate pulmonary sound events for developing signal processing and machine learning algorithms. In this paper, we highlight several practical challenges to collect and annotate physiological data and acoustic symptoms from chronic pulmonary patients and ways to improve data quality. We show that the waveform visualization of the audio signal improves annotation quality which leads to a 6.59% increase in cough classification accuracy and a 6% increase in spirometry event classification accuracy. Findings from this study inform future studies focusing on developing explainable machine learning models to extract pulmonary digital bio-markers using mobile sensors.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3329189.3329204</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SONET: A SEMANTIC ONTOLOGICAL NETWORK GRAPH FOR MANAGING POINTS OF INTEREST DATA HETEROGENEITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Palumbo, Rachel and Thompson, Laura and Thakur, Gautam</field>
  <field name="keywords">points of interest, openstreetmap, ontology, graph database, big data</field>
  <field name="abstract">Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3356991.3365474</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">LARGE-SCALE SEMANTIC INTEGRATION OF LINKED DATA: A SURVEY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mountantonakis, Michalis and Tzitzikas, Yannis</field>
  <field name="keywords">Data integration, RDF, big data, data discovery, semantic web</field>
  <field name="abstract">A large number of published datasets (or sources) that follow Linked Data principles is currently available and this number grows rapidly. However, the major target of Linked Data, i.e., linking and integration, is not easy to achieve. In general, information integration is difficult, because (a) datasets are produced, kept, or managed by different organizations using different models, schemas, or formats, (b) the same real-world entities or relationships are referred with different URIs or names and in different natural languages,&lt;?brk?&gt;(c) datasets usually contain complementary information, (d) datasets can contain data that are erroneous, out-of-date, or conflicting, (e) datasets even about the same domain may follow different conceptualizations of the domain, (f) everything can change (e.g., schemas, data) as time passes. This article surveys the work that has been done in the area of Linked Data integration, it identifies the main actors and use cases, it analyzes and factorizes the integration process according to various dimensions, and it discusses the methods that are used in each step. Emphasis is given on methods that can be used for integrating several datasets. Based on this analysis, the article concludes with directions that are worth further research.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3345551</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">MYND: A PLATFORM FOR LARGE-SCALE NEUROSCIENTIFIC STUDIES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Hohmann, Matthias R. and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard</field>
  <field name="keywords">user-centered design, neuroscience, electrophysiology, smartphone application, wearable sensors, medical studies, big data</field>
  <field name="abstract">We present a smartphone application for at-home participation in large-scale neuroscientific studies. Our goal is to establish user-experience design as a paradigm in basic neuroscientific research to overcome the limits of current studies, especially in rare neurological disorders.The presented application guides users through the fitting procedure of the EEG headset and automatically encrypts and uploads recorded data to a remote server. User-feedback and neurophysiological data from a pilot study with eighteen subjects indicate that the application can be used outside of a laboratory, without the need for external guidance. We hope to inspire future work on the intersection between basic neuroscience and human-computer interaction as a promising paradigm to accelerate research on rare neurological diseases and assistive neurotechnology.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3290607.3313002</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ROBUST LOG-BASED ANOMALY DETECTION ON UNSTABLE LOG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei</field>
  <field name="keywords">Log Analysis, Anomaly Detection, Deep Learning, Data Quality, Log Instability</field>
  <field name="abstract">Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3338906.3338931</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A REAL-WORLD DISTRIBUTED INFRASTRUCTURE FOR PROCESSING FINANCIAL DATA AT SCALE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Frischbier, Sebastian and Paic, Mario and Echler, Alexander and Roth, Christian</field>
  <field name="keywords">publish/subscribe, stream-processing, infrastructure, quality of information, financial data, broker network, big data, Event-processing</field>
  <field name="abstract">Financial markets are event- and data-driven to an extremely high degree. For making decisions and triggering actions stakeholders require notifications about significant events and reliable background information that meet their individual requirements in terms of timeliness, accuracy, and completeness. As one of Europe's leading providers of financial data and regulatory solutions vwd: processes an average of 18 billion event notifications from 500+ data sources for 30 million symbols per day. Our large-scale distributed event-based systems handle daily peak rates of 1+ million event notifications per second and additional load generated by singular pivotal events with global impact.In this poster we give practical insights into our IT systems. We outline the infrastructure we operate and the event-driven architecture we apply at vwd. In particular we showcase the (geo)distributed publish/subscribe broker network we operate across locations and countries to provide market data to our customers with varying quality of information (QoI) properties.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3328905.3332513</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BARRIERS TO USING OPEN GOVERNMENT DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wieczorkowski, Jundefineddrzej</field>
  <field name="keywords">OGD, LOD, Linked Data, Central Repository for Public Information, Open Data, Big Data, Linked Open Data, E-government, Open Government Data, CRPI</field>
  <field name="abstract">The article describes the issues of Open Government Data (OGD) and problems with the use of such data. Good quality and proper publishing of OGD enable (apart from the control function) their business use. This affects the economic benefits. The author has identified the main problems of data publication based on Central Repositories for Public Information (CRPI) in Poland, the USA, the UK and Germany. The article focuses on the maturity of data formats, automated processing with Application Programming Interface (API), using the concept of Linked Open Data (LOD). The aim of the article is to identify barriers to the implementation of OGD-based solutions and to indicate recommendations to overcome these barriers. The research shows that the methods of sharing OGD differ significantly between countries despite common guidelines. The main problem is the use of unstructured data, unsuitable for the use of LOD.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3340017.3340022</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">URBAN COMPUTING LEVERAGING LOCATION-BASED SOCIAL NETWORK DATA: A SURVEY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele</field>
  <field name="keywords">urban informatics, city dynamics, urban sensing, location-based social networks, Urban computing, urban societies, big data</field>
  <field name="abstract">Urban computing is an emerging area of investigation in which researchers study cities using digital data. Location-Based Social Networks (LBSNs) generate one specific type of digital data that offers unprecedented geographic and temporal resolutions. We discuss fundamental concepts of urban computing leveraging LBSN data and present a survey of recent urban computing studies that make use of LBSN data. We also point out the opportunities and challenges that those studies open.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3301284</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">NSF BIGDATA PI MEETING - DOMAIN-SPECIFIC RESEARCH DIRECTIONS AND DATA SETS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Singh, Lisa and Deshpande, Amol and Zhou, Wenchao and Banerjee, Arindam and Bowers, Alex and Friedler, Sorelle and Jagadish, H.V. and Karypis, George and Obradovic, Zoran and Vullikanti, Anil and Zuo, Wangda</field>
  <field name="keywords">nan</field>
  <field name="abstract">In March 2017, PIs and co-PIs funded through the NSF BIGDATA program were brought together along with selected industry and government invitees to discuss current research, identify current challenges, discuss promising future directions, foster new collaborations, and share accomplishments, at BDPI-2017. Given that two recent NITRD [2] and NSF [1] meeting reports contained a set of recommendations, grand challenges, and high impact priorities for Big Data, the organizers of this meeting shifted the focus of the breakout sessions to discuss problems and available data sets that exist in five application domains - policy, health, education, economy &amp; finance, and environment &amp; energy. These domains were selected based on a survey of the PIs/co-PIs and should not be interpreted as being more important than others. Slides that were presented by the different breakout group leaders are available at https://www.bi.vt.edu/ nsf-big-data/. We hope this report will serve as a blueprint for promising big data research in five application domains.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3316416.3316425</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PRODUCT SELECTION STRATEGY ANALYSIS OF CROWDSOURCING PLATFORM FROM THE FULL COST PERSPECTIVE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Li, Ruixue and Peng, Can and Sun, Huiliang</field>
  <field name="keywords">Crowdsourcing platform, Selection Strategy Analysis, Appropriate products, Full cost</field>
  <field name="abstract">From the perspective of full cost, this paper uses Coase's transaction cost theory to analyze the causes of crowdsourcing, and on this basis to analyze the applicability of crowdsourcing platform products. At the same time, based on the crowdsourcing platform--zbj.com, we use the big data technology to grasp and analyze the related data of the crowdsourcing platform's successful cases in the past five months, and use the relevant statistical analysis method to categorize and analyze the industry attributes of the top five orders of the success cases of the zbj.com, in order to verify the theory mentioned in the article.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3335550.3335577</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">METAMORPHIC RELATIONS FOR DATA VALIDATION: A CASE STUDY OF TRANSLATED TEXT MESSAGES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Yan, Boyang and Yecies, Brian and Zhou, Zhi Quan</field>
  <field name="keywords">sentiment analysis, metamorphic relation, Oracle problem, social media, metamorphic testing, data validation, natural language processing, Douban, data quality assessment, machine translation</field>
  <field name="abstract">In conventional metamorphic testing, metamorphic relations (MRs) are identified as necessary properties of a computer program's intended functionality, whereby violations of MRs reveal faults in the program---under the assumption that the source and follow-up inputs (test cases used in metamorphic testing) are valid. In the present study, the authors argue that MRs can also be used to validate and assess the quality of the program's input data---under the assumption that the source or follow-up inputs can be inappropriately generated. Using this new perspective, a case study in the natural language processing domain is used to explore the different types of text messages that are difficult to interpret by (Chinese-English) machine translation. A total of 46,180 short user comments on Personal Tailor (a 2013 Chinese film), collected from Douban (a popular Chinese social media platform), has been used as the primary dataset of this study, and the analysis of results demonstrates that the proposed MR-based data validation method is useful for the automatic identification of poorly translated text messages.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1109/MET.2019.00018</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE DATA FIREHOSE AND AI IN GOVERNMENT: WHY DATA MANAGEMENT IS A KEY TO VALUE AND ETHICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Harrison, Teresa and F. Luna-Reyes, Luis and Pardo, Theresa and De Paula, Nic and Najafabadi, Mahdi and Palmer, Jillian</field>
  <field name="keywords">Artificial Intelligence, DMBOK, Policy Analysis, Data Analytics, Data Management</field>
  <field name="abstract">Technical and organizational innovations such as Open Data, Internet of Things and Big Data have fueled renewed interest in policy analytics in the public sector. This revamped version of policy analysis continues the long-standing tradition of applying statistical modeling to better understand policy effects and decision making, but also incorporates other computational approaches such as artificial intelligence (AI) and computer simulation. Although much attention has been given to the development of capabilities for data analysis, there is much less attention to understanding the role of data management in a context of AI in government. In this paper, we argue that data management capabilities are foundational to data analysis of any kind, but even more important in the present AI context. This is so because without proper data management, simply acquiring data or systems will not produce desired outcomes. We also argue that realizing the potential of AI for social good relies on investments specifically focused on this social outcome, investments in the processes of building trust in government data, and ensuring the data are ready and suitable for use, for both immediate and future uses.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3325112.3325245</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A DATA COMPACTING TECHNIQUE TO REDUCE THE NETFLOW SIZE IN BOTNET DETECTION WITH BOTCLUSTER</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wang, Chun-Yu and Fuh, Shih-Hao and Lo, Ta-Chun and Cheng, Qi-Jun and Chen, Yu-Cheng and Cho, Feng-Min and Chang, Jyh-Biau and Shieh, Ce-Kuen</field>
  <field name="keywords">data compression, botnet, netflow, data compacting, p2p botnet, data reduction, big data, mapreduce framework</field>
  <field name="abstract">Big data analytics helps us to find potentially valuable knowledge, but as the size of the dataset increases, the computing cost also grows exponentially. In our previous work, BotCluster, we had designed a pre-processing filtering pipeline, including whitelist filter and flow loss-response rate (FLR) filter, for data reduction, which intended to wipe out irrelative noises and reduce the computing overhead. However, we still face a data redundancy phenomenon in which some of the same feature vectors repeatedly emerged. In this paper, we propose a data compacting approach aimed to reduce the input volume and keep enough representative feature vectors to fit DBSCAN's (Density-based spatial clustering of applications with noise) criteria. It purges the redundant vectors according to a purging threshold and keeps the primary representatives. Experimental results have shown that the average data reduction ratio is about 81.34%, while the precision has only slightly decreased by 1.6% on average, and the results still have 99.88% of IPs overlapped with the previous system.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3365109.3368778</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA MINING OF STUDENTS' COURSE SELECTION BASED ON CURRENCY RULES AND DECISION TREE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liang, Yu and Duan, Xuliang and Ding, Yuanjun and Kou, Xifeng and Huang, Jingcheng</field>
  <field name="keywords">decision tree, course selection information, currency rules, data mining</field>
  <field name="abstract">The currency of data can ensure that data is not obsolete and outdated. As one of the important bases for evaluating data quality, it plays an important role in the availability of data. Data currency rules can effectively discriminate the temporal relationship between data sets. The decision tree can availably classify and predict the data, and can test the attribute values very well. In this paper, the currency rules are combined with the C4.5 algorithm in the decision tree, and the improved algorithm is applied to the college elective data in recent years. Through experiments, the algorithm used in this paper can extract the statute rules from the student elective database. According to the currency rules, the college teaching plan can be planned in advance and the curriculum resources can be allocated reasonably.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3335484.3335541</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">UNDERSTANDING E-LEARNERS' BEHAVIOUR USING DATA MINING TECHNIQUES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Al Fanah, Muna and Ansari, Muhammad Ayub</field>
  <field name="keywords">Association Rules, Accuracy, Radom Forests, Precision, Bayesian Networks</field>
  <field name="abstract">The information from Higher Education Institutions (HEIs) is primarily relevant for decision maker and educators. This study tackles e-learners behaviour using machine learning, particularly association rules and classifiers. Learners are characterized by a set of behaviours and attitudes that determine their learning abilities and skills. Learning from data generated by online learners may have significant impacts, however, few studies cover this resource from machine learning perspectives. We examine different data mining techniques including Random Forests, Logistic Regressions and Bayesian Networks as classifiers used for predicting e-learners' classes (High, Medium and Low). The novelty of this study is that it explores and compares classifiers performance on the behaviour of online learners on four variables: raise hands, visiting IT resources, view announcement and discussion impact on e-learners. The results of this study indicate an 80% accuracy level obtained by Bayesian Networks; in contrast, the Random Forests have only 63% accuracy level and Logistic Regressions for 58%.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3322134.3322145</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">IMPACT OF NEURAL NETWORK ARCHITECTURES ON ARABIC SENTIMENT ANALYSIS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Chahidi, Hamza and Omara, Hicham and Lazaar, Mohamed and Al Achhab, Mohammed</field>
  <field name="keywords">Arabic, Multi-layer Perceptron, Machine Learning, Deep Learning, Sentiment Analysis</field>
  <field name="abstract">Sentiment Analysis (SA), commonly known as opinion mining, during last couple of years, it becomes the fastest growing research areas in computer science. Conventionally, it helps to automatically detect if a text express is a positive, negative or neutral opinion. It enables us to identify and extract subjective information in a piece of writing, and this leads to gain an overview of wider public opinions or attitudes toward topics, products or services. Many researches have been done in this area, but most of them have focused on English and other Indo-European languages. Insufficient studies have actually accosted Sentiment Analysis in morphologically rich language such as Arabic. Regardless, given the increasing number of Arabic users and the exponential growth of online content, SA in this language has gained the attention of many researches last years, since Arabic raises many challenges because of its derivational, inflectional and agglutinative morphology. The objective of this paper is to promote the performance of Arabic Sentiment Analysis (ASA) by using Deep learning techniques. For that we implement Multi-Layer perceptron model in order to process and classify a dataset (Tweets). In fact, the experimental results prove that MLP as a deep learning model has a better performance for ASA than classical approaches.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3372938.3372950</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ARCHITECTURE TO MANAGE INTERNET OF THINGS DATA USING BLOCKCHAIN AND FOG COMPUTING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">El Kafhali, Said and Chahir, Chorouk and Hanini, Mohamed and Salah, Khaled</field>
  <field name="keywords">Internet of Things, Fog computing, NFV, SDN, Smart Contracts, Blockchain, Edge Computing</field>
  <field name="abstract">In this paper, we propose a novel architecture that utilizes features of Blockchain, fog computing, and cloud computing to manage IoT data. Blockchain allows to have a distributed peer-to-peer network in which non-trusting participants can interact with each other without a trusted intermediary or third party. We evaluate how this mechanism works to face the challenges of IoT with respect to multiple accessibility to IoT devises. We consider a Blockchain architecture in presence of edge computing layer. With fog or fog computing, the sensitive data can be analyzed locally instead of sending it to the cloud for analysis. Edge nodes can also keep track and control of the IoT devices that collect, analyze and store data. We show that this control can be better executed when Software Defined Network (SDN) and Network Functions Virtualization (NFV) are integrated into our process for optimal resource management. In this paper, we present our system architecture with a detailed description of the different interactions. We remark that the integration of Blockchain, IoT, and edge computing when coupled with SDN and NFV-enabled cloud infrastructure can bring to more superior and efficient platform for accessing, managing, and processing the huge influx of IoT data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3372938.3372970</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ENSURING HIGH-QUALITY PRIVATE DATA FOR RESPONSIBLE DATA SCIENCE: VISION AND CHALLENGES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Srivastava, Divesh and Scannapieco, Monica and Redman, Thomas C.</field>
  <field name="keywords">quality of private data, Responsible data science, data trust, private data</field>
  <field name="abstract">High-quality data is critical for effective data science. As the use of data science has grown, so too have concerns that individuals’ rights to privacy will be violated. This has led to the development of data protection regulations around the globe and the use of sophisticated anonymization techniques to protect privacy. Such measures make it more challenging for the data scientist to understand the data, exacerbating issues of data quality. Responsible data science aims to develop useful insights from the data while fully embracing these considerations.We pose the high-level problem in this article, “How can a data scientist develop the needed trust that private data has high quality?” We then identify a series of challenges for various data-centric communities and outline research questions for data quality and privacy researchers, which would need to be addressed to effectively answer the problem posed in this article.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3287168</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">INTERNET USAGE PATTERNS MINING FROM FIREWALL EVENT LOGS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Polpinij, Jantima and Namee, Khanista</field>
  <field name="keywords">Event logs, Generalized Sequential Pattern, Data mining, Sequential pattern mining, Inappropriate user pattern, Internet usage</field>
  <field name="abstract">Understanding users' behavior of internet usage is essential for the quality of service (QoS) analysis on the internet. If the internet providers can better understand their users, they may be able to provide better service, and also enhance the quality of the service. In general, the information about users' behavior is stored as the internet access log files, called event logs, on the server. To have the patterns of users' behavior from the event logs, this work aims to extract an interesting pattern of inappropriate user behaviors through the method of internet usage patterns mining. The primary mechanism of the proposed method is the Generalized Sequential Pattern (GSP) algorithm, which is an algorithm of sequential pattern mining. This study uses real event logs from an organization in Thailand. The results have identified exciting findings that have made possible to propose some improvements and increasing the QoS of the internet service.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3322134.3322155</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE COGNITIVE ENHANCEMENT PROCESS OF SCIENTIFIC DATA RETRIEVAL</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liu, Jianping and Wang, Jian and Zhou, Guomin and Zhang, Guilan and Cui, Yunpeng and Liu, Juan</field>
  <field name="keywords">User relevance, Scientific data retrieval, Relevance criteria</field>
  <field name="abstract">Is there a stable cognitive structure of scientific data retrieval process? Based on the theory and method of user relevance research, this study explores the cognitive characteristics of user scientific data query and retrieval. The semi-structured interview method used to collect relevant data, and the content analysis method used to encode and analyze the cognitive process of users' scientific data query and retrieval. The results show that (1) users scientific data relevance judgment not only depend on topicality, but also use accessibility, quality, authority and usefulness. (2) There are 7 combination patterns for the use of user's scientific data relevance criteria, and (3) different patterns correspond to different user relevance types and different user information need states. These 7 criteria usage patterns reveal the cognitive enhancement of user scientific data relevance judgment. The research results have a great inspiration for the development of interactive scientific data retrieval system based on user cognitive enhancement characteristics.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3331453.3360954</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PSGAN: A MINIMAX GAME FOR PERSONALIZED SEARCH WITH LIMITED AND NOISY CLICK DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lu, Shuqi and Dou, Zhicheng and Jun, Xu and Nie, Jian-Yun and Wen, Ji-Rong</field>
  <field name="keywords">generative adversarial network, personalized web search</field>
  <field name="abstract">Personalized search aims to adapt document ranking to user's personal interests. Traditionally, this is done by extracting click and topical features from historical data in order to construct a user profile. In recent years, deep learning has been successfully used in personalized search due to its ability of automatic feature learning. However, the small amount of noisy personal data poses challenges to deep learning models to learn the personalized classification boundary between relevant and irrelevant results. In this paper, we propose PSGAN, a Generative Adversarial Network (GAN) framework for personalized search. By means of adversarial training, we enforce the model to pay more attention to training data that are difficult to distinguish. We use the discriminator to evaluate personalized relevance of documents and use the generator to learn the distribution of relevant documents. Two alternative ways to construct the generator in the framework are tested: based on the current query or based on a set of generated queries. Experiments on data from a commercial search engine show that our models can yield significant improvements over state-of-the-art models.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3331184.3331218</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PROFILING THE SEMANTICS OF N-ARY WEB TABLE DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lehmberg, Oliver and Bizer, Christian</field>
  <field name="keywords">n-ary relations, web tables, data profiling, key detection</field>
  <field name="abstract">The Web contains millions of relational HTML tables, which cover a multitude of different, often very specific topics. This rich pool of data has motivated a growing body of research on methods that use web table data to extend local tables with additional attributes or add missing facts to knowledge bases. Nearly all existing approaches for these tasks build upon the assumption that web table data consists of binary relations, meaning that an attribute value depends on a single key attribute, and that the key attribute value is contained in the HTML table. Inspecting randomly chosen tables on the Web, however, quickly reveals that both assumptions are wrong for a large fraction of the tables. In order to better understand the potential of non-binary web table data for downstream applications, this papers analyses a corpus of 5 million web tables originating from 80 thousand different web sites with respect to how many web table attributes are non-binary, what composite keys are required to correctly interpret the semantics of the non-binary attributes, and whether the values of these keys are found in the table itself or need to be extracted from the page surrounding the table. The profiling of the corpus shows that at least 38% of the relations are non-binary. Recognizing these relations requires information from the title or the URL of the web page in 50% of the cases. We find that different websites use keys of varying length for the same dependent attribute, e.g. one cluster of websites presents employment numbers depending on time, another cluster presents them depending on time and profession. By identifying these clusters, we lay the foundation for selecting Web data sources according to the specificity of the keys that are used to determine specific attributes.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3323878.3325806</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A CASE STUDY OF THE AUGMENTATION AND EVALUATION OF TRAINING DATA FOR DEEP LEARNING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Ding, Junhua and Li, Xinchuan and Kang, Xiaojun and Gudivada, Venkat N.</field>
  <field name="keywords">Data quality, deep learning, machine learning, diffraction image, support vector machine, convolutional neural network</field>
  <field name="abstract">Deep learning has been widely used for extracting values from big data. As many other machine learning algorithms, deep learning requires significant training data. Experiments have shown both the volume and the quality of training data can significantly impact the effectiveness of the value extraction. In some cases, the volume of training data is not sufficiently large for effectively training a deep learning model. In other cases, the quality of training data is not high enough to achieve the optimal performance. Many approaches have been proposed for augmenting training data to mitigate the deficiency. However, whether the augmented data are “fit for purpose” of deep learning is still a question. A framework for comprehensively evaluating the effectiveness of the augmented data for deep learning is still not available. In this article, we first discuss a data augmentation approach for deep learning. The approach includes two components: the first one is to remove noisy data in a dataset using a machine learning based classification to improve its quality, and the second one is to increase the volume of the dataset for effectively training a deep learning model. To evaluate the quality of the augmented data in fidelity, variety, and veracity, a data quality evaluation framework is proposed. We demonstrated the effectiveness of the data augmentation approach and the data quality evaluation framework through studying an automated classification of biology cell images using deep learning. The experimental results clearly demonstrated the impact of the volume and quality of training data to the performance of deep learning and the importance of the data quality evaluation. The data augmentation approach and the data quality evaluation framework can be straightforwardly adapted for deep learning study in other domains.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3317573</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">APPLICATION RESEARCH OF POWER GRID FULL-BUSINESS MONITORING AND ANALYSIS BASED ON MULTI-SOURCE BUSINESS AND DATA FUSION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Min, Han Xue and Feng, Zheng Gao and Xi, Liu Peng and Dong, Wang Xu and Ping, Xu Zhong</field>
  <field name="keywords">mining analysis, business and data fusion, Big data technology</field>
  <field name="abstract">With the development of power enterprise informationization after more than ten years of development, Achieve full coverage of the company's business, effectively supporting the full-business operation of the power grid, and the accumulated business data has exploded. However, there are still problems such as low data quality, insufficient integration of multi-source business and data fusion, which makes it difficult to monitor and analyze the full-business of the power grid. This paper will combine the big data technology to study how to conduct monitoring and analysis of power grid full-business operation based on multi-source business and data fusion, and realize the three-layer architecture of business and data combination layer, business and data integration layer and business and data aggregation layer. Different levels of analysis and application, such as indicator monitoring analysis, subject monitoring analysis, and special monitoring analysis, effectively support enterprise management analysis and analytical decision.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3357292.3357306</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BLOCKCHAIN TECHNOLOGY AS AN APPROACH FOR DATA MARKETPLACES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Lawrenz, Sebastian and Sharma, Priyanka and Rausch, Andreas</field>
  <field name="keywords">Blockchain, Security, Smart Contracts, Data marketplaces, Data quality</field>
  <field name="abstract">In the digital Economy 'Data is the new oil. In the last decade technology has disrupted every filed imaginable. One such booming technology is Blockchain. A blockchain is essentially a distributed database of records or public ledger of all transactions or digital events that have been executed and shared among participating parties. And once entered, the information is immutable. Ongoing projects and prior work in the fields of big data, data mining and data science has revealed how relevant data can be used to enhance products and services. There are uncountable applications and advantages of relevant data. The most valuable companies of today treat data as a commodity, which they trade and earn revenues.But use of relevant data has also drawn attention by the other non-conventional organizations and domains. To facilitate such trading, data marketplaces have emerged. In this paper we present a global data marketplace for users to easily buy and share data. The main focus of this research is to have a central data sharing platform for the recycling industry. This paper is a part of the research project "Recycling 4.0" which is focusing on sustainably improving the recycling process through exchange of information. We identify providing secure platform, data integrity and data quality as some major challenges for a data marketplace. In this paper we also explore how global data marketplace could be implemented using blockchain and similar technologies.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3320154.3320165</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">OPINIONS CONCERNING CROWDSOURCING APPLICATIONS IN AGRICULTURE IN D.C.</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Posadas, Brianna B. and Hanumappa, Mamatha and Gilbert, Juan E.</field>
  <field name="keywords">urban agriculture, big data, focus groups, precision agriculture</field>
  <field name="abstract">As big data has become increasingly necessary in modern farming techniques, the dependence on high quality and quantity of ground truthing data has risen. Collecting ground truthing data is one of the most labor-intensive aspects of the research process. A crowdsourcing platform application to aid laypeople in completing ground truthing data can improve the quality and quantity of data for growers and agricultural researchers. Focus groups were conducted to gauge opinions on crowdsourcing initiatives in agriculture to inform the design of the platform. Preliminary results demonstrate that the greatest motivation for the participants was having opportunities to develop their skills and access to educational resources. They also discussed having a finite timeframe for collecting the data, feeling appreciated by the researchers, and being informed on the context and next steps of the research. The results of these focus groups will be used to develop design prototypes for the crowdsourcing platform.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3358961.3358970</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">USING SPARK AND SCALA FOR DISCOVERING LATENT TRENDS IN JOB MARKETS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Mbah, Raymond Blanch K. and Rege, Manjeet and Misra, Bhabani</field>
  <field name="keywords">Big Data, Scala, Singular Value Decomposition (SVD), Latent Semantic Analysis(LSA), Natural Language Processing(NLP), Spark</field>
  <field name="abstract">Job markets are experiencing an exponential growth in data alongside the recent explosion of big data in various domains including health, security and finance. Staying current with job market trends entails collecting, processing and analyzing huge amounts of data. A typical challenge with analyzing job listings is that they vary drastically with regards to verbiage, for instance a given job title or skill can be referred to using different words or industry jargons. As a result, it becomes incumbent to go beyond words present in job listings and carry out analysis aimed at discovering latent structures and trends in job listings. In this paper, we present a systematic approach of uncovering latent trends in job markets using big data technologies (Apache Spark and Scala) and distributed semantic techniques such as latent semantic analysis (LSA). We show how LSA can uncover patterns/relationships/trends that will otherwise remain hidden if using traditional text mining techniques that rely only on word frequencies in documents.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3314545.3314566</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A LARGE-SCALE AND EXTENSIBLE PLATFORM FOR PRECISION MEDICINE RESEARCH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Belghait, Fodil and April, Alain and Hamet, Pavel and Tremblay, Johanne and Desrosiers, Christian</field>
  <field name="keywords">genomics, big data, precision medicine, bioinformatics, clinical databases</field>
  <field name="abstract">The massive adoption of high-throughput genomics, deep sequencing technologies and big data technologies have made possible the era of precision medicine. However, the volume of data and its complexity remain important challenges for precision medicine research, hindering development in this field. The literature on precision medicine research describes a few platforms to support specific types of studies, but none of these offer researchers the level of customization required to meet their specific needs [1]. Methods: We propose to design and develop a platform able to import and integrate a very large volume of genetics, clinical, demographical and environmental data in a cloud computing infrastructure. In our previous publication, we presented an approach that can customize existing data models to fit any precision medicine research data requirement [1] and the requirement for future large-scale precision medicine platforms, in terms of data extensibility and the scalability of processing on demand. We also proposed a framework to meet the specific requirement of any precision medicine research [2]. In this paper, we describe how this new framework was implemented and trialed by the precision medicine researchers at the Centre Hospitalier Universitaire de l'Universit\'{e} de Montr\'{e}al (CHUM). Results: The data analysis simulations showed that the random forest algorithm presents better accuracy results. We obtained an F1-Score of 72% for random forest, 69% using linear regression and 62% using the neural network algorithm. Conclusion: The results suggest that the proposed precision medicine data analysis platform allows researchers to configure, prepare the analysis environment and customize the platform data model to their specific research in very optimal delays, at very low cost and with minimal technical skills.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3357729.3357742</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PERFDEBUG: PERFORMANCE DEBUGGING OF COMPUTATION SKEW IN DATAFLOW SYSTEMS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Teoh, Jason and Gulzar, Muhammad Ali and Xu, Guoqing Harry and Kim, Miryung</field>
  <field name="keywords">big data systems, fault localization, data intensive scalable computing, Performance debugging, data provenance</field>
  <field name="abstract">Performance is a key factor for big data applications, and much research has been devoted to optimizing these applications. While prior work can diagnose and correct data skew, the problem of computation skew---abnormally high computation costs for a small subset of input data---has been largely overlooked. Computation skew commonly occurs in real-world applications and yet no tool is available for developers to pinpoint underlying causes.To enable a user to debug applications that exhibit computation skew, we develop a post-mortem performance debugging tool. PerfDebug automatically finds input records responsible for such abnormalities in a big data application by reasoning about deviations in performance metrics such as job execution time, garbage collection time, and serialization time. The key to PerfDebug's success is a data provenance-based technique that computes and propagates record-level computation latency to keep track of abnormally expensive records throughout the pipeline. Finally, the input records that have the largest latency contributions are presented to the user for bug fixing. We evaluate PerfDebug via in-depth case studies and observe that remediation such as removing the single most expensive record or simple code rewrite can achieve up to 16X performance improvement.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3357223.3362727</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA TRANSPARENCY WITH BLOCKCHAIN AND AI ETHICS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Bertino, Elisa and Kundu, Ahish and Sura, Zehra</field>
  <field name="keywords">privacy, accountability, data provenance, Big data</field>
  <field name="abstract">Providing a 360° view of a given data item especially for sensitive data is essential toward not only protecting the data and associated privacy but also assuring trust, compliance, and ethics of the systems that use or manage such data. With the advent of General Data Protection Regulation, California Data Privacy Law, and other such regulatory requirements, it is essential to support data transparency in all such dimensions. Moreover, data transparency should not violate privacy and security requirements. In this article, we put forward a vision for how data transparency would be achieved in a de-centralized fashion using blockchain technology.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3312750</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CONSTRUCTION AND IMPLEMENTATION OF BIG DATA FRAMEWORK FOR CROP GERMPLASM RESOURCES</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing</field>
  <field name="keywords">Crop germplasm resources, Data analysis, Big data architecture, Data management</field>
  <field name="abstract">Based on understanding the application of big data and the research status of crop germplasm resources, this paper proposes a system architecture that is suitable for crop germplasm resources big data. Among them, the overall architecture of germplasm resources is elaborated through six functional modules, including data source, data integration, data processing, data application, big data operation and maintenance platform, and data management and security. The logical functional architecture specification was formulated and the technical implementation and selection are defined. The technical implementation framework describes the technical implementation of germplasm resources big data, and jointly supports the construction and operation of germplasm resources big data. Finally, a verification system is established to verify the feasibility of the big data system framework for germplasm resources.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3331453.3361308</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">QUALITY CONTROL FRAMEWORK OF BIG DATA FOR EARLY WARNING OF AGRICULTURAL METEOROLOGICAL DISASTERS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Li, Jiale and Liao, Shunbao</field>
  <field name="keywords">agro-meteorological disasters, big data, framework, early warning, quality control</field>
  <field name="abstract">Agricultural meteorological disasters, including floods, droughts, dry hot winds, low temperature chills, typhoons, hail and continuous rain, can lead to significant reduction in agricultural output. Big data platform for early warning of agricultural meteorological disaster is the basis of business operation system for early warning of agricultural meteorological disasters, and the data quality is an important guarantee for success of the early warning. Quality control of big data for early warning of agricultural meteorological disaster involves names of data sets, metadata, data documents and content of data sets. The quality control for contents of data sets is divided into quality control of attribute data and that of spatial data, and quality control of spatial data is divided into quality control of vector data and that of raster data. Methods for data quality control are divided into fully automatic, semi-automatic and full manual control methods.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3349341.3349371</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">BIG DATA MANAGEMENT AND ANALYTICS IN INTELLIGENT SMART ENVIRONMENTS: STATE-OF-THE-ART ANALYSIS AND FUTURE RESEARCH DIRECTIONS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Cuzzocrea, Alfredo</field>
  <field name="keywords">Big data management, Intelligent smart environments, Big data analytics</field>
  <field name="abstract">This paper focuses on big data management and analytics in intelligent smart environments, with particular regards to intelligent transportation and logistics systems, and provides relevant research directions that may represent a milestone for future years.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3366030.3366044</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">STUDY ON STANDARD SYSTEM OF AEROSPACE QUALITY DATA RESOURCES INTEGRATION UNDER THE BACKGROUND OF BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jia, Fengsheng and Gao, Yang and Wang, Yuming</field>
  <field name="keywords">system planning, standard system, quality data resource integration, aerospace products</field>
  <field name="abstract">The integration and application of aerospace product quality data resources is an important way to carry out quality improvement, quality evaluation and precise management. Standardization is the basis for promoting quality data resources integration. The unified and normative standard system is the guarantee for efficient development of integration standards. Firstly, we analyzed the features of quality data resources according to the status quo of integration. Integration structure of quality data resources in terms of vertical and horizontal integration was proposed by adopting the methods of "decomposition-integration" and "classification-association". Secondly, we constructed a three-dimensions architecture of quality data resource integration using the method of system engineering methodology, from the layer dimension (basis, common, special), technical dimension (description, collection, storage, transmission, processing, comprehensive management) and category dimension (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard by adopting the top-down approach. Some standard development suggestions are proposed based on the characteristics of quality data resources and standard research strategies. Finally, we applied the quality problem data resource standard construction and application to verify the proposed method.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3341620.3341624</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">APPLICATION RESEARCH OF BIG DATA FOR LAUNCH SUPPORT SYSTEM AT SPACE LAUNCH SITE</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Dong, Wei and Xiao, Litian and Niu, Shengfen and Niu, Jianjun and Wang, Fei</field>
  <field name="keywords">Application research, Big data, Space launch site, Launch support system</field>
  <field name="abstract">At the space launch site, the big data of the launch support system comes from the construction of the launch site, the ground service, the comprehensive support process, and launch mission organization and command. The big data is extensive sources, various types, large scale, and rapid growth. The big data application can improve the data processing and management efficiency for the launch support system. Then the application can enhance the support capability of flight mission and success rate. This paper analyzes the existing data application of launch support system. The challenges and requirements of big data application are studied by the construction of intelligent launch site. The application pattern and target are put forward from four aspects of launch mission organization and command, mission application, comprehensive support, and information security. The classification of big data is proposed for a launch support system. The architecture of big data application system is designed, which meets the application pattern and target. It lays a foundation for the future big data project at the launch site.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3331453.3360973</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">A RULE BASED DATA QUALITY ASSESSMENT ARCHITECTURE AND APPLICATION FOR ELECTRICAL DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Liu, He and Wang, Xiaohui and Lei, Shuya and Zhang, Xi and Liu, Weiwei and Qin, Ming</field>
  <field name="keywords">outlier, data quality, quality assessment, electrical data</field>
  <field name="abstract">Data quality assessment plays an important role in electricity consumption big data. It can help business people master the overall data situation, which can provide a strong guarantee for subsequent data improvement, analysis and decision. According to the electrical data quality issues, we design a rule-based data quality assessment architecture for electrical big data. It includes six types of data quality assessment indexes (such as comprehensiveness, accuracy, completeness), and the related data quality rules (such as non-empty rule and range rule), which can be used to guide the electrical data quality inspection. Meanwhile, for the accuracy, we propose an outlier detection method based on time time-relevant k-means, which is used to detect the voltage, curve and power data issues in electricity data. The experimental and simulation results show that the proposed architecture and method can work well for the comprehensive data quality assessment of electrical data.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3371425.3371435</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">THE CONSTRUCTION STUDY OF COLLEGE INFORMATIONIZATION TEACHING SERVICE SYSTEM UNDER THE BACKGROUND OF BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Pengxi, Li</field>
  <field name="keywords">Big data, Service system, Teaching informatization</field>
  <field name="abstract">Under the background of the rapid development of big data technology, the construction of college informationization teaching service system is the basis of the informationization teaching in colleges and universities. That is very important for the success of the informatization in Universities what is service realization model, business logic, architecture and platform conform to the whole development strategy of universities. Information management organization supports the planning, implementation, operation, maintenance and management of business information system. This paper analyzes the reform mode of college education information service system supported by big data technology. Based on the analysis of the reform mode of college informationization teaching service system supported by big data technology, this paper puts forward the design idea of post system based on big data. At the same time, with the case of "big data assisted employment", the post design and adjustment were carried out. The results show that big data assisted employment has greatly improved the efficiency and quality of the school's employment department, providing students with better employment security. Finally, the problems that need to be solved in the informatization teaching service are sorted out.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3341069.3341086</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RISK PREDICTION AND ASSESSMENT OF FOODBORNE DISEASE BASED ON BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Zhang, Mingke and Guo, Danhuai and Hu, Jinyong and Jin, Wei</field>
  <field name="keywords">big data, foodborne disease, machine learning, risk assessment</field>
  <field name="abstract">In recent years, the outbreak of foodborne diseases has been on an upward trend clearly. It is of great significance for us to predict the outbreak of foodborne diseases accurately and conduct quantitative risk assessment timely. Traditional prediction methods based on a single data source have drawbacks such as complex prediction processes and inaccurate prediction results. In this article, we figure out the scientific issues of how to improve the temporal and spatial accuracy of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze the different foodborne disease risk factors caused by the spread of water pollution in Beijing, and study the methods of collecting and preprocessing multi-source data. Then, through the comparison of different regression models and parameters tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source data fusion model to predict the outbreak of foodborne disease. Finally, we use the risk map to detect and predict foodborne disease outbreak in different business districts of Beijing based on visualization techniques, aiming to provide prevention and control assessment for decision-makers quickly and precisely.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3356998.3365776</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CUSTOMIZABLE AND SCALABLE FUZZY JOIN FOR BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Chen, Zhimin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit</field>
  <field name="keywords">nan</field>
  <field name="abstract">Fuzzy join is an important primitive for data cleaning. The ability to customize fuzzy join is crucial to allow applications to address domain-specific data quality issues such as synonyms and abbreviations. While efficient indexing techniques exist for single-node implementations of customizable fuzzy join, the state-of-the-art scale-out techniques do not support customization, and exhibit poor performance and scalability characteristics. We describe the design of a scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing (LSH) based signature scheme, and introduce optimizations that result in significant speed up with negligible impact on recall. We evaluate our implementation on the Azure Databricks version of Spark using several real-world and synthetic data sets. We observe speedups exceeding 50X compared to the best-known prior scale-out technique, and close to linear scalability with data size and number of nodes.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.14778/3352063.3352128</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ETHICAL DIMENSIONS FOR DATA QUALITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo</field>
  <field name="keywords">Data integration, knowledge extraction, source selection</field>
  <field name="abstract">nan</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3362121</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">AUGMENTING DATA QUALITY THROUGH HIGH-PRECISION GENDER CATEGORIZATION</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">M\"{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk</field>
  <field name="keywords">record completion, patenting, Data quality improvement, gender name mapping</field>
  <field name="abstract">Mappings of first name to gender have been widely recognized as a critical tool for the completion, study, and validation of data records in a range of areas. In this study, we investigate how organizations with large databases of existing entities can create their own mappings between first names and gender and how these mappings can be improved and utilized. Therefore, we first explore a dataset with demographic information on more than 4 million people, which was provided by a car insurance company. Then, we study how naming conventions have changed over time and how they differ by nationality. Next, we build a probabilistic first-name-to-gender mapping and augment the mapping by adding nationality and decade of birth to improve the mapping's performance. We test our mapping in two-label and three-label settings and further validate our mapping by categorizing patent filings by gender of the inventor. We compare the results with previous studies’ outcomes and find that our mapping produces high-precision results. We validate that the additional information of nationality and year of birth improve the precision scores of name-to-gender mappings. Therefore, the proposed approach constitutes an efficient process for improving the data quality of organizations’ records, if the gender attribute is missing or unreliable.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3297720</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">RESEARCH AND IMPLEMENTATION OF EFFICIENT PARALLEL PROCESSING OF BIG DATA AT TELBE USER FACILITY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Bawatna, Mohammed and Green, Bertram and Kovalev, Sergey and Deinert, Jan-Christoph and Knodel, Oliver and Spallek, Rainer G.</field>
  <field name="keywords">big data, signal processing, data acquisition systems, data processing pipeline, data analytics</field>
  <field name="abstract">In recent years, improvements in high-speed Analog-to-Digital Converters (ADC) and sensor technology has encouraged researchers to improve the performance of Data Acquisition (DAQ) systems for scientific experiments which require high speed and continuous data measurements --- in particular, measuring the electronic and magnetic properties of materials using pump-probe experiments at high repetition rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw data every ten minutes. The DAQ system used at TELBE partitions the raw data into various subdirectories for further parallel processing utilizing the multicore structure of modern CPUs.Furthermore, several other types of processors that accelerate data processing like the GPU and FPGA have emerged to solve the challenges of processing the massive amount of raw data. However, the memory and network bottlenecks become a significant challenge in big data processing, and new scalable programming techniques are needed to solve these challenges. In this contribution, we will outline the design and implementation of our practical software approach for efficient parallel processing of our large data sets at the TELBE user facility.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">DATA QUALITY RULE DEFINITION AND DISCOVERY</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">nan</field>
  <field name="keywords">nan</field>
  <field name="abstract">Data quality is one of the most important problems in data management, since dirty data often leads to inaccurate data analytics results and incorrect business decisions. Poor data across businesses and the U.S. government are reported to cost trillions of dollars a year. Multiple surveys show that dirty data is the most common barrier faced by data scientists. Not surprisingly, developing effective and efficient data cleaning solutions is challenging and is rife with deep theoretical and engineering problems.This book is about data cleaning, which is used to refer to all kinds of tasks and activities to detect and repair errors in the data. Rather than focus on a particular data cleaning task, we give an overview of the endto- end data cleaning process, describing various error detection and repair methods, and attempt to anchor these proposals with multiple taxonomies and views. Specifically, we cover four of the most common and important data cleaning tasks, namely, outlier detection, data transformation, error repair (including imputing missing values), and data deduplication. Furthermore, due to the increasing popularity and applicability of machine learning techniques, we include a chapter that specifically explores how machine learning techniques are used for data cleaning, and how data cleaning is used to improve machine learning models.This book is intended to serve as a useful reference for researchers and practitioners who are interested in the area of data quality and data cleaning. It can also be used as a textbook for a graduate course. Although we aim at covering state-of-the-art algorithms and techniques, we recognize that data cleaning is still an active field of research and therefore provide future directions of research whenever appropriate.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">APPLICATION OF "ARTIFICIAL INTELLIGENCE AND BIG DATA" IN SPORTS REHABILITATION FOR CHINESE JUDICIAL ADMINISTRATIVE DRUG ADDICTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Jia, Dong-Ming and Yuan, Cun-Feng and Guo, Song and Jiang, Zu-Zhen and Xu, Ding and Wang, Da-An</field>
  <field name="keywords">big data, rehabilitation training, artificial intelligence, judicial administrative, Sports rehabilitation</field>
  <field name="abstract">Under the background of "Wisdom Drug Rehabilitation", we introduced "Artificial Intelligence and Big Data" into "exercise rehabilitation" work of drug addicts in judicial administrative system. It is a practical innovation of drug treatment in China. This article will elaborate this innovation of the construction and application of "Exercise Rehabilitation" intelligence platform system. This system will improve mental status, alleviate physical and psychological symptoms, ensure safety in places, lighten the burden of professional police officers, make rapid analysis, make accurate decisions and improve the integrity rate of the addict.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3358331.3358336</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">ANALYTICAL VISUALIZATION OF HIGHER EDUCATION INSTITUTIONS' BIG DATA FOR DECISION MAKING</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Cabanban-Casem, Christianne Lynnette</field>
  <field name="keywords">Knowledge Management, Higher Education Data, Data Science</field>
  <field name="abstract">Education is an important element towards learning and human development, thus, it is the key towards identifying competencies and better productivity for the workforce. As part of the Commission on Higher Education's (CHED) thrust for improving efficiency and effectiveness by simplifying the collection process for all the stakeholders, the developed system will drastically improve the availability of data for making informed decisions and efficient generation of reports.This research outlines opportunities and challenges associated with the implementation and governance of Big Data in higher education through development and implementation of data analytics tool.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3314527.3314537</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">EMBRACING OPPORTUNITIES OF LIVESTOCK BIG DATA INTEGRATION WITH PRIVACY CONSTRAINTS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Papst, Franz and Saukh, Olga and R\"{o}mer, Kay and Grandl, Florian and Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J\"{u}rgen and Egger-Danner, Christa</field>
  <field name="keywords">agriculture, privacy-preserving data analysis, data privacy</field>
  <field name="abstract">Today's herd management undergoes a major transformation triggered by the penetration of cheap sensor solutions into cattle farms, and the promise of predictive analytics to detect animal health issues and product-related problems before they occur. The latter is particularly important to prevent disease spread, ensure animal health, animal welfare and product quality. Sensor businesses entering the market tend to build their solutions as end-to-end pipelines spanning sensors, proprietary algorithms, cloud services, and mobile apps. Since data privacy is an important issue in this industry, as a result, disconnected data silos, heterogeneity of APIs, and lack of common standards limit the value the sensor technologies could provide for herd management. In the last few years, researchers and communities proposed a number of data integration architectures to enable exchange between streams of sensor data. This paper surveys the existing efforts and outlines the opportunities they fail to address by treating sensor data as a black box. We discuss alternative solutions to the problem based on privacy-preserving collaborative learning, and provide a set of scenarios to show their benefits for both farmers and businesses.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3365871.3365900</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">PROCESSES, POTENTIAL BENEFITS, AND LIMITATIONS OF BIG DATA ANALYTICS: A CASE ANALYSIS OF 311 DATA FROM CITY OF MIAMI</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Hagen, Loni and Seon Yi, Hye and Pietri, Siana and E. Keller, Thomas</field>
  <field name="keywords">big data analytics, e-government, 311 data, information visualization</field>
  <field name="abstract">As part of the open government movement, an increasing number of 311 call centers have made their datasets available to the public. Studies have found that 311 request patterns are associated with personal attributes and living conditions. Most of these studies used New York City 311 data. In this study, we use 311 data from the City of Miami, a smaller local government, as a case study. This study contributes to digital government research and practices by making suggestions on best practices regarding the use of big data analytics on 311 data. In addition, we discuss limitations of 311 data and analytics results. Finally, we expect our results to inform decision making within the City of Miami government and other local governments.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3325112.3325212</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">SOCIAL MEDIA DATA PROCESSING INFRASTRUCTURE BY USING APACHE SPARK BIG DATA PLATFORM: TWITTER DATA ANALYSIS</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Podhoranyi, Michal and Vojacek, Lukas</field>
  <field name="keywords">social network data, data processing architecture, Twitter, Apache Spark</field>
  <field name="abstract">Social media provide continuous data streams that contain information with different level of sensitivity, validity and accuracy. Therefore, this type of information has to be properly filtered, extracted and processed to avoid noisy and inaccurate results. The main goal of this work is to propose architecture and workflow able to process Twitter social network data in near real-time. The primary design of the introduced modern architecture covers all processing aspects from data ingestion and storing to data processing and analysing. This paper presents Apache Spark and Hadoop implementation. The secondary objective is to analyse tweets with the defined topic --- floods. The word frequency method (Word Clouds) is shown as a major tool to analyse the content of the input dataset. The experimental architecture confirmed the usefulness of many well-known functions of Spark and Hadoop in the social data domain. The platforms which were used provided effective tools for optimal data ingesting, storing as well as processing and analysing. Based on the analytical part, it was observed that the word frequency method (n-grams) can effectively reveal the tweets content. According to the results of this study, the tweets proved their high informative potential regarding data quality and quantity.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3361821.3361825</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">CITESEERX: 20 YEARS OF SERVICE TO SCHOLARLY BIG DATA</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Wu, Jian and Kim, Kunho and Giles, C. Lee</field>
  <field name="keywords">digital libraries, scholarly big data, CiteSeerX, search engines</field>
  <field name="abstract">We overview CiteSeerX, the pioneer digital library search engine, that has been serving academic communities for more than 20 years (first released in 1998), from three perspectives. The system perspective summarizes its architecture evolution in three phases over the past 20 years. The data perspective describes how CiteSeerX has created searchable scholarly big datasets and made them freely available for multiple purposes. In order to be scalable and effective, AI technologies are employed in all essential modules. To effectively train these models, a sufficient amount of data has been labeled, which can then be reused for training future models. Finally, we discuss the future of CiteSeerX. Our ongoing work is to make CiteSeerX more sustainable. To this end, we are working to ingest all open access scholarly papers, estimated to be 30-40 million. Part of the plan is to discover dataset mentions and metadata in scholarly articles and make them more accessible via search interfaces. Users will have more opportunities to explore and trace datasets that can be reused and discover other datasets for new research projects. We summarize what was learned to make a similar system more sustainable and useful.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">10.1145/3359115.3359119</field>
</item>
<item>
  <field name="rank">nan</field>
  <field name="title">REAL-TIME SUPPLY CHAIN SIMULATION: A BIG DATA-DRIVEN APPROACH</field>
  <field name="total_cities">nan</field>
  <field name="type_publication_x">nan</field>
  <field name="jcr_value">nan</field>
  <field name="scimago_value">nan</field>
  <field name="author">Vieira, Ant\'{o}nio A. C. and Dias, Lu\'{\i}s M. S. and Santos, Maribel Y. and Pereira, Guilherme A. B. and Oliveira, Jos\'{e} A.</field>
  <field name="keywords">nan</field>
  <field name="abstract">Simulation of Supply Chains comprises huge amounts of data, resulting in numerous entities flowing in the model. These networks are highly dynamic systems, where entities' relationships and other elements evolve with time, paving the way for real-time Supply Chain decision-support tools capable of using real data. In light of this, a solution comprising of a Big Data Warehouse to store relevant data and a simulation model of an automotive plant, are being developed. The purpose of this paper is to address the modelling approach, which allowed the simulation model to automatically adapt to the data stored in a Big Data Warehouse and thus adapt to new scenarios without manual intervention. The main characteristics of the conceived solution were demonstrated, with emphasis to the real-time and the ability to allow the model to load the state of the system from the Big Data Warehouse.</field>
  <field name="year">2019</field>
  <field name="type_publication_y">nan</field>
  <field name="doi">nan</field>
</item>